{
  "title": "IDE <-> Noted <-> AI Agent Documentation Pipeline",
  "subtitle": "Use Noted as communication layer between developer, IDE, and AI agents",
  "version": "1.0.0",
  "date": "2025-10-09",
  "status": "planning",
  "priority": "high",
  "effort": "high (30-40 hours for MVP)",

  "overview": {
    "description": "Transform Noted into a central knowledge hub where developers write documentation in their IDE, notes automatically sync to Noted app, and AI agents (like Claude Code) can read/write to the same knowledge base using RAG. Enables persistent agent memory and living documentation.",
    "core_concept": "Noted becomes the 'shared brain' between human developers and AI agents",
    "key_innovation": "Agent has persistent access to project context without developer repeating information every session",
    "target_users": [
      "Developers using Claude Code or similar AI coding assistants",
      "Teams wanting shared documentation accessible to both humans and AI",
      "Solo developers building complex projects with AI assistance"
    ]
  },

  "the_problem": {
    "current_state": [
      "AI agents have no memory between sessions",
      "Developer must repeat project context every time",
      "Documentation lives separate from agent workflow",
      "Agent-generated docs don't persist anywhere useful",
      "No feedback loop: agent learns -> documents -> developer corrects -> agent relearns"
    ],
    "pain_points": [
      "Context switching: leave IDE -> open separate docs -> copy/paste",
      "Lost context: agent forgets decisions from previous sessions",
      "Manual documentation: developer must write everything agent figures out",
      "Scattered knowledge: project info across Notion, Slack, code comments, etc."
    ]
  },

  "the_solution": {
    "architecture": "Three-way real-time sync: IDE <-> Noted <-> AI Agent",
    "data_flow": [
      "Developer writes/edits notes in IDE (VS Code, Cursor, etc.)",
      "IDE extension syncs changes to Noted app via API",
      "Notes get vector embeddings (RAG infrastructure)",
      "Agent queries Noted via RAG when user asks questions",
      "Agent can write new notes back to Noted",
      "New notes appear in IDE automatically",
      "Developer reviews/corrects -> cycle continues"
    ],
    "benefits": [
      "Agent has persistent project knowledge across sessions",
      "Living documentation that both humans and AI maintain",
      "Context automatically available to agent (no copy/paste)",
      "Agent documents its work -> developer reviews in IDE",
      "Single source of truth for project knowledge",
      "Collaboration between human and AI made seamless"
    ]
  },

  "architecture_diagram": {
    "visual": "+---------------+\n|   VS Code     |  Developer writes docs\n|   (+ Ext)     |  Auto-save triggers sync\n+-------+-------+\n        |\n        | REST API / WebSocket\n        v\n+---------------+\n|  Noted App    |  Central knowledge hub\n|  + Supabase   |  - Notes database\n|  + RAG        |  - Vector embeddings\n+-------+-------+  - Search/query API\n        |\n        | RAG Query API\n        v\n+---------------+\n|  AI Agent     |  Claude Code, Cursor AI, etc.\n| (Claude Code) |  - Reads via RAG\n+---------------+  - Writes via API",
    "components": {
      "ide_extension": "VS Code extension monitoring project files",
      "noted_api": "Supabase REST API + custom Edge Functions",
      "rag_system": "Vector embeddings + semantic search (from RAG plan)",
      "agent_interface": "API methods for agents to read/write notes"
    }
  },

  "use_cases": {
    "use_case_1_agent_learns_from_docs": {
      "title": "Agent learns project context from developer's notes",
      "scenario": [
        "1. Developer creates `/docs/architecture.md` in project",
        "2. IDE extension syncs to Noted app",
        "3. Noted generates embedding (RAG)",
        "4. Developer asks agent: 'How does authentication work?'",
        "5. Agent queries RAG -> finds architecture.md",
        "6. Agent answers based on YOUR documentation",
        "7. No need to repeat context every session"
      ],
      "value": "Agent has persistent memory of project decisions"
    },
    "use_case_2_agent_documents_work": {
      "title": "Agent documents implementation decisions as it works",
      "scenario": [
        "1. Developer: 'Implement user authentication'",
        "2. Agent implements auth system",
        "3. Agent creates note 'Auth Implementation.md' via API",
        "4. Note syncs to Noted app",
        "5. Note appears in IDE sidebar automatically",
        "6. Developer reviews agent's documentation",
        "7. Developer makes corrections in IDE",
        "8. Changes sync back to Noted",
        "9. Agent sees updates on next query"
      ],
      "value": "Living documentation maintained by both human and AI"
    },
    "use_case_3_team_knowledge_base": {
      "title": "Shared knowledge base for team + AI",
      "scenario": [
        "1. Team members write docs in their IDEs",
        "2. All docs sync to central Noted workspace",
        "3. Any team member's agent can query all docs",
        "4. Agent A learns from what Agent B documented",
        "5. Consistent context across entire team",
        "6. New team member: agent reads all docs instantly"
      ],
      "value": "Single source of truth accessible by humans and AI"
    },
    "use_case_4_iterative_refinement": {
      "title": "Iterative documentation refinement loop",
      "scenario": [
        "1. Agent generates initial API design doc",
        "2. Developer reviews in IDE, adds corrections",
        "3. Agent reads updated doc",
        "4. Agent refines implementation based on corrections",
        "5. Agent updates doc with final decisions",
        "6. Developer approves -> doc becomes reference",
        "7. Future queries use refined doc"
      ],
      "value": "Documentation improves through human-AI collaboration"
    },
    "use_case_5_context_persistence": {
      "title": "Agent remembers across coding sessions",
      "scenario": [
        "Day 1: Agent helps design database schema",
        "        Agent writes 'Database Schema.md'",
        "Day 2: New session, developer asks about tables",
        "        Agent queries RAG -> reads yesterday's doc",
        "        Agent remembers schema without being told",
        "Day 3: Schema changes, developer updates doc",
        "        Agent automatically sees new version"
      ],
      "value": "Agent context persists days, weeks, months"
    }
  },

  "implementation": {
    "phase_1_vs_code_extension_foundation": {
      "description": "Build basic VS Code extension that syncs markdown files to Noted",
      "duration": "8-12 hours",
      "deliverables": [
        "VS Code extension package (TypeScript)",
        "File watcher for project /docs folder or .noted/ folder",
        "One-way sync: IDE -> Noted (create/update notes)",
        "Authentication via API key stored in VS Code settings",
        "Status bar indicator (synced/syncing/error)"
      ],
      "technical_details": {
        "extension_structure": {
          "src/extension.ts": "Main extension entry point",
          "src/fileWatcher.ts": "Monitor .md files for changes",
          "src/notedClient.ts": "API client for Noted backend",
          "src/auth.ts": "Handle API key authentication",
          "package.json": "Extension manifest"
        },
        "api_calls": [
          "POST /api/notes/sync - Create or update note from file",
          "GET /api/notes/list - List all synced notes",
          "DELETE /api/notes/{id} - Delete note when file removed"
        ],
        "file_watching": "Use VS Code workspace.createFileSystemWatcher to monitor *.md files",
        "sync_strategy": "Debounce file changes (500ms) to avoid excessive API calls",
        "conflict_resolution_phase1": "Last write wins (simple MVP approach)"
      },
      "setup_flow": {
        "step_1": "User installs 'Noted Sync' extension from VS Code marketplace",
        "step_2": "Extension prompts for Noted API key (from app settings)",
        "step_3": "User configures sync folder (default: /docs or /.noted)",
        "step_4": "Extension scans folder and uploads existing .md files",
        "step_5": "Status bar shows 'Synced ✓' when complete"
      },
      "user_settings": {
        "noted.apiKey": "User's API key from Noted app",
        "noted.syncFolder": "Folder to watch (default: 'docs')",
        "noted.autoSync": "Enable/disable auto-sync (default: true)",
        "noted.syncOnSave": "Sync immediately on save vs debounced (default: true)"
      },
      "client_reliability_patterns": {
        "overview": {
          "problem": "Network failures, API timeouts, and rate limits require sophisticated client-side error handling",
          "approach": "Implement retry logic, circuit breaker, batch operations, and dead letter queue for resilience"
        },
        "1_retry_logic": {
          "strategy": "Exponential backoff with jitter",
          "implementation": {
            "retry_schedule": [
              "Attempt 1: Immediate (0ms delay)",
              "Attempt 2: 1 second delay",
              "Attempt 3: 2 seconds delay",
              "Attempt 4: 4 seconds delay",
              "Attempt 5: 8 seconds delay",
              "Attempt 6: 16 seconds delay (max)",
              "After 6 attempts: move to dead letter queue"
            ],
            "jitter": "Add random ±20% to delay to prevent thundering herd (e.g., 2s becomes 1.6s-2.4s)",
            "retry_conditions": {
              "always_retry": ["Network timeout (ETIMEDOUT)", "Connection refused (ECONNREFUSED)", "DNS lookup failed (ENOTFOUND)", "429 Rate Limit Exceeded", "500 Internal Server Error", "502 Bad Gateway", "503 Service Unavailable", "504 Gateway Timeout"],
              "never_retry": ["400 Bad Request (client error)", "401 Unauthorized (invalid API key)", "403 Forbidden (insufficient permissions)", "404 Not Found (note doesn't exist)", "422 Unprocessable Entity (validation error)"],
              "conditional_retry": "409 Conflict: retry once after user resolves conflict"
            },
            "respect_retry_after": {
              "behavior": "If 429 response includes Retry-After header, use that value instead of exponential backoff",
              "example": "Retry-After: 60 -> wait 60 seconds before next attempt"
            },
            "code_example": "async function retryWithBackoff(fn, maxAttempts = 6) {\n  for (let attempt = 1; attempt <= maxAttempts; attempt++) {\n    try {\n      return await fn();\n    } catch (error) {\n      if (!isRetriable(error) || attempt === maxAttempts) {\n        throw error;\n      }\n      const delay = Math.min(1000 * Math.pow(2, attempt - 1), 16000);\n      const jitter = delay * (0.8 + Math.random() * 0.4);\n      await sleep(jitter);\n    }\n  }\n}"
          },
          "user_visibility": {
            "status_bar": "Show 'Retrying... (attempt 3/6)' during retries",
            "notification": "Only show error notification after all retries exhausted",
            "logs": "Log each retry attempt with delay and reason: 'Retry 3/6 after 4.2s due to network timeout'"
          }
        },
        "2_circuit_breaker": {
          "purpose": "Stop making requests to failing endpoint, fail fast to conserve resources",
          "states": {
            "CLOSED": "Normal operation, requests pass through",
            "OPEN": "Endpoint is failing, reject requests immediately without calling API",
            "HALF_OPEN": "Test if endpoint recovered, allow limited requests"
          },
          "state_transitions": {
            "CLOSED_to_OPEN": "After 5 consecutive failures OR 50% failure rate over 10 requests",
            "OPEN_to_HALF_OPEN": "After 30 seconds in OPEN state",
            "HALF_OPEN_to_CLOSED": "After 3 consecutive successful requests",
            "HALF_OPEN_to_OPEN": "If any request fails in HALF_OPEN state"
          },
          "implementation": {
            "per_endpoint": "Separate circuit breaker for each endpoint (create, update, delete, list)",
            "per_user": "Each user has independent circuit breakers (one user's failures don't affect others)",
            "state_storage": "Store circuit state in extension global state: {endpoint: {state: 'OPEN', failureCount: 5, lastFailureTime: 1234567890}}",
            "metrics_tracked": {
              "total_requests": "Count of all requests",
              "failed_requests": "Count of failed requests",
              "success_rate": "failed_requests / total_requests",
              "consecutive_failures": "Reset to 0 on success"
            }
          },
          "user_experience": {
            "OPEN_state": "Show warning notification: 'Noted API is currently unavailable. Retrying in 30 seconds...'",
            "status_bar": "Show 'API Unavailable ⚠️' in status bar",
            "queuing": "Queue pending sync operations, process when circuit closes",
            "manual_override": "Provide 'Retry Now' command to force HALF_OPEN test"
          },
          "fallback_behavior": {
            "create_note": "Queue create operation in local database (IndexedDB)",
            "update_note": "Queue update operation with timestamp",
            "delete_note": "Queue delete operation",
            "list_notes": "Return cached list from last successful fetch",
            "on_recovery": "Flush queued operations in order when circuit closes"
          }
        },
        "3_dead_letter_queue": {
          "purpose": "Permanently failed sync operations need manual inspection",
          "triggers": [
            "Max retries exhausted (6 attempts)",
            "Validation errors (422 response)",
            "Oversized content (>1MB)",
            "Malformed markdown (parse errors)",
            "API key revoked mid-sync"
          ],
          "storage": {
            "table": "failed_syncs (local IndexedDB in extension)",
            "schema": {
              "id": "UUID (unique identifier)",
              "operation": "create | update | delete",
              "note_title": "string",
              "note_content": "string (full content for inspection)",
              "ide_file_path": "string",
              "error_message": "string (last error)",
              "error_code": "string (e.g., 'RATE_LIMIT', 'NETWORK_TIMEOUT')",
              "retry_count": "number (6 when added to DLQ)",
              "first_attempt_at": "ISO timestamp",
              "last_attempt_at": "ISO timestamp",
              "queued_at": "ISO timestamp (when added to DLQ)"
            }
          },
          "user_interface": {
            "sidebar_view": "New 'Failed Syncs' section in Noted sidebar",
            "notification": "Show notification when item added to DLQ: 'Failed to sync architecture.md. View details.'",
            "inspection": "Click item to open diff view: local version vs last known remote version",
            "actions": {
              "retry": "Manually retry single failed sync",
              "retry_all": "Retry all failed syncs (max 10 at a time)",
              "discard": "Remove from DLQ without syncing",
              "edit_and_retry": "Open editor to fix content, then retry"
            }
          },
          "automatic_cleanup": {
            "max_age": "Items older than 30 days automatically removed",
            "max_size": "Keep max 100 items in DLQ (FIFO eviction)",
            "user_notification": "Warn if DLQ >20 items: 'You have 23 failed syncs. Review?'"
          }
        },
        "4_batch_operations": {
          "problem": "Initial sync of 500 files = 500 sequential API calls = 5 minutes (at 100ms per call)",
          "solution": "Batch endpoint accepts multiple notes in single request",
          "api_design": {
            "endpoint": "POST /ide-sync/batch",
            "request_schema": {
              "notes": [
                {
                  "title": "string",
                  "content": "string",
                  "ide_file_path": "string",
                  "ide_last_modified": "ISO timestamp",
                  "idempotency_key": "UUID",
                  "sync_origin_id": "string",
                  "content_sha256": "string"
                }
              ]
            },
            "limits": {
              "max_notes_per_batch": 100,
              "max_total_size": "10MB per batch",
              "rate_limit": "10 batch requests per minute (same quota as 1000 individual requests)"
            },
            "response_schema": {
              "results": [
                {
                  "idx": 0,
                  "success": true,
                  "note_id": "uuid",
                  "message": "Note created successfully"
                },
                {
                  "idx": 1,
                  "success": false,
                  "error_code": "VALIDATION_ERROR",
                  "error_message": "Title exceeds 200 chars"
                }
              ],
              "summary": {
                "total": 100,
                "succeeded": 98,
                "failed": 2
              }
            }
          },
          "client_implementation": {
            "initial_sync": [
              "1. Scan sync folder for all .md files",
              "2. Read file metadata (size, mtime)",
              "3. Sort by size (smallest first for quick wins)",
              "4. Chunk into batches of 100 files each",
              "5. Send batches sequentially (not parallel to respect rate limits)",
              "6. Show progress: 'Syncing 100/500 files...'",
              "7. Collect failed items from each batch",
              "8. Retry failed items individually with exponential backoff"
            ],
            "incremental_sync": [
              "Use individual create/update endpoints (not batch)",
              "Batch only for bulk operations (initial sync, folder import)"
            ],
            "optimization": {
              "incremental_hash_check": "Before batching, compute content_sha256 for each file, skip if matches last known hash (reduces batch size by ~80%)",
              "parallel_hashing": "Compute hashes for 100 files in parallel using worker threads",
              "compression": "Gzip batch request body (reduces transfer time by ~60%)"
            }
          },
          "server_implementation": {
            "transaction_handling": "Process batch in single database transaction for atomicity",
            "rollback_on_failure": "If >10% of batch fails, rollback entire transaction, return error, client retries with smaller batches",
            "partial_success": "If <10% fails, commit successful items, return partial success response",
            "database_optimization": "Use PostgreSQL COPY or batch INSERT for performance: INSERT INTO notes VALUES (...), (...), (...)"
          },
          "performance_impact": {
            "before": "500 files × 100ms per API call = 50 seconds",
            "after": "500 files / 100 per batch × 500ms per batch = 2.5 seconds",
            "speedup": "20x faster for initial sync"
          }
        },
        "5_connection_pooling": {
          "problem": "Supabase Edge Functions may exhaust DB connections under load",
          "solution": "PgBouncer connection pooler between Edge Functions and PostgreSQL",
          "configuration": {
            "pool_mode": "Transaction pooling (connections released after transaction commits)",
            "pool_size": "20 connections per Edge Function",
            "max_client_conn": "100 (max concurrent clients)",
            "default_pool_size": "20",
            "reserve_pool_size": "5 (emergency reserve for admin queries)"
          },
          "monitoring": {
            "metrics": ["Active connections", "Idle connections", "Connection wait time", "Pool exhaustion events"],
            "alerts": "Alert if wait time >1 second OR pool exhaustion >10 events/min"
          }
        },
        "6_caching_strategy": {
          "problem": "Agent repeatedly queries same architecture docs, wasting API calls",
          "solution": "Multi-layer caching: client-side (extension) + server-side (Redis)",
          "client_side_cache": {
            "what_to_cache": ["Note metadata (title, id, updated_at)", "Note content (full markdown)", "List of IDE-synced notes"],
            "cache_key": "note:{note_id}",
            "ttl": "5 minutes for content, 1 minute for metadata",
            "invalidation": "Invalidate on local edit OR remote update notification",
            "storage": "VS Code Memento (persisted across restarts)"
          },
          "server_side_cache": {
            "backend": "Redis (Upstash for serverless)",
            "what_to_cache": ["RAG query results: rag:{query_hash} -> [note_ids]", "Note content: note:{id} -> {title, content}", "User's synced notes list: user:{id}:notes -> [note_ids]"],
            "ttl": "1 minute for RAG queries, 5 minutes for note content",
            "invalidation": "On note update: DEL note:{id}, SCAN for rag:* keys containing note_id and delete",
            "cache_hit_rate_target": ">70%"
          },
          "etag_support": {
            "implementation": "Compute ETag: SHA256(content + updated_at), return in ETag header",
            "client_request": "Include If-None-Match: {etag} in request header",
            "server_response": "If match: return 304 Not Modified (no body), else return 200 with new ETag",
            "bandwidth_savings": "~60% reduction for unchanged notes"
          },
          "expected_impact": {
            "api_load_reduction": "70% fewer API calls",
            "latency_improvement": "Cache hit: <10ms vs API call: 50-150ms",
            "cost_savings": "$100/month in API costs → $30/month"
          }
        }
      }
    },

    "phase_2_noted_api_sync_endpoints": {
      "description": "Add API endpoints to Noted backend for IDE sync operations",
      "duration": "4-6 hours",
      "deliverables": [
        "New Supabase Edge Function: ide-sync",
        "API routes for create/update/delete notes from IDE",
        "Special note metadata: synced_from_ide: boolean, ide_file_path: string",
        "Rate limiting (100 requests/minute per user)",
        "Webhook support (optional for phase 2)"
      ],
      "technical_details": {
        "edge_function": {
          "file": "supabase/functions/ide-sync/index.ts",
          "routes": [
            "POST /ide-sync/create - Create new note from IDE file",
            "PUT /ide-sync/update/{id} - Update existing note",
            "DELETE /ide-sync/delete/{id} - Delete note (file removed in IDE)",
            "GET /ide-sync/list - List all IDE-synced notes for user",
            "GET /ide-sync/status - Check sync status and last sync time"
          ]
        },
        "api_contracts": {
          "authentication_header": {
            "header": "Authorization: Bearer {api_key}",
            "validation": "SELECT user_id FROM user_api_keys WHERE key = {api_key} AND expires_at > NOW()",
            "error_codes": {
              "401": "Invalid or expired API key",
              "403": "API key does not have required permissions"
            }
          },
          "endpoints": {
            "create_note": {
              "method": "POST",
              "path": "/ide-sync/create",
              "request_schema": {
                "title": "string (required, max 200 chars)",
                "content": "string (required, max 1MB)",
                "ide_file_path": "string (required, e.g., 'docs/architecture.md')",
                "ide_last_modified": "ISO 8601 timestamp (required)",
                "folder_id": "uuid | null (optional, default null)",
                "idempotency_key": "string (required, client-generated UUID, prevents duplicate creates)",
                "sync_origin_id": "string (required, unique ID for this IDE/Agent instance, prevents sync loops)",
                "content_sha256": "string (required, SHA-256 hash of content, verifies integrity)"
              },
              "request_example": {
                "title": "Architecture Overview",
                "content": "# System Architecture\n\nThis document describes...",
                "ide_file_path": "docs/architecture.md",
                "ide_last_modified": "2025-10-09T10:30:00Z",
                "folder_id": null,
                "idempotency_key": "f7a3b2c1-8e4d-4f9a-b5c6-1a2b3c4d5e6f",
                "sync_origin_id": "vscode-instance-abc123",
                "content_sha256": "e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855"
              },
              "response_schema": {
                "success": "boolean",
                "note": {
                  "id": "uuid",
                  "title": "string",
                  "content": "string",
                  "synced_from_ide": "boolean (always true)",
                  "ide_file_path": "string",
                  "last_ide_sync_at": "ISO 8601 timestamp",
                  "created_at": "ISO 8601 timestamp",
                  "updated_at": "ISO 8601 timestamp"
                }
              },
              "response_example": {
                "success": true,
                "note": {
                  "id": "a1b2c3d4-e5f6-7890-abcd-ef1234567890",
                  "title": "Architecture Overview",
                  "content": "# System Architecture\n\nThis document describes...",
                  "synced_from_ide": true,
                  "ide_file_path": "docs/architecture.md",
                  "last_ide_sync_at": "2025-10-09T10:30:05Z",
                  "created_at": "2025-10-09T10:30:05Z",
                  "updated_at": "2025-10-09T10:30:05Z"
                }
              },
              "error_responses": {
                "400": {
                  "error": "Invalid request",
                  "message": "Missing required field: ide_file_path"
                },
                "401": {
                  "error": "Unauthorized",
                  "message": "Invalid or expired API key"
                },
                "413": {
                  "error": "Payload too large",
                  "message": "Content exceeds 1MB limit"
                },
                "429": {
                  "error": "Rate limit exceeded",
                  "message": "Maximum 100 requests per minute exceeded. Retry after 60 seconds.",
                  "retry_after": 60
                }
              }
            },
            "update_note": {
              "method": "PUT",
              "path": "/ide-sync/update/{id}",
              "path_params": {
                "id": "uuid (note ID)"
              },
              "request_schema": {
                "content": "string (required, max 1MB)",
                "title": "string (optional, max 200 chars)",
                "ide_last_modified": "ISO 8601 timestamp (required)",
                "folder_id": "uuid | null (optional)"
              },
              "request_example": {
                "content": "# System Architecture\n\nUpdated content...",
                "title": "Architecture Overview (Updated)",
                "ide_last_modified": "2025-10-09T11:45:00Z"
              },
              "response_schema": {
                "success": "boolean",
                "note": {
                  "id": "uuid",
                  "title": "string",
                  "content": "string",
                  "last_ide_sync_at": "ISO 8601 timestamp",
                  "updated_at": "ISO 8601 timestamp"
                }
              },
              "response_example": {
                "success": true,
                "note": {
                  "id": "a1b2c3d4-e5f6-7890-abcd-ef1234567890",
                  "title": "Architecture Overview (Updated)",
                  "content": "# System Architecture\n\nUpdated content...",
                  "last_ide_sync_at": "2025-10-09T11:45:05Z",
                  "updated_at": "2025-10-09T11:45:05Z"
                }
              },
              "error_responses": {
                "400": {
                  "error": "Invalid request",
                  "message": "Missing required field: content"
                },
                "401": {
                  "error": "Unauthorized",
                  "message": "Invalid or expired API key"
                },
                "404": {
                  "error": "Not found",
                  "message": "Note with ID a1b2c3d4-e5f6-7890-abcd-ef1234567890 not found"
                },
                "409": {
                  "error": "Conflict",
                  "message": "Note was modified in Noted app. IDE timestamp: 2025-10-09T11:45:00Z, Noted timestamp: 2025-10-09T11:50:00Z",
                  "conflict_resolution_required": true,
                  "noted_version": {
                    "content": "...",
                    "updated_at": "2025-10-09T11:50:00Z"
                  }
                },
                "429": {
                  "error": "Rate limit exceeded",
                  "message": "Maximum 100 requests per minute exceeded. Retry after 60 seconds.",
                  "retry_after": 60
                }
              }
            },
            "delete_note": {
              "method": "DELETE",
              "path": "/ide-sync/delete/{id}",
              "path_params": {
                "id": "uuid (note ID)"
              },
              "request_schema": {},
              "response_schema": {
                "success": "boolean",
                "message": "string"
              },
              "response_example": {
                "success": true,
                "message": "Note a1b2c3d4-e5f6-7890-abcd-ef1234567890 deleted successfully"
              },
              "error_responses": {
                "401": {
                  "error": "Unauthorized",
                  "message": "Invalid or expired API key"
                },
                "404": {
                  "error": "Not found",
                  "message": "Note with ID a1b2c3d4-e5f6-7890-abcd-ef1234567890 not found"
                },
                "429": {
                  "error": "Rate limit exceeded",
                  "message": "Maximum 100 requests per minute exceeded. Retry after 60 seconds.",
                  "retry_after": 60
                }
              }
            },
            "list_notes": {
              "method": "GET",
              "path": "/ide-sync/list",
              "query_params": {
                "folder_id": "uuid (optional, filter by folder)",
                "limit": "number (optional, default 100, max 500)",
                "offset": "number (optional, default 0)",
                "modified_since": "ISO 8601 timestamp (optional, only return notes modified after this time)"
              },
              "request_example": "/ide-sync/list?folder_id=abc123&limit=50&modified_since=2025-10-09T10:00:00Z",
              "response_schema": {
                "success": "boolean",
                "notes": [
                  {
                    "id": "uuid",
                    "title": "string",
                    "content": "string",
                    "synced_from_ide": "boolean",
                    "ide_file_path": "string | null",
                    "last_ide_sync_at": "ISO 8601 timestamp | null",
                    "created_at": "ISO 8601 timestamp",
                    "updated_at": "ISO 8601 timestamp"
                  }
                ],
                "total": "number (total matching notes)",
                "has_more": "boolean (true if more results available)"
              },
              "response_example": {
                "success": true,
                "notes": [
                  {
                    "id": "a1b2c3d4-e5f6-7890-abcd-ef1234567890",
                    "title": "Architecture Overview",
                    "content": "# System Architecture\n\n...",
                    "synced_from_ide": true,
                    "ide_file_path": "docs/architecture.md",
                    "last_ide_sync_at": "2025-10-09T10:30:05Z",
                    "created_at": "2025-10-09T10:00:00Z",
                    "updated_at": "2025-10-09T10:30:05Z"
                  }
                ],
                "total": 1,
                "has_more": false
              },
              "error_responses": {
                "401": {
                  "error": "Unauthorized",
                  "message": "Invalid or expired API key"
                },
                "429": {
                  "error": "Rate limit exceeded",
                  "message": "Maximum 100 requests per minute exceeded. Retry after 60 seconds.",
                  "retry_after": 60
                }
              }
            },
            "sync_status": {
              "method": "GET",
              "path": "/ide-sync/status",
              "request_schema": {},
              "response_schema": {
                "success": "boolean",
                "status": {
                  "total_synced_notes": "number",
                  "last_sync_at": "ISO 8601 timestamp | null",
                  "sync_enabled": "boolean",
                  "rate_limit_remaining": "number (requests remaining in current minute)",
                  "rate_limit_reset_at": "ISO 8601 timestamp"
                }
              },
              "response_example": {
                "success": true,
                "status": {
                  "total_synced_notes": 15,
                  "last_sync_at": "2025-10-09T10:30:05Z",
                  "sync_enabled": true,
                  "rate_limit_remaining": 85,
                  "rate_limit_reset_at": "2025-10-09T10:31:00Z"
                }
              },
              "error_responses": {
                "401": {
                  "error": "Unauthorized",
                  "message": "Invalid or expired API key"
                }
              }
            }
          },
          "rate_limiting_implementation": {
            "strategy": "Token bucket algorithm with per-user limits",
            "limits": {
              "requests_per_minute": 100,
              "burst_allowance": 10
            },
            "enforcement": {
              "storage": "Redis or Supabase realtime for rate limit counters",
              "key_format": "rate_limit:{user_id}:{minute_timestamp}",
              "algorithm": [
                "1. Extract user_id from API key",
                "2. Get current minute timestamp (e.g., '2025-10-09T10:30')",
                "3. Increment counter: INCR rate_limit:{user_id}:{timestamp}",
                "4. Set expiry: EXPIRE rate_limit:{user_id}:{timestamp} 120",
                "5. If counter > 100, return 429 with retry_after header",
                "6. Otherwise, process request"
              ]
            },
            "headers_returned": {
              "X-RateLimit-Limit": "100",
              "X-RateLimit-Remaining": "85",
              "X-RateLimit-Reset": "1728475860 (Unix timestamp)"
            }
          }
        },
        "api_design_enhancements": {
          "overview": {
            "problem": "Production API needs versioning, standardized responses, advanced security, and health monitoring",
            "approach": "Add versioning strategy, response envelopes, fine-grained permissions, request signing, resource limits, and health checks"
          },
          "1_api_versioning": {
            "problem": "Breaking changes affect deployed extensions - no migration path",
            "solution": "Version endpoints with /v1/ prefix, maintain backward compatibility for 6 months",
            "implementation": {
              "url_structure": {
                "versioned_endpoints": [
                  "/v1/ide-sync/create",
                  "/v1/ide-sync/update/{id}",
                  "/v1/ide-sync/delete/{id}",
                  "/v1/ide-sync/list",
                  "/v1/ide-sync/status",
                  "/v1/rag-query"
                ],
                "unversioned_fallback": "Requests to /ide-sync/* redirect to /v1/ide-sync/* with deprecation warning header"
              },
              "version_negotiation": {
                "request_header": "Accept: application/vnd.noted.v2+json (client specifies desired version)",
                "response_header": "API-Version: v1 (server indicates version used)",
                "version_mismatch": "If client requests v3 but server only supports v1-v2, return 406 Not Acceptable"
              },
              "deprecation_policy": {
                "timeline": [
                  "T+0: v2 released, v1 marked deprecated",
                  "T+3 months: Warning logged for v1 users",
                  "T+6 months: v1 sunset, 410 Gone returned for v1 requests",
                  "Migration guide published with breaking changes"
                ],
                "deprecation_header": "Deprecation: version=v1, sunset=2025-04-09, link=https://docs.noted.com/api/migration-v1-v2"
              },
              "breaking_vs_non_breaking": {
                "non_breaking": ["Adding new optional fields", "Adding new endpoints", "Adding new error codes"],
                "breaking": ["Removing fields", "Changing field types", "Changing authentication", "Renaming endpoints"],
                "minor_version": "Non-breaking changes increment minor version (v1.0 -> v1.1), no URL change",
                "major_version": "Breaking changes increment major version (v1 -> v2), new URL required"
              }
            },
            "guarantees": "Extensions continue working during transitions, clear migration timeline, no surprise breakage"
          },
          "2_webhook_delivery": {
            "problem": "Webhooks mentioned but no delivery guarantees - could lose events",
            "solution": "HMAC-signed webhooks with exponential backoff retry and delivery receipts",
            "implementation": {
              "webhook_registration": {
                "endpoint": "POST /v1/webhooks",
                "request_schema": {
                  "url": "https://customer.com/noted-webhook",
                  "events": ["note.created", "note.updated", "note.deleted", "conflict.detected"],
                  "secret": "customer-provided secret for HMAC signing"
                }
              },
              "payload_signing": {
                "algorithm": "HMAC-SHA256",
                "header": "X-Noted-Signature: sha256=a1b2c3d4...",
                "computation": "HMAC-SHA256(webhook_secret, request_body)",
                "verification": "Customer computes HMAC of received body, compares to X-Noted-Signature header"
              },
              "delivery_attempt": {
                "timeout": "5 seconds per attempt",
                "success_criteria": "HTTP 200-299 response",
                "failure_criteria": "Non-2xx response OR timeout OR network error",
                "retry_schedule": ["Immediate", "30s later", "5m later", "30m later"],
                "max_attempts": 4
              },
              "delivery_tracking": {
                "database_table": "webhook_deliveries",
                "columns": [
                  "id UUID PRIMARY KEY",
                  "webhook_id UUID REFERENCES webhooks(id)",
                  "event_type TEXT",
                  "payload JSONB",
                  "attempt_count INTEGER DEFAULT 0",
                  "last_attempt_at TIMESTAMP",
                  "status TEXT ('pending' | 'delivered' | 'failed')",
                  "http_status_code INTEGER",
                  "error_message TEXT"
                ]
              },
              "delivery_receipts_api": {
                "endpoint": "GET /v1/webhooks/deliveries?status=failed",
                "response": "List of failed webhook deliveries with retry option",
                "manual_retry": "POST /v1/webhooks/deliveries/{id}/retry"
              }
            },
            "guarantees": "At-least-once delivery, cryptographic authenticity, manual recovery for permanent failures"
          },
          "3_response_envelope": {
            "problem": "Success vs error response shapes inconsistent - difficult to parse",
            "solution": "Standardize response format: {data, meta, errors}",
            "implementation": {
              "success_response": {
                "format": "{\n  data: T | null,\n  meta: {\n    request_id: string,\n    timestamp: string,\n    api_version: string\n  },\n  errors: []\n}",
                "example": "{\n  data: {id: 'note-123', title: 'Architecture'},\n  meta: {request_id: 'req-abc', timestamp: '2025-10-09T12:00:00Z', api_version: 'v1'},\n  errors: []\n}"
              },
              "error_response": {
                "format": "{\n  data: null,\n  meta: {...},\n  errors: [\n    {code: string, message: string, field?: string, details?: object}\n  ]\n}",
                "example": "{\n  data: null,\n  meta: {request_id: 'req-xyz', timestamp: '2025-10-09T12:00:00Z', api_version: 'v1'},\n  errors: [\n    {code: 'VALIDATION_ERROR', message: 'Title exceeds 200 chars', field: 'title'},\n    {code: 'VALIDATION_ERROR', message: 'Content cannot be empty', field: 'content'}\n  ]\n}"
              },
              "error_codes": {
                "standardized_codes": ["VALIDATION_ERROR", "NOT_FOUND", "UNAUTHORIZED", "RATE_LIMIT_EXCEEDED", "CONFLICT", "INTERNAL_ERROR", "NETWORK_TIMEOUT"],
                "machine_readable": "Clients can switch on error.code instead of parsing error.message",
                "i18n_friendly": "error.message in English, clients can translate using error.code"
              },
              "meta_field_usage": {
                "request_id": "Include in bug reports for debugging: 'Error occurred with request_id: req-abc'",
                "timestamp": "Client-side time skew detection: compare server timestamp to local time",
                "api_version": "Log which API version was used for analytics"
              }
            },
            "guarantees": "Consistent parsing logic, better error handling, easier debugging"
          },
          "4_api_key_scopes": {
            "problem": "API key has all-or-nothing access - cannot restrict agent to read-only",
            "solution": "Add scopes array to API keys with granular permissions",
            "implementation": {
              "scope_definition": {
                "available_scopes": ["notes:read", "notes:write", "notes:delete", "rag:query", "ide:sync", "webhooks:manage", "api_keys:manage"],
                "scope_format": "resource:action (e.g., notes:read means read notes)"
              },
              "database_schema": {
                "table": "user_api_keys",
                "new_column": "scopes TEXT[] DEFAULT ARRAY['notes:read', 'notes:write']",
                "example": "scopes = ['notes:read', 'rag:query'] (read-only agent key)"
              },
              "scope_validation": {
                "edge_function_middleware": [
                  "1. Extract API key from Authorization header",
                  "2. Query database: SELECT scopes FROM user_api_keys WHERE key = $1",
                  "3. Determine required scope for endpoint: POST /ide-sync/create requires 'notes:write'",
                  "4. Check if required scope in user's scopes array",
                  "5. If not: return 403 Forbidden with error: 'Missing required scope: notes:write'",
                  "6. If yes: proceed with request"
                ]
              },
              "default_scopes": {
                "user_keys": "['notes:read', 'notes:write', 'notes:delete', 'ide:sync'] (full access except admin)",
                "agent_keys": "['notes:read', 'rag:query'] (read-only)",
                "integration_keys": "['notes:read', 'notes:write', 'webhooks:manage'] (no delete)"
              },
              "ui_for_scope_selection": {
                "create_api_key_modal": "Checkboxes for each scope with explanations",
                "example": "[✓] notes:read - Read note content\n[✓] rag:query - Perform semantic search\n[ ] notes:delete - Delete notes (agents typically don't need this)"
              }
            },
            "guarantees": "Principle of least privilege, limit blast radius of compromised keys"
          },
          "5_request_signing": {
            "problem": "MITM could modify requests in transit despite TLS (defense in depth)",
            "solution": "Sign sensitive operations with HMAC-SHA256(api_key, request_body)",
            "implementation": {
              "signed_operations": ["POST /ide-sync/create", "PUT /ide-sync/update", "DELETE /ide-sync/delete", "POST /notes/{id}/restore"],
              "signature_generation": {
                "client_side": [
                  "1. Stringify request body: JSON.stringify(body)",
                  "2. Compute HMAC: crypto.createHmac('sha256', api_key).update(body_string).digest('hex')",
                  "3. Include in header: X-Signature: {hmac_hex}",
                  "4. Optionally include timestamp: X-Signature-Timestamp: {unix_timestamp}"
                ]
              },
              "signature_verification": {
                "server_side": [
                  "1. Extract X-Signature header from request",
                  "2. Extract api_key from Authorization header",
                  "3. Compute expected HMAC: HMAC-SHA256(api_key, request.body)",
                  "4. Compare expected HMAC to X-Signature (constant-time comparison)",
                  "5. If mismatch: return 403 Forbidden with error: 'Invalid request signature'",
                  "6. If match: proceed with request"
                ]
              },
              "replay_attack_prevention": {
                "include_timestamp": "Client includes X-Signature-Timestamp: {unix_timestamp} in header",
                "server_validation": "Reject requests with timestamp >5 minutes old",
                "rationale": "Even if attacker captures signed request, can only replay within 5-minute window"
              },
              "optional_for_reads": {
                "reads_unsigned": "GET requests don't require signatures (lower overhead)",
                "writes_signed": "All POST/PUT/DELETE require signatures (higher security)"
              }
            },
            "guarantees": "Cryptographic proof request came from legitimate client, prevents MITM tampering"
          },
          "6_resource_rate_limiting": {
            "problem": "Global rate limits don't prevent quota exhaustion (single user creating 1000 notes)",
            "solution": "Per-resource limits enforced before database insert",
            "implementation": {
              "resource_quotas": {
                "max_notes_per_user": 500,
                "max_notes_per_folder": 100,
                "max_notes_per_day_from_agent": 50,
                "max_folders_per_user": 50,
                "max_api_keys_per_user": 10
              },
              "enforcement_points": {
                "create_note": "SELECT COUNT(*) FROM notes WHERE user_id = $1; IF count >= 500 THEN return 403",
                "create_note_in_folder": "SELECT COUNT(*) FROM notes WHERE folder_id = $1; IF count >= 100 THEN return 403",
                "agent_create_note": "SELECT COUNT(*) FROM notes WHERE user_id = $1 AND note_type = 'agent_generated' AND created_at > NOW() - INTERVAL '1 day'; IF count >= 50 THEN return 403"
              },
              "error_response": {
                "status_code": 403,
                "error_code": "QUOTA_EXCEEDED",
                "message": "You have reached the maximum of 500 notes per user. Please delete some notes or upgrade your plan.",
                "details": {
                  "current_count": 500,
                  "limit": 500,
                  "resource": "notes"
                }
              },
              "quota_visibility": {
                "api_endpoint": "GET /v1/quotas -> {notes_used: 450, notes_limit: 500, folders_used: 20, folders_limit: 50}",
                "ui_display": "Show quota usage in settings: 'Notes: 450 / 500'"
              }
            },
            "guarantees": "Prevent resource exhaustion, fair usage across users, upgrade incentive for power users"
          },
          "7_health_check_endpoint": {
            "problem": "Extension should detect API unavailability before attempting sync",
            "solution": "GET /health endpoint with <100ms response time showing system status",
            "implementation": {
              "endpoint": "GET /health",
              "response_schema": {
                "healthy": "{\n  status: 'healthy',\n  checks: {\n    database: 'ok',\n    rag: 'ok',\n    storage: 'ok'\n  },\n  version: 'v1.2.3',\n  uptime_seconds: 86400\n}",
                "degraded": "{\n  status: 'degraded',\n  checks: {\n    database: 'ok',\n    rag: 'degraded',\n    storage: 'ok'\n  },\n  version: 'v1.2.3',\n  uptime_seconds: 86400\n}",
                "down": "{\n  status: 'down',\n  checks: {\n    database: 'error',\n    rag: 'ok',\n    storage: 'ok'\n  },\n  version: 'v1.2.3',\n  uptime_seconds: 86400\n}"
              },
              "check_implementation": {
                "database": "SELECT 1 - if succeeds within 50ms: 'ok', else 'error'",
                "rag": "SELECT COUNT(*) FROM note_embeddings LIMIT 1 - if succeeds within 50ms: 'ok', else 'degraded'",
                "storage": "Check Supabase storage connectivity - if responds: 'ok', else 'error'"
              },
              "response_time_target": "<100ms p95 (must be fast to avoid blocking client startup)",
              "caching": "Cache health check result for 10 seconds to handle thundering herd",
              "extension_polling": {
                "frequency": "Poll every 60 seconds",
                "on_unhealthy": "Show status bar warning: 'Noted API degraded ⚠️'",
                "on_down": "Show status bar error: 'Noted API offline ❌', disable sync operations"
              }
            },
            "guarantees": "Early detection of API issues, graceful degradation, user visibility into system health"
          }
        },
        "database_changes": {
          "migration": "20251010_add_ide_sync_metadata.sql",
          "changes": [
            "ALTER TABLE notes ADD COLUMN synced_from_ide BOOLEAN DEFAULT FALSE;",
            "ALTER TABLE notes ADD COLUMN ide_file_path TEXT;",
            "ALTER TABLE notes ADD COLUMN last_ide_sync_at TIMESTAMP;",
            "CREATE INDEX idx_notes_ide_synced ON notes(user_id, synced_from_ide) WHERE synced_from_ide = TRUE;"
          ]
        },
        "authentication": "Use existing user_api_keys table, validate API key in Edge Function",
        "rate_limiting": "Use Supabase Edge Function rate limiting (100 req/min)",
        "sync_metadata": {
          "synced_from_ide": true,
          "ide_file_path": "docs/architecture.md",
          "ide_last_modified": "2025-10-09T10:30:00Z",
          "last_ide_sync_at": "2025-10-09T10:30:05Z"
        },
        "distributed_systems_fundamentals": {
          "overview": {
            "problem": "Three-way sync (IDE <-> Noted <-> Agent) creates distributed system with network failures, concurrent updates, and ordering challenges",
            "approach": "Implement idempotency, causality tracking, sync loop prevention, and distributed transactions to ensure correctness"
          },
          "1_idempotency": {
            "problem": "Network retry after timeout could create duplicate notes (client doesn't know if first request succeeded)",
            "solution": "Client-generated idempotency keys with 24-hour deduplication window",
            "implementation": {
              "client_behavior": [
                "1. Client generates UUID for idempotency_key before each create/update request",
                "2. Client includes Idempotency-Key header in request",
                "3. On network timeout/error, client retries with SAME idempotency_key",
                "4. Client only generates new key for genuinely new operations"
              ],
              "server_behavior": [
                "1. Extract idempotency_key from request",
                "2. Check idempotent_requests table: SELECT response_body FROM idempotent_requests WHERE key = $1 AND created_at > NOW() - INTERVAL '24 hours'",
                "3. If found: return cached response immediately (200 OK with original response)",
                "4. If not found: process request normally",
                "5. Before returning success: INSERT INTO idempotent_requests (key, response_body, created_at) VALUES ($1, $2, NOW())",
                "6. Background job deletes keys older than 24 hours"
              ],
              "database_schema": {
                "table": "idempotent_requests",
                "columns": [
                  "id UUID PRIMARY KEY",
                  "idempotency_key UUID UNIQUE NOT NULL",
                  "user_id UUID REFERENCES users(id)",
                  "endpoint TEXT NOT NULL (e.g., '/ide-sync/create')",
                  "request_hash TEXT (SHA-256 of request body, optional validation)",
                  "response_body JSONB NOT NULL (cached response)",
                  "status_code INTEGER NOT NULL (e.g., 200, 201)",
                  "created_at TIMESTAMP DEFAULT NOW()"
                ],
                "indexes": [
                  "CREATE UNIQUE INDEX idx_idempotent_key ON idempotent_requests(idempotency_key);",
                  "CREATE INDEX idx_idempotent_created_at ON idempotent_requests(created_at) WHERE created_at > NOW() - INTERVAL '24 hours';"
                ]
              },
              "edge_cases": {
                "request_body_changed": "If request_hash differs, return 422 Unprocessable Entity: 'Idempotency key reused with different request body'",
                "concurrent_requests": "Use INSERT ... ON CONFLICT DO NOTHING with row-level lock to handle simultaneous requests with same key",
                "failure_during_processing": "If request fails (400/500 error), do NOT cache response - allow retry with same key"
              }
            },
            "guarantees": "Exactly-once semantics for create operations despite network failures or client retries"
          },
          "2_sync_loop_prevention": {
            "problem": "Agent writes note -> syncs to Noted -> syncs to IDE -> file watcher triggers -> syncs back to Noted -> infinite loop",
            "solution": "Track sync origin ID and ignore changes originating from self",
            "implementation": {
              "origin_id_generation": {
                "format": "{client_type}-{instance_uuid} (e.g., 'vscode-a1b2c3d4', 'agent-x9y8z7w6')",
                "persistence": "Extension stores in workspace state (survives restart but unique per workspace)",
                "uniqueness": "UUID v4 generated on first activation, reused for lifetime of workspace",
                "header": "X-Sync-Origin: {origin_id} included in all API requests"
              },
              "server_storage": {
                "database": "ALTER TABLE notes ADD COLUMN last_sync_origin_id TEXT;",
                "update_on_write": "UPDATE notes SET last_sync_origin_id = $1 WHERE id = $2 (store origin that last modified note)"
              },
              "client_filtering": {
                "polling_logic": [
                  "1. Extension polls GET /ide-sync/changes?since={timestamp}",
                  "2. Server returns notes with last_sync_origin_id included",
                  "3. Extension filters out notes where last_sync_origin_id == self.originId",
                  "4. Extension only processes notes from OTHER origins",
                  "5. Extension writes filtered notes to local filesystem"
                ],
                "file_watcher_logic": [
                  "1. File watcher detects change to docs/architecture.md",
                  "2. Extension checks: did I just write this file from sync? (track recent writes)",
                  "3. If yes (last write <5 seconds ago): ignore event, don't sync",
                  "4. If no (user edit): sync to Noted with self.originId"
                ]
              },
              "multi_agent_scenario": {
                "problem": "Two agents (agent-A, agent-B) both writing to same note",
                "solution": "Each agent has unique origin_id, both changes propagate normally, conflict detection handles race",
                "example": [
                  "1. Agent-A writes note at 10:00:00, origin=agent-A",
                  "2. Agent-B writes same note at 10:00:05, origin=agent-B",
                  "3. IDE extension sees both updates (different origins)",
                  "4. Conflict detected (both modified since last IDE sync)",
                  "5. User resolves conflict manually"
                ]
              }
            },
            "guarantees": "Prevents circular syncing within single origin while allowing multi-origin updates"
          },
          "3_causality_tracking": {
            "problem": "Timestamp comparison fails with clock skew (IDE clock 5 min ahead causes false conflicts)",
            "solution": "Version vectors (Lamport timestamps) for causal ordering independent of wall-clock time",
            "implementation": {
              "version_vector_structure": {
                "format": "Map of origin_id -> counter (e.g., {'vscode-abc': 5, 'agent-xyz': 2, 'app-web': 3})",
                "storage": "JSONB column in notes table: ALTER TABLE notes ADD COLUMN version_vector JSONB DEFAULT '{}'::jsonb;",
                "initialization": "New note starts with {'origin_id': 1}",
                "increment_rule": "Each update increments counter for its origin: version_vector[origin_id]++",
                "example_evolution": [
                  "Create in IDE: {'vscode-abc': 1}",
                  "Edit in IDE: {'vscode-abc': 2}",
                  "Edit by Agent: {'vscode-abc': 2, 'agent-xyz': 1}",
                  "Edit in App: {'vscode-abc': 2, 'agent-xyz': 1, 'app-web': 1}"
                ]
              },
              "conflict_detection_logic": {
                "algorithm": "Compare version vectors using happens-before relation",
                "happens_before": "V1 < V2 iff for all k: V1[k] <= V2[k] AND exists k: V1[k] < V2[k]",
                "concurrent": "V1 || V2 iff neither V1 < V2 nor V2 < V1 (conflict!)",
                "example_conflict": [
                  "IDE version: {'vscode-abc': 3, 'agent-xyz': 1}",
                  "Noted version: {'vscode-abc': 2, 'agent-xyz': 2}",
                  "Neither < other (vscode counter decreased in Noted version, agent counter increased)",
                  "Result: CONCURRENT EDIT -> CONFLICT"
                ],
                "example_no_conflict": [
                  "IDE version: {'vscode-abc': 2, 'agent-xyz': 1}",
                  "Noted version: {'vscode-abc': 3, 'agent-xyz': 1}",
                  "IDE < Noted (vscode counter increased, all others same)",
                  "Result: Noted version is newer -> auto-sync to IDE, no conflict"
                ]
              },
              "merge_semantics": {
                "on_successful_sync": "Merge version vectors: new_vector = max(V1, V2) for each key",
                "example": [
                  "IDE: {'vscode-abc': 5, 'agent-xyz': 2}",
                  "Noted: {'vscode-abc': 4, 'agent-xyz': 3, 'app-web': 1}",
                  "Merged: {'vscode-abc': 5, 'agent-xyz': 3, 'app-web': 1}"
                ]
              },
              "api_changes": {
                "requests": "Include current version_vector in update requests: PUT /ide-sync/update/{id} body includes version_vector field",
                "responses": "Return updated version_vector in response for client to cache",
                "conflict_response": "409 Conflict with both version vectors: {local_vector: {...}, remote_vector: {...}}"
              }
            },
            "guarantees": "Correct conflict detection even with arbitrary clock skew, based on causal ordering not wall-clock time"
          },
          "4_distributed_transactions": {
            "problem": "Creating note + generating embedding is multi-step: if embedding fails, orphaned note exists",
            "solution": "Saga pattern with compensating transactions",
            "implementation": {
              "approach_chosen": "Saga pattern (not 2PC - Supabase Edge Functions can't hold DB locks)",
              "saga_steps": [
                "Step 1: Insert note into notes table (local transaction)",
                "Step 2: Queue embedding job (via database webhook trigger)",
                "Step 3: Edge Function generates embedding (async)",
                "Step 4: Insert embedding into note_embeddings (local transaction)"
              ],
              "failure_handling": {
                "embedding_failure": {
                  "detection": "Edge Function catches error from Voyage AI API",
                  "compensating_action": "Mark note with embedding_failed flag instead of deleting (preserve data)",
                  "retry_strategy": "Exponential backoff: retry after 1m, 5m, 15m (max 3 attempts)",
                  "permanent_failure": "After 3 failures, set embedding_status = 'failed_permanently', alert monitoring",
                  "user_visibility": "RAG queries show 'Embedding pending' for failed notes, exclude from search results"
                },
                "note_insertion_failure": {
                  "detection": "Database constraint violation (duplicate key, invalid FK)",
                  "action": "Return 400/422 to client immediately, no compensation needed (transaction rolled back)"
                }
              },
              "database_schema_additions": {
                "table": "notes",
                "new_columns": [
                  "embedding_status TEXT DEFAULT 'pending' CHECK (embedding_status IN ('pending', 'in_progress', 'completed', 'failed_permanently'))",
                  "embedding_retry_count INTEGER DEFAULT 0",
                  "embedding_last_attempt_at TIMESTAMP",
                  "embedding_error_message TEXT"
                ]
              },
              "idempotent_embedding_generation": {
                "problem": "Retry could regenerate embedding for already-embedded note",
                "solution": "Check embedding_status before generating: IF embedding_status = 'completed' THEN skip, ELSE generate"
              },
              "workflow_visualization": [
                "Happy path:",
                "  Client -> POST /ide-sync/create -> Insert note (pending) -> Trigger webhook -> Generate embedding -> Update (completed) -> Return 201",
                "",
                "Failure path:",
                "  Client -> POST /ide-sync/create -> Insert note (pending) -> Trigger webhook -> Embedding fails -> Retry 3x -> Mark failed_permanently -> Alert monitoring"
              ]
            },
            "guarantees": "Note always exists (no orphaned data), embedding eventually consistent with automatic retry, failures logged and alertable"
          },
          "5_content_integrity": {
            "problem": "Network corruption or malicious MITM could modify content in transit",
            "solution": "Client computes SHA-256 hash, server validates before persisting",
            "implementation": {
              "client_side": [
                "1. Client reads file content",
                "2. Compute hash: crypto.createHash('sha256').update(content).digest('hex')",
                "3. Include in request body: content_sha256 field"
              ],
              "server_side": [
                "1. Receive request with content and content_sha256",
                "2. Compute server-side hash of received content",
                "3. Compare: IF server_hash !== content_sha256 THEN return 422 Unprocessable Entity",
                "4. Error response: {error: 'Content integrity check failed', expected: '...', actual: '...'}",
                "5. Only persist if hashes match"
              ],
              "storage": "ALTER TABLE notes ADD COLUMN content_sha256 TEXT; (enables deduplication and change detection)",
              "change_detection_optimization": [
                "1. Before syncing, client checks local hash vs last known remote hash",
                "2. If identical: skip sync (no changes)",
                "3. Reduces unnecessary API calls by ~80%"
              ]
            },
            "guarantees": "Detects any content corruption or tampering, ensures data integrity end-to-end"
          },
          "6_eventual_consistency_model": {
            "problem": "CAP theorem: can't have consistency + availability + partition tolerance - must choose",
            "choice": "AP system (availability + partition tolerance) with eventual consistency and conflict detection",
            "consistency_guarantees": {
              "read_your_writes": "Client always sees its own writes immediately (caching in extension local state)",
              "monotonic_reads": "Client never sees older version after seeing newer (version vectors enforce this)",
              "eventual_consistency": "All replicas (IDE, Noted, Agent) converge to same state given no new updates",
              "convergence_time": {
                "polling_mode": "Max 10 seconds staleness (poll interval)",
                "websocket_mode": "Max 1 second staleness (real-time push)",
                "conflict_resolution": "Synchronous with user input (blocks until resolved)"
              }
            },
            "staleness_handling": {
              "acceptable_staleness": "10 seconds (design choice for MVP)",
              "monitoring": "Track p99 staleness latency: time from write to read across all clients",
              "alert_threshold": "Alert if p99 staleness >30 seconds (indicates polling failure)"
            },
            "partition_handling": {
              "network_partition_scenario": "IDE offline for 1 hour, user makes edits, comes back online",
              "resolution": [
                "1. Extension queues local changes during offline period",
                "2. On reconnect, sync queued changes to Noted",
                "3. Pull remote changes from Noted",
                "4. Detect conflicts using version vectors",
                "5. Prompt user to resolve conflicts",
                "6. Resume normal sync"
              ]
            }
          }
        },
        "data_integrity_enhancements": {
          "overview": {
            "problem": "Distributed sync requires additional data integrity mechanisms beyond basic conflict detection",
            "approach": "Add merge base tracking, tombstone records, event coalescing, revision history, polymorphic types, and file metadata"
          },
          "7_merge_base_tracking": {
            "problem": "Three-way merge needs common ancestor content, current design only tracks last_ide_sync_at timestamp",
            "solution": "Store merge_base_content at last successful sync for true three-way diff",
            "implementation": {
              "database_schema": {
                "table": "notes",
                "new_column": "merge_base_content TEXT (content at last successful sync across all clients)"
              },
              "update_strategy": {
                "on_successful_sync": "UPDATE notes SET merge_base_content = content, last_ide_sync_at = NOW() WHERE id = $1",
                "on_conflict_resolution": "After user resolves conflict, set merge_base_content = resolved_content"
              },
              "three_way_diff_usage": {
                "algorithm": "diff3(merge_base_content, ide_version_content, noted_version_content)",
                "tools": "Use diff-match-patch library or git diff3 algorithm",
                "output": "Merged content with conflict markers where automatic merge fails",
                "example": [
                  "Base: 'Hello World'",
                  "IDE: 'Hello Beautiful World'",
                  "Noted: 'Hello Amazing World'",
                  "Diff3 output: 'Hello <<<<<<< IDE Beautiful ======= Amazing >>>>>>> Noted World'"
                ]
              },
              "storage_optimization": {
                "compression": "Compress merge_base_content with gzip (reduces storage by ~60%)",
                "cleanup": "Delete merge_base after 30 days of no conflicts (reclaim space)"
              }
            },
            "guarantees": "Enables accurate conflict resolution by showing what changed from common ancestor"
          },
          "8_tombstone_records": {
            "problem": "Deleted files in IDE need distributed deletion semantics - race condition if IDE deletes file while Agent queries note",
            "solution": "Soft delete with deleted_at timestamp, tombstones persist 30 days before garbage collection",
            "implementation": {
              "database_schema": {
                "table": "notes",
                "new_columns": [
                  "deleted_at TIMESTAMP DEFAULT NULL",
                  "deleted_by TEXT (origin_id of client that deleted note)"
                ],
                "index": "CREATE INDEX idx_notes_deleted ON notes(deleted_at) WHERE deleted_at IS NOT NULL;"
              },
              "delete_workflow": {
                "client_side": [
                  "1. User deletes file in IDE",
                  "2. Extension calls DELETE /ide-sync/delete/{id}",
                  "3. Server sets deleted_at = NOW(), deleted_by = sync_origin_id",
                  "4. Server does NOT physically delete row"
                ],
                "sync_propagation": [
                  "1. Extension polls GET /ide-sync/changes?since={timestamp}",
                  "2. Server includes notes with deleted_at in response",
                  "3. Extension detects deleted_at field",
                  "4. Extension removes local file",
                  "5. Other clients (Agent, Noted app) also see deletion"
                ],
                "queries": {
                  "active_notes": "SELECT * FROM notes WHERE deleted_at IS NULL",
                  "deleted_notes": "SELECT * FROM notes WHERE deleted_at IS NOT NULL",
                  "tombstones_to_gc": "SELECT * FROM notes WHERE deleted_at < NOW() - INTERVAL '30 days'"
                }
              },
              "garbage_collection": {
                "trigger": "Cron job runs daily",
                "query": "DELETE FROM notes WHERE deleted_at < NOW() - INTERVAL '30 days'",
                "rationale": "30 days allows all clients to see deletion and sync, then reclaim space"
              },
              "race_condition_handling": {
                "scenario": "IDE deletes note, Agent queries same note before sync completes",
                "resolution": "Agent query returns note with deleted_at field, Agent shows 'This note was deleted' message",
                "no_data_loss": "Tombstone ensures Agent knows about deletion, prevents stale data"
              }
            },
            "guarantees": "Distributed deletion semantics with eventual consistency, no data loss during delete propagation"
          },
          "9_file_system_event_coalescing": {
            "problem": "Text editors emit multiple events per save (write, rename, chmod) - could trigger duplicate syncs",
            "solution": "Accumulate FS events in 100ms window, deduplicate by file path + event type, process final state only",
            "implementation": {
              "event_buffer": {
                "data_structure": "Map<string, Event[]> keyed by file path",
                "accumulation_window": "100ms after first event",
                "example": [
                  "t=0ms: write event for architecture.md -> start 100ms timer",
                  "t=20ms: chmod event for architecture.md -> add to buffer",
                  "t=50ms: rename event for architecture.md -> add to buffer",
                  "t=100ms: timer expires -> process accumulated events"
                ]
              },
              "deduplication_logic": {
                "priority_order": "deleted > renamed > modified > created (higher priority events supersede lower)",
                "rules": [
                  "If deleted event exists: ignore all other events, process only delete",
                  "If renamed event exists: ignore modified/created, process only rename",
                  "If modified events exist: collapse into single modified event",
                  "Multiple created events: process only first (should not happen)"
                ],
                "example": "Events: [write, chmod, write, write] -> Coalesced: [write]"
              },
              "implementation_code": {
                "typescript": "const eventBuffer = new Map<string, Event[]>();\nconst timers = new Map<string, NodeJS.Timeout>();\n\nfunction onFileEvent(event: Event) {\n  const key = event.filePath;\n  if (!eventBuffer.has(key)) {\n    eventBuffer.set(key, []);\n    timers.set(key, setTimeout(() => processEvents(key), 100));\n  }\n  eventBuffer.get(key)!.push(event);\n}\n\nfunction processEvents(key: string) {\n  const events = eventBuffer.get(key)!;\n  const finalEvent = coalesceEvents(events);\n  syncToServer(finalEvent);\n  eventBuffer.delete(key);\n  timers.delete(key);\n}"
              },
              "edge_cases": {
                "rapid_file_changes": "User saves file 10 times in 1 second -> coalesced to 10 sync operations (one per 100ms window)",
                "interleaved_files": "Changes to file A and file B are processed independently (separate timers)",
                "extension_restart": "In-progress buffers are lost (acceptable - next file save will trigger sync)"
              }
            },
            "guarantees": "Reduces spurious sync operations by ~80%, prevents duplicate API calls from editor noise"
          },
          "10_revision_history": {
            "problem": "Conflict resolution needs historical versions, current design only stores current state",
            "solution": "Create note_revisions table, store revision on every update, limit to last 10 revisions per note",
            "implementation": {
              "database_schema": {
                "table": "note_revisions",
                "columns": [
                  "id UUID PRIMARY KEY",
                  "note_id UUID REFERENCES notes(id) ON DELETE CASCADE",
                  "content TEXT NOT NULL",
                  "version_number INTEGER NOT NULL (sequential version counter)",
                  "created_by TEXT (origin_id of client that created this revision)",
                  "created_at TIMESTAMP DEFAULT NOW()"
                ],
                "indexes": [
                  "CREATE INDEX idx_revisions_note_id ON note_revisions(note_id, created_at DESC);",
                  "CREATE UNIQUE INDEX idx_revisions_note_version ON note_revisions(note_id, version_number);"
                ]
              },
              "revision_creation": {
                "trigger": "Database trigger on notes table UPDATE",
                "trigger_code": "CREATE TRIGGER note_updated_trigger AFTER UPDATE ON notes FOR EACH ROW EXECUTE FUNCTION create_revision();",
                "function_logic": [
                  "1. Check if content actually changed (OLD.content != NEW.content)",
                  "2. Get current max version_number for note",
                  "3. INSERT INTO note_revisions (note_id, content, version_number, created_by) VALUES (NEW.id, OLD.content, max_version + 1, NEW.last_sync_origin_id)",
                  "4. If revision count > 10: DELETE oldest revision"
                ]
              },
              "revision_limit_enforcement": {
                "max_revisions": 10,
                "cleanup_query": "WITH ranked AS (SELECT id, ROW_NUMBER() OVER (PARTITION BY note_id ORDER BY created_at DESC) as rn FROM note_revisions) DELETE FROM note_revisions WHERE id IN (SELECT id FROM ranked WHERE rn > 10)"
              },
              "api_endpoints": {
                "list_revisions": "GET /notes/{id}/revisions -> returns array of {version_number, created_at, created_by}",
                "get_revision": "GET /notes/{id}/revisions/{version} -> returns full content of specific revision",
                "restore_revision": "POST /notes/{id}/restore/{version} -> sets note content to specified revision, creates new revision"
              },
              "conflict_resolution_usage": {
                "scenario": "Conflict detected between IDE and Noted versions",
                "workflow": [
                  "1. Show conflict resolution UI with 3 options: Keep IDE, Keep Noted, View History",
                  "2. User clicks 'View History'",
                  "3. Extension fetches GET /notes/{id}/revisions",
                  "4. Show timeline of last 10 revisions with timestamps and authors",
                  "5. User selects revision to restore",
                  "6. Extension calls POST /notes/{id}/restore/{version}",
                  "7. Conflict resolved with historical version"
                ]
              }
            },
            "guarantees": "Enables time-travel debugging, recover from accidental overwrites, view edit history"
          },
          "11_polymorphic_note_types": {
            "problem": "Notes from different sources (IDE, Agent, App) treated identically - cannot distinguish origin or apply type-specific logic",
            "solution": "Add note_type ENUM and source_metadata JSONB for type-specific data",
            "implementation": {
              "database_schema": {
                "table": "notes",
                "new_columns": [
                  "note_type TEXT DEFAULT 'user_created' CHECK (note_type IN ('user_created', 'ide_synced', 'agent_generated', 'import', 'template'))",
                  "source_metadata JSONB DEFAULT '{}'::jsonb (stores type-specific metadata)"
                ],
                "index": "CREATE INDEX idx_notes_type ON notes(user_id, note_type);"
              },
              "note_types": {
                "user_created": {
                  "description": "Note created directly in Noted app",
                  "source_metadata": {}
                },
                "ide_synced": {
                  "description": "Note synced from IDE",
                  "source_metadata": {
                    "ide_file_path": "docs/architecture.md",
                    "original_file_size": 5000,
                    "file_permissions": "0644",
                    "file_author": "user@example.com"
                  }
                },
                "agent_generated": {
                  "description": "Note created by AI agent",
                  "source_metadata": {
                    "agent_name": "Claude",
                    "agent_version": "claude-3-opus",
                    "generation_prompt": "Document the authentication flow",
                    "confidence_score": 0.95
                  }
                },
                "import": {
                  "description": "Note imported from external source",
                  "source_metadata": {
                    "import_source": "Notion",
                    "import_date": "2025-10-09",
                    "original_url": "https://notion.so/page-123"
                  }
                },
                "template": {
                  "description": "Template note for quick creation",
                  "source_metadata": {
                    "template_category": "meeting_notes",
                    "template_author": "admin"
                  }
                }
              },
              "type_specific_behavior": {
                "rendering": "Agent-generated notes show AI badge, IDE-synced notes show folder icon",
                "editing": "Template notes prompt 'Use this template?' before editing",
                "deletion": "Agent-generated notes require confirmation: 'This note was AI-generated. Delete anyway?'",
                "search": "Filter by type: 'Show me all agent-generated notes'"
              },
              "api_changes": {
                "create_request": "Include note_type and source_metadata in POST /ide-sync/create",
                "list_response": "Include note_type in GET /ide-sync/list for client-side filtering",
                "analytics": "Track metrics per note type (e.g., 'Agent generates 30% of notes')"
              }
            },
            "guarantees": "Enable type-specific UI, analytics, and workflows based on note origin"
          },
          "12_file_metadata_preservation": {
            "problem": "Original file mtime, author, permissions lost during sync - cannot track file provenance",
            "solution": "Store file metadata in JSONB column for attribution and forensics",
            "implementation": {
              "database_schema": {
                "table": "notes",
                "new_column": "file_metadata JSONB DEFAULT '{}'::jsonb",
                "example_data": {
                  "original_mtime": "2025-10-09T10:30:00Z",
                  "file_size_bytes": 5000,
                  "file_permissions": "0644",
                  "author_name": "John Doe",
                  "author_email": "john@example.com",
                  "git_commit_hash": "a1b2c3d4e5f6",
                  "git_branch": "main",
                  "ide_version": "VS Code 1.85.0",
                  "platform": "darwin"
                }
              },
              "client_collection": {
                "vscode_extension": [
                  "1. Read file stats: fs.stat(filePath)",
                  "2. Extract metadata: {mtime, size, mode}",
                  "3. Get git info: git log -1 --format='%H %an %ae' {filePath}",
                  "4. Get VS Code version: vscode.version",
                  "5. Get platform: os.platform()",
                  "6. Send in request: {file_metadata: {...}}"
                ]
              },
              "use_cases": {
                "attribution": "Show 'Last modified by John Doe on 2025-10-09' in note UI",
                "forensics": "Investigate data breach: 'Which files were modified between 10am-11am on 2025-10-09?'",
                "compliance": "Audit trail: 'Who accessed customer_data.md?' -> check file_metadata.author_email",
                "sync_optimization": "Compare file_size_bytes before syncing, skip if size unchanged (early optimization)"
              },
              "privacy_considerations": {
                "pii_warning": "file_metadata may contain PII (author_email) - apply same encryption as note content",
                "user_control": "Settings option: 'Include author information in synced notes' (default: true)",
                "gdpr_compliance": "file_metadata included in data export for user data portability"
              }
            },
            "guarantees": "Full file provenance tracking, enables attribution and audit trails"
          }
        },
        "observability_infrastructure": {
          "overview": {
            "problem": "Production system needs monitoring, debugging, and performance visibility - blind operation leads to undetected failures",
            "approach": "Implement structured logging, metrics collection, synthetic monitoring, load testing, and alerting for operational excellence"
          },
          "1_structured_logging": {
            "problem": "Unstructured logs make debugging distributed systems difficult - can't filter by user_id, correlate requests, or measure latency",
            "solution": "JSON-formatted logs with consistent schema including correlation IDs, request context, and structured fields",
            "implementation": {
              "log_format": {
                "schema": "{\n  level: 'info' | 'warn' | 'error' | 'debug',\n  message: string,\n  context: {\n    user_id: string,\n    note_id?: string,\n    operation: string,\n    duration_ms?: number,\n    sync_origin_id?: string,\n    api_version?: string\n  },\n  timestamp: ISO8601,\n  correlation_id: string (X-Correlation-ID header or generated UUID)\n}",
                "example_log_entry": "{\n  \"level\": \"info\",\n  \"message\": \"IDE sync completed successfully\",\n  \"context\": {\n    \"user_id\": \"usr_abc123\",\n    \"note_id\": \"note_xyz789\",\n    \"operation\": \"ide_sync_update\",\n    \"duration_ms\": 245,\n    \"sync_origin_id\": \"vscode-ext-123\",\n    \"api_version\": \"v1\"\n  },\n  \"timestamp\": \"2025-10-09T10:30:05.123Z\",\n  \"correlation_id\": \"req_a1b2c3d4e5f6\"\n}"
              },
              "logging_library": {
                "recommendation": "Use Pino for Node.js (fast, low overhead) or Winston for more features",
                "configuration": "{\n  level: process.env.LOG_LEVEL || 'info',\n  format: 'json',\n  redactPaths: ['context.api_key', 'context.password'], // PII redaction\n  destination: process.env.LOG_DESTINATION || 'stdout'\n}"
              },
              "log_aggregation": {
                "options": [
                  "Supabase Edge Function logs (built-in, 7-day retention)",
                  "Datadog (APM + logs, expensive but comprehensive)",
                  "Axiom (affordable, 30-day retention)",
                  "LogTail (Betterstack, good UI)"
                ],
                "recommendation": "Start with Supabase logs, migrate to Axiom when log volume exceeds 1GB/month"
              },
              "key_operations_to_log": [
                "ide_sync_create (user creates note in IDE)",
                "ide_sync_update (user edits note in IDE)",
                "ide_sync_delete (user deletes note in IDE)",
                "ide_sync_poll (extension polls for changes)",
                "conflict_detected (three-way merge needed)",
                "rag_query (agent queries notes)",
                "embedding_generated (vector embedding created)",
                "api_error (any API failure with error code and stack trace)"
              ],
              "correlation_id_propagation": {
                "flow": [
                  "1. Client generates correlation_id (UUID) for each request chain",
                  "2. Client includes X-Correlation-ID header in all API calls",
                  "3. Server logs all operations with same correlation_id",
                  "4. Server propagates correlation_id to background jobs (embedding generation)",
                  "5. Can trace entire operation chain across services with single ID"
                ],
                "benefit": "Debug distributed operations by filtering logs with single correlation_id"
              }
            },
            "guarantees": "Full request traceability, filterable logs, easy debugging of distributed operations"
          },
          "2_metrics_collection": {
            "problem": "Can't optimize what you don't measure - need visibility into latency, error rates, and resource usage",
            "solution": "Collect and expose key metrics using Prometheus format or push to Datadog/Axiom",
            "implementation": {
              "metrics_to_track": {
                "sync_latency_ms": {
                  "type": "histogram",
                  "labels": ["operation (create|update|delete)", "sync_origin"],
                  "percentiles": ["p50", "p95", "p99"],
                  "target": "p95 < 500ms for updates, p95 < 200ms for creates"
                },
                "sync_success_rate": {
                  "type": "counter",
                  "labels": ["operation", "status (success|failure)", "error_code"],
                  "calculation": "success_count / (success_count + failure_count)",
                  "target": ">99.5% success rate"
                },
                "conflict_rate": {
                  "type": "counter",
                  "labels": ["resolution_strategy (auto|manual)"],
                  "calculation": "conflicts_detected / total_syncs",
                  "target": "<5% conflict rate"
                },
                "embedding_generation_latency_ms": {
                  "type": "histogram",
                  "labels": ["embedding_model"],
                  "target": "p95 < 2000ms"
                },
                "rag_query_latency_ms": {
                  "type": "histogram",
                  "labels": ["query_type (vector_only|hybrid)"],
                  "target": "p95 < 500ms"
                },
                "api_error_rate": {
                  "type": "counter",
                  "labels": ["endpoint", "error_code", "http_status"],
                  "target": "<0.5% error rate"
                },
                "database_query_duration_ms": {
                  "type": "histogram",
                  "labels": ["query_type (select|insert|update|delete)"],
                  "target": "p95 < 100ms"
                },
                "active_sync_origins": {
                  "type": "gauge",
                  "description": "Number of unique sync_origin_ids in last 5 minutes",
                  "target": "Track growth over time"
                }
              },
              "collection_approach": {
                "option_1_push_based": {
                  "method": "Push metrics from Edge Functions to Axiom/Datadog",
                  "pros": "Simple, no infrastructure needed",
                  "cons": "Adds latency to requests, costs per metric",
                  "code_example": "import { track } from '@axiom/js';\n\nawait track('sync_latency_ms', {\n  value: duration,\n  operation: 'update',\n  sync_origin: origin_id,\n  user_id: userId\n});"
                },
                "option_2_supabase_pg_stat": {
                  "method": "Query Supabase pg_stat_statements for DB metrics",
                  "pros": "Built-in, no extra cost",
                  "cons": "Only covers database layer",
                  "query": "SELECT query, mean_exec_time, calls FROM pg_stat_statements WHERE query LIKE '%notes%' ORDER BY mean_exec_time DESC LIMIT 10;"
                },
                "option_3_opentelemetry": {
                  "method": "Instrument code with OpenTelemetry SDK, export to any backend",
                  "pros": "Vendor-neutral, rich ecosystem, distributed tracing",
                  "cons": "More setup complexity",
                  "recommendation": "Phase 4 when scaling beyond single developer"
                }
              },
              "recommended_approach": "Start with Supabase pg_stat_statements + structured logging, migrate to OpenTelemetry when needed"
            },
            "guarantees": "Data-driven performance optimization, early detection of latency regressions"
          },
          "3_synthetic_monitoring": {
            "problem": "API could be down but you won't know until users report it - reactive vs proactive detection",
            "solution": "Automated health checks from external service every 5 minutes, alert on failures",
            "implementation": {
              "test_scenario": {
                "frequency": "Every 5 minutes",
                "test_flow": [
                  "1. Create test note via POST /ide-sync/create with idempotency key",
                  "2. Verify note appears in GET /ide-sync/list",
                  "3. Update note via PUT /ide-sync/update/{id}",
                  "4. Verify update succeeded",
                  "5. Delete note via DELETE /ide-sync/delete/{id}",
                  "6. Verify note marked as deleted",
                  "7. Cleanup (permanently delete after 30 days via tombstone GC)"
                ],
                "assertion_checks": [
                  "HTTP status 200/201 for all operations",
                  "Response time < 1000ms for each operation",
                  "Created note has correct content",
                  "Updated note reflects changes",
                  "Deleted note has deleted_at timestamp"
                ],
                "test_account": "Create dedicated service account with API key: svc_synthetic_monitor"
              },
              "monitoring_services": {
                "option_1_betterstack": {
                  "name": "Betterstack Uptime",
                  "pricing": "Free for 10 checks, $10/mo for 50 checks",
                  "features": ["HTTP checks", "Multi-region", "Incident timelines", "Status pages"],
                  "recommendation": "Best for simple HTTP checks"
                },
                "option_2_checkly": {
                  "name": "Checkly",
                  "pricing": "$7/mo for 10K check runs",
                  "features": ["Playwright-based E2E tests", "API checks", "Multi-step workflows"],
                  "recommendation": "Best for complex workflows"
                },
                "option_3_supabase_cron": {
                  "name": "Supabase Edge Function + pg_cron",
                  "pricing": "Free (included)",
                  "features": ["Custom logic", "No external dependency"],
                  "recommendation": "Best for budget-conscious self-hosted"
                }
              },
              "recommended_approach": "Start with Betterstack (free tier), migrate to Checkly when workflows get complex",
              "alert_conditions": {
                "trigger": "Alert if 2 consecutive checks fail (avoids false positives from transient network issues)",
                "channels": ["Email", "Discord webhook", "PagerDuty (if critical)"],
                "escalation": "Page on-call engineer if downtime exceeds 10 minutes"
              }
            },
            "guarantees": "Proactive downtime detection, <5 minute MTTD (mean time to detect), visibility into regional outages"
          },
          "4_load_testing": {
            "problem": "System may work fine with 1 user but fail with 100 concurrent users - capacity planning requires load testing",
            "solution": "Simulate realistic load with k6 or Artillery, identify bottlenecks before production traffic hits",
            "implementation": {
              "test_scenarios": {
                "scenario_1_baseline": {
                  "description": "100 concurrent users, each syncing 10 notes/min for 10 minutes",
                  "virtual_users": 100,
                  "duration": "10 minutes",
                  "operations": [
                    "70% reads (GET /ide-sync/list)",
                    "20% updates (PUT /ide-sync/update)",
                    "8% creates (POST /ide-sync/create)",
                    "2% deletes (DELETE /ide-sync/delete)"
                  ],
                  "expected_throughput": "100 users × 10 notes/min = 1000 notes/min = 16.7 notes/second",
                  "success_criteria": [
                    "p95 latency < 500ms",
                    "Error rate < 0.5%",
                    "Database connections < 80% of pool size (default 20)",
                    "CPU usage < 70%"
                  ]
                },
                "scenario_2_spike": {
                  "description": "Sudden traffic spike - 0 to 500 users in 1 minute",
                  "pattern": "Ramp from 0 to 500 users over 60 seconds, hold for 5 minutes",
                  "expected_behavior": [
                    "Rate limiting kicks in (429 errors expected)",
                    "Circuit breaker prevents cascade failures",
                    "System recovers after spike ends"
                  ],
                  "success_criteria": [
                    "p95 latency < 2000ms during spike",
                    "No database connection pool exhaustion",
                    "No memory leaks (memory returns to baseline after spike)"
                  ]
                },
                "scenario_3_endurance": {
                  "description": "Long-running test to detect memory leaks and resource exhaustion",
                  "virtual_users": 50,
                  "duration": "2 hours",
                  "success_criteria": [
                    "Memory usage stable (no upward trend)",
                    "Database connections stable",
                    "p95 latency does not degrade over time"
                  ]
                }
              },
              "load_testing_tools": {
                "option_1_k6": {
                  "name": "k6 (Grafana Labs)",
                  "pricing": "Open source (free), cloud offering available",
                  "strengths": ["JavaScript scripting", "Great metrics", "CI/CD integration"],
                  "example_script": "import http from 'k6/http';\nimport { check } from 'k6';\n\nexport let options = {\n  vus: 100,\n  duration: '10m',\n};\n\nexport default function () {\n  let res = http.post('https://api.noted.dev/v1/ide-sync/create', JSON.stringify({\n    content: 'Load test note',\n    idempotency_key: `loadtest-${__VU}-${__ITER}`\n  }), {\n    headers: { 'Content-Type': 'application/json', 'Authorization': 'Bearer sk-test' },\n  });\n  check(res, { 'status is 201': (r) => r.status === 201 });\n}"
                },
                "option_2_artillery": {
                  "name": "Artillery",
                  "pricing": "Open source (free)",
                  "strengths": ["YAML config (no coding)", "Distributed load testing", "Playwright support"],
                  "example_config": "config:\n  target: https://api.noted.dev\n  phases:\n    - duration: 600\n      arrivalRate: 10\n      name: Warm up\nscenarios:\n  - name: Create note\n    flow:\n      - post:\n          url: /v1/ide-sync/create\n          json:\n            content: Load test\n            idempotency_key: \"{{ $uuid }}\""
                }
              },
              "recommended_approach": "Use k6 for detailed performance analysis, run baseline test before each major release",
              "execution_strategy": {
                "frequency": [
                  "Baseline test: Before each major release",
                  "Spike test: Monthly",
                  "Endurance test: Quarterly"
                ],
                "environment": "Run against staging environment with production-like data (not production!)",
                "cleanup": "Delete all test notes after completion (use idempotency_key prefix 'loadtest-' for easy identification)"
              }
            },
            "guarantees": "Capacity planning based on data, early detection of performance regressions, confidence in system scalability"
          },
          "5_alerting_thresholds": {
            "problem": "Metrics are useless without actionable alerts - need to know when system is degraded",
            "solution": "Define alert rules with thresholds that trigger notifications to engineers",
            "implementation": {
              "alert_rules": {
                "critical_alerts": [
                  {
                    "alert_name": "APIDown",
                    "condition": "health_check_failures >= 2 consecutive failures",
                    "severity": "P1 (page immediately)",
                    "action": "Investigate API, check Supabase status, review recent deployments"
                  },
                  {
                    "alert_name": "HighErrorRate",
                    "condition": "api_error_rate > 5% for 5 minutes",
                    "severity": "P1 (page immediately)",
                    "action": "Check error logs for common error codes, investigate database connection issues"
                  },
                  {
                    "alert_name": "DatabaseDown",
                    "condition": "database_connection_failures > 0",
                    "severity": "P1 (page immediately)",
                    "action": "Check Supabase dashboard, verify connection pool settings"
                  }
                ],
                "warning_alerts": [
                  {
                    "alert_name": "HighLatency",
                    "condition": "sync_latency_p95 > 1000ms for 10 minutes",
                    "severity": "P2 (notify during business hours)",
                    "action": "Check database query performance (pg_stat_statements), investigate slow operations"
                  },
                  {
                    "alert_name": "HighConflictRate",
                    "condition": "conflict_rate > 10% for 30 minutes",
                    "severity": "P3 (review next day)",
                    "action": "Investigate conflict resolution logic, check if multiple users editing same notes"
                  },
                  {
                    "alert_name": "RateLimitTriggered",
                    "condition": "rate_limit_exceeded_count > 100 in 10 minutes",
                    "severity": "P3 (review next day)",
                    "action": "Check if legitimate traffic spike or potential abuse, review rate limit settings"
                  }
                ],
                "informational_alerts": [
                  {
                    "alert_name": "EmbeddingGenerationSlow",
                    "condition": "embedding_generation_latency_p95 > 5000ms for 30 minutes",
                    "severity": "P4 (informational)",
                    "action": "Check Voyage AI API status, consider batching embedding requests"
                  },
                  {
                    "alert_name": "DiskSpaceGrowth",
                    "condition": "database_size_growth > 10GB per day",
                    "severity": "P4 (informational)",
                    "action": "Review data retention policies, check if tombstone GC is running"
                  }
                ]
              },
              "alerting_platforms": {
                "option_1_betterstack": {
                  "name": "Betterstack Uptime",
                  "integration": "Built-in alerting for uptime checks",
                  "channels": ["Email", "SMS", "Slack", "Discord", "Webhook"]
                },
                "option_2_datadog": {
                  "name": "Datadog Alerts",
                  "integration": "Advanced alert conditions, anomaly detection",
                  "channels": ["Email", "PagerDuty", "Slack", "Webhook"]
                },
                "option_3_supabase_webhooks": {
                  "name": "Custom alerting via Edge Functions",
                  "integration": "Write custom alert logic, call Discord/Slack webhooks",
                  "channels": ["Discord webhook", "Slack webhook", "Email via SendGrid"]
                }
              },
              "recommended_approach": "Start with Betterstack for uptime alerts, add Supabase webhooks for custom metric alerts",
              "on_call_rotation": {
                "schedule": "Single developer: Self (no rotation)",
                "escalation_policy": [
                  "P1: Immediate notification (SMS + Email)",
                  "P2: Email during business hours",
                  "P3: Batched daily digest",
                  "P4: Weekly summary"
                ],
                "runbook_links": "Each alert includes link to runbook with investigation steps"
              }
            },
            "guarantees": "Proactive issue detection, prioritized response based on severity, reduced MTTR (mean time to resolution)"
          }
        },
        "rag_optimization": {
          "overview": {
            "problem": "Full note re-embedding on every sync is expensive and slow - need efficient vector search with minimal latency",
            "approach": "Implement incremental sync with hash-based change detection, async embedding queue, versioning, hybrid search, and smart chunking"
          },
          "1_incremental_sync": {
            "problem": "Regenerating embeddings for unchanged notes wastes API costs and time - need to detect content changes efficiently",
            "solution": "Store SHA-256 hash of note content, only generate embeddings when hash changes",
            "implementation": {
              "database_schema": {
                "table": "note_embeddings",
                "new_column": "content_hash TEXT NOT NULL (SHA-256 hash of note content at embedding time)",
                "index": "CREATE INDEX idx_note_embeddings_hash ON note_embeddings(note_id, content_hash);"
              },
              "sync_workflow": {
                "step_1_compute_hash": "current_hash = sha256(note.content)",
                "step_2_check_existing": "SELECT content_hash FROM note_embeddings WHERE note_id = ? ORDER BY created_at DESC LIMIT 1",
                "step_3_skip_if_unchanged": "IF current_hash == existing_hash THEN skip_embedding ELSE generate_new_embedding",
                "step_4_store_hash": "INSERT INTO note_embeddings (note_id, content_hash, embedding, ...) VALUES (?, current_hash, ?, ...)"
              },
              "hash_implementation": {
                "language": "JavaScript (Node.js)",
                "code": "import crypto from 'crypto';\n\nfunction computeContentHash(content: string): string {\n  return crypto.createHash('sha256').update(content, 'utf8').digest('hex');\n}\n\n// Usage\nconst hash = computeContentHash(note.content);\nconst existing = await db.query('SELECT content_hash FROM note_embeddings WHERE note_id = $1 ORDER BY created_at DESC LIMIT 1', [note.id]);\n\nif (existing?.content_hash === hash) {\n  console.log('Content unchanged, skipping embedding generation');\n  return existing.embedding;\n}\n\nconst embedding = await generateEmbedding(note.content);\nawait db.query('INSERT INTO note_embeddings (note_id, content_hash, embedding) VALUES ($1, $2, $3)', [note.id, hash, embedding]);"
              },
              "cost_savings": {
                "scenario": "User syncs 100 notes, only 5 changed",
                "without_hashing": "100 embedding API calls × $0.0001 = $0.01",
                "with_hashing": "5 embedding API calls × $0.0001 = $0.0005",
                "savings": "95% cost reduction for typical sync operations"
              },
              "edge_cases": {
                "whitespace_changes": "Hash includes whitespace - reformatting triggers re-embedding (acceptable tradeoff)",
                "encoding_issues": "Always use UTF-8 encoding for hash computation to ensure consistency",
                "hash_collisions": "SHA-256 collision probability is negligible (2^-256)"
              }
            },
            "guarantees": "Minimize embedding costs, faster sync operations, idempotent embedding generation"
          },
          "2_async_embedding_queue": {
            "problem": "Embedding generation blocks API response (2-5 seconds) - poor user experience, higher latency",
            "solution": "Queue embedding generation as background job, return API response immediately",
            "implementation": {
              "architecture": {
                "sync_endpoint": "POST /ide-sync/create returns 201 immediately after note created in DB (no embedding generated yet)",
                "background_queue": "Job queued with note_id and content_hash",
                "worker_process": "Separate process polls queue, generates embeddings asynchronously",
                "completion": "Note embedding marked as 'ready' after generation completes"
              },
              "database_schema": {
                "table": "embedding_jobs",
                "columns": [
                  "id UUID PRIMARY KEY DEFAULT gen_random_uuid()",
                  "note_id UUID REFERENCES notes(id) ON DELETE CASCADE",
                  "content_hash TEXT NOT NULL",
                  "status TEXT DEFAULT 'pending' CHECK (status IN ('pending', 'processing', 'completed', 'failed'))",
                  "attempts INTEGER DEFAULT 0",
                  "error_message TEXT",
                  "created_at TIMESTAMP DEFAULT NOW()",
                  "completed_at TIMESTAMP"
                ],
                "indexes": [
                  "CREATE INDEX idx_embedding_jobs_status ON embedding_jobs(status, created_at) WHERE status IN ('pending', 'failed');",
                  "CREATE INDEX idx_embedding_jobs_note ON embedding_jobs(note_id);"
                ]
              },
              "queue_implementation": {
                "option_1_pg_notify": {
                  "method": "PostgreSQL LISTEN/NOTIFY for real-time job notification",
                  "pros": "No external dependencies, low latency",
                  "cons": "Not durable (lost if worker crashes)",
                  "code": "-- Trigger on insert\nCREATE OR REPLACE FUNCTION notify_embedding_job()\nRETURNS TRIGGER AS $$\nBEGIN\n  PERFORM pg_notify('embedding_jobs', NEW.id::text);\n  RETURN NEW;\nEND;\n$$ LANGUAGE plpgsql;\n\nCREATE TRIGGER embedding_job_created\nAFTER INSERT ON embedding_jobs\nFOR EACH ROW EXECUTE FUNCTION notify_embedding_job();"
                },
                "option_2_supabase_functions": {
                  "method": "Supabase Edge Function invoked via Database Webhook",
                  "pros": "Serverless, auto-scaling",
                  "cons": "Cold start latency (~500ms)",
                  "setup": "Database Webhooks → embedding_jobs INSERT → https://your-project.supabase.co/functions/v1/process-embedding"
                },
                "option_3_polling": {
                  "method": "Worker polls embedding_jobs table every 5 seconds",
                  "pros": "Simple, reliable, durable",
                  "cons": "Higher latency (up to 5s delay)",
                  "query": "SELECT * FROM embedding_jobs WHERE status = 'pending' ORDER BY created_at ASC LIMIT 10 FOR UPDATE SKIP LOCKED"
                }
              },
              "recommended_approach": "Use polling (option 3) for simplicity and durability, migrate to pg_notify when latency becomes critical",
              "worker_logic": {
                "pseudocode": "while (true) {\n  const jobs = await db.query('SELECT * FROM embedding_jobs WHERE status = \\'pending\\' ORDER BY created_at ASC LIMIT 10 FOR UPDATE SKIP LOCKED');\n  \n  for (const job of jobs) {\n    try {\n      await db.query('UPDATE embedding_jobs SET status = \\'processing\\', attempts = attempts + 1 WHERE id = $1', [job.id]);\n      \n      const note = await db.query('SELECT content FROM notes WHERE id = $1', [job.note_id]);\n      const embedding = await generateEmbedding(note.content);\n      \n      await db.query('INSERT INTO note_embeddings (note_id, content_hash, embedding) VALUES ($1, $2, $3)', [job.note_id, job.content_hash, embedding]);\n      await db.query('UPDATE embedding_jobs SET status = \\'completed\\', completed_at = NOW() WHERE id = $1', [job.id]);\n    } catch (error) {\n      await db.query('UPDATE embedding_jobs SET status = \\'failed\\', error_message = $1 WHERE id = $2', [error.message, job.id]);\n      \n      if (job.attempts >= 3) {\n        console.error(`Embedding job ${job.id} failed after 3 attempts`);\n        // Alert developer\n      } else {\n        // Retry with exponential backoff\n        await sleep(Math.pow(2, job.attempts) * 1000);\n        await db.query('UPDATE embedding_jobs SET status = \\'pending\\' WHERE id = $1', [job.id]);\n      }\n    }\n  }\n  \n  await sleep(5000); // Poll every 5 seconds\n}"
              },
              "retry_strategy": {
                "max_attempts": 3,
                "backoff": "Exponential: 2s, 4s, 8s",
                "permanent_failure": "After 3 attempts, mark as 'failed', send alert to developer"
              },
              "monitoring": {
                "metrics": [
                  "embedding_queue_depth (gauge: pending jobs count)",
                  "embedding_job_latency_ms (histogram: created_at to completed_at)",
                  "embedding_job_failure_rate (counter: failed jobs / total jobs)"
                ],
                "alerts": [
                  "Queue depth > 100 for 10 minutes (worker may be down)",
                  "Job failure rate > 5% (API issues or invalid content)"
                ]
              }
            },
            "guarantees": "Fast API responses (<100ms), eventual consistency for RAG queries, automatic retry for transient failures"
          },
          "3_embedding_model_versioning": {
            "problem": "Upgrading embedding model (e.g., Voyage v2 -> v3) requires re-embedding all notes - need migration strategy",
            "solution": "Track embedding_model version in database, detect model changes, trigger bulk re-embedding",
            "implementation": {
              "database_schema": {
                "table": "note_embeddings",
                "new_column": "embedding_model TEXT DEFAULT 'voyage-2' NOT NULL",
                "composite_index": "CREATE INDEX idx_note_embeddings_model_note ON note_embeddings(note_id, embedding_model, created_at DESC);"
              },
              "query_modifications": {
                "current_model": "const EMBEDDING_MODEL = 'voyage-3'; // Update when changing models",
                "query_with_model": "SELECT embedding FROM note_embeddings WHERE note_id = $1 AND embedding_model = $2 ORDER BY created_at DESC LIMIT 1",
                "fallback_to_old_model": "If no embedding found with current model, fall back to older model (degraded accuracy but no query failure)"
              },
              "migration_workflow": {
                "step_1_deploy_new_model": "Update EMBEDDING_MODEL constant to 'voyage-3', deploy code",
                "step_2_new_notes_use_new_model": "All new/updated notes automatically use voyage-3",
                "step_3_background_migration": "Run background job to re-embed existing notes with old embeddings",
                "step_4_cleanup_old_embeddings": "After migration completes (weeks/months), delete embeddings with old model"
              },
              "background_migration_script": {
                "approach": "Batch process old embeddings in background without blocking production traffic",
                "query": "SELECT DISTINCT n.id, n.content \nFROM notes n\nLEFT JOIN note_embeddings ne ON n.id = ne.note_id AND ne.embedding_model = 'voyage-3'\nWHERE ne.id IS NULL  -- No embedding with new model\nAND n.deleted_at IS NULL\nLIMIT 100;",
                "pseudocode": "const OLD_MODEL = 'voyage-2';\nconst NEW_MODEL = 'voyage-3';\n\nwhile (true) {\n  const notes = await db.query(migrationQuery);\n  \n  if (notes.length === 0) {\n    console.log('Migration complete!');\n    break;\n  }\n  \n  for (const note of notes) {\n    const embedding = await generateEmbedding(note.content, NEW_MODEL);\n    await db.query('INSERT INTO note_embeddings (note_id, embedding, embedding_model, content_hash) VALUES ($1, $2, $3, $4)', \n      [note.id, embedding, NEW_MODEL, computeContentHash(note.content)]);\n    \n    console.log(`Migrated note ${note.id} to ${NEW_MODEL}`);\n  }\n  \n  await sleep(10000); // 10 second delay between batches to avoid rate limiting\n}",
                "rate_limiting": "Process 100 notes per batch with 10-second delay = 6 batches/min = 600 notes/min = 36,000 notes/hour"
              },
              "cost_estimation": {
                "scenario": "10,000 existing notes, migrating from voyage-2 to voyage-3",
                "cost": "10,000 notes × $0.0001 per embedding = $1.00",
                "duration": "10,000 notes ÷ 600 notes/hour = ~17 hours"
              },
              "graceful_degradation": {
                "query_strategy": "Try current model first, fall back to old model if not found",
                "code": "async function getEmbedding(noteId: string): Promise<number[] | null> {\n  const current = await db.query('SELECT embedding FROM note_embeddings WHERE note_id = $1 AND embedding_model = $2 ORDER BY created_at DESC LIMIT 1', [noteId, CURRENT_MODEL]);\n  \n  if (current) return current.embedding;\n  \n  // Fall back to old model\n  const fallback = await db.query('SELECT embedding FROM note_embeddings WHERE note_id = $1 ORDER BY created_at DESC LIMIT 1', [noteId]);\n  \n  if (fallback) {\n    console.warn(`Using fallback embedding with model ${fallback.embedding_model} for note ${noteId}`);\n    return fallback.embedding;\n  }\n  \n  return null; // No embedding exists\n}",
                "benefit": "Zero downtime migration - old embeddings continue working while new ones are generated"
              }
            },
            "guarantees": "Zero-downtime model upgrades, backward compatibility, gradual migration without service disruption"
          },
          "4_hybrid_search": {
            "problem": "Vector search alone misses exact keyword matches - 'PostgreSQL' query might miss note containing exact word",
            "solution": "Combine vector similarity search with PostgreSQL full-text search, rank results by weighted score",
            "implementation": {
              "database_setup": {
                "full_text_index": "ALTER TABLE notes ADD COLUMN content_tsv tsvector GENERATED ALWAYS AS (to_tsvector('english', content)) STORED;\nCREATE INDEX idx_notes_content_tsv ON notes USING GIN(content_tsv);",
                "benefit": "GIN index enables fast full-text search (<10ms for most queries)"
              },
              "search_query": {
                "vector_search": "SELECT note_id, 1 - (embedding <=> query_embedding) AS vector_score\nFROM note_embeddings\nWHERE embedding_model = 'voyage-3'\nORDER BY embedding <=> query_embedding\nLIMIT 50;",
                "full_text_search": "SELECT id AS note_id, ts_rank(content_tsv, websearch_to_tsquery('english', $1)) AS text_score\nFROM notes\nWHERE content_tsv @@ websearch_to_tsquery('english', $1)\nORDER BY text_score DESC\nLIMIT 50;",
                "hybrid_combine": "WITH vector_results AS (\n  SELECT note_id, 1 - (embedding <=> $1) AS vector_score\n  FROM note_embeddings\n  WHERE embedding_model = 'voyage-3'\n  ORDER BY embedding <=> $1\n  LIMIT 50\n),\ntext_results AS (\n  SELECT id AS note_id, ts_rank(content_tsv, websearch_to_tsquery('english', $2)) AS text_score\n  FROM notes\n  WHERE content_tsv @@ websearch_to_tsquery('english', $2)\n  ORDER BY text_score DESC\n  LIMIT 50\n)\nSELECT \n  COALESCE(v.note_id, t.note_id) AS note_id,\n  COALESCE(v.vector_score, 0) * 0.7 + COALESCE(t.text_score, 0) * 0.3 AS hybrid_score\nFROM vector_results v\nFULL OUTER JOIN text_results t ON v.note_id = t.note_id\nORDER BY hybrid_score DESC\nLIMIT 20;"
              },
              "weight_tuning": {
                "default_weights": "70% vector, 30% full-text",
                "rationale": "Vector search captures semantic meaning, full-text ensures exact keyword matches aren't missed",
                "tuning_approach": "A/B test different weights with real queries, measure relevance with user feedback"
              },
              "query_examples": {
                "example_1_exact_match": {
                  "query": "PostgreSQL connection pooling",
                  "vector_alone": "May return generic database docs",
                  "full_text_alone": "Misses semantically related 'Postgres pool management'",
                  "hybrid": "Returns both exact 'PostgreSQL' matches AND semantically similar content"
                },
                "example_2_semantic": {
                  "query": "How do I speed up my API?",
                  "vector_alone": "Finds 'performance optimization', 'caching strategies'",
                  "full_text_alone": "Struggles with question phrasing",
                  "hybrid": "Best of both - exact keyword matches + semantic understanding"
                }
              },
              "performance_optimization": {
                "parallel_execution": "Run vector and full-text queries in parallel (Promise.all)",
                "caching": "Cache popular queries for 5 minutes (Redis)",
                "target_latency": "p95 < 500ms for hybrid search"
              }
            },
            "guarantees": "Better search relevance, no missed exact matches, handles both semantic and keyword queries"
          },
          "5_chunking_strategy": {
            "problem": "Embedding entire note loses granularity - can't pinpoint relevant section in 5000-word document",
            "solution": "Split notes into semantic chunks (by heading/section), embed each chunk separately, return specific sections",
            "implementation": {
              "chunking_algorithm": {
                "strategy": "Split by markdown headings (## Section Title), fallback to paragraph breaks if no headings",
                "heading_detection": "Use regex: /^#{1,6}\\s+(.+)$/gm to detect markdown headings",
                "paragraph_fallback": "If note has no headings, split by double newline (\\n\\n) into paragraphs",
                "chunk_size_limits": {
                  "min_chunk_size": "100 characters (avoid tiny chunks)",
                  "max_chunk_size": "2000 characters (voyager embedding limit is 8k tokens ≈ 32k chars)",
                  "overlap": "50 characters overlap between chunks to preserve context at boundaries"
                }
              },
              "database_schema": {
                "table": "note_embeddings",
                "new_columns": [
                  "chunk_index INTEGER DEFAULT 0 (0 = full note, 1+ = chunk number)",
                  "chunk_text TEXT (actual chunk content for display)",
                  "chunk_heading TEXT (section heading for this chunk)"
                ],
                "composite_index": "CREATE INDEX idx_note_embeddings_chunks ON note_embeddings(note_id, chunk_index, embedding_model);"
              },
              "chunking_implementation": {
                "code": "interface Chunk {\n  index: number;\n  heading: string | null;\n  text: string;\n}\n\nfunction chunkMarkdown(content: string): Chunk[] {\n  const chunks: Chunk[] = [];\n  const headingRegex = /^(#{1,6})\\s+(.+)$/gm;\n  \n  let match;\n  let lastIndex = 0;\n  let chunkIndex = 1;\n  \n  while ((match = headingRegex.exec(content)) !== null) {\n    const heading = match[2];\n    const headingStart = match.index;\n    \n    // Add previous section as chunk\n    if (lastIndex < headingStart) {\n      const sectionText = content.slice(lastIndex, headingStart).trim();\n      if (sectionText.length >= 100) {\n        chunks.push({ index: chunkIndex++, heading: null, text: sectionText });\n      }\n    }\n    \n    lastIndex = headingStart;\n  }\n  \n  // Add final section\n  const finalText = content.slice(lastIndex).trim();\n  if (finalText.length >= 100) {\n    chunks.push({ index: chunkIndex++, heading: null, text: finalText });\n  }\n  \n  // Fallback: if no chunks, embed entire note\n  if (chunks.length === 0) {\n    chunks.push({ index: 0, heading: null, text: content });\n  }\n  \n  return chunks;\n}",
                "usage": "const chunks = chunkMarkdown(note.content);\nfor (const chunk of chunks) {\n  const embedding = await generateEmbedding(chunk.text);\n  await db.query('INSERT INTO note_embeddings (note_id, chunk_index, chunk_text, chunk_heading, embedding, embedding_model) VALUES ($1, $2, $3, $4, $5, $6)',\n    [note.id, chunk.index, chunk.text, chunk.heading, embedding, EMBEDDING_MODEL]);\n}"
              },
              "query_modifications": {
                "search_all_chunks": "SELECT ne.note_id, ne.chunk_index, ne.chunk_text, ne.chunk_heading, 1 - (ne.embedding <=> $1) AS similarity\nFROM note_embeddings ne\nWHERE ne.embedding_model = 'voyage-3'\nORDER BY ne.embedding <=> $1\nLIMIT 20;",
                "display_results": "Show specific chunk with heading, provide link to jump to section in full note"
              },
              "agent_integration": {
                "benefit": "Agent can reference specific sections: 'According to the API Documentation → Authentication section...'",
                "context_window_savings": "Return only relevant 500-word chunk instead of entire 5000-word document (90% token savings)"
              },
              "tradeoffs": {
                "pros": [
                  "Pinpoint relevant sections",
                  "Better retrieval accuracy",
                  "Reduced context window usage for agent"
                ],
                "cons": [
                  "More embeddings to store (3-5x per note)",
                  "Higher embedding costs (3-5x)",
                  "More complex query logic"
                ],
                "recommendation": "Implement chunking only for notes >2000 characters, use full-note embedding for shorter notes"
              }
            },
            "guarantees": "Section-level retrieval accuracy, reduced agent token usage, better citation quality"
          }
        },
        "agent_improvements": {
          "overview": {
            "problem": "AI agents can generate nonsensical, hallucinated, or low-quality content - need quality gates and feedback loops",
            "approach": "Implement write validation rules and feedback tracking to improve agent output quality over time"
          },
          "1_write_validation": {
            "problem": "Agent might generate malformed markdown, duplicate content, or hallucinated information - need validation before saving",
            "solution": "Validation pipeline that checks content quality, rejects invalid writes, and provides feedback to agent",
            "implementation": {
              "validation_rules": {
                "rule_1_markdown_syntax": {
                  "check": "Parse markdown with remark/unified, ensure no syntax errors",
                  "invalid_examples": ["Unclosed code blocks (```)", "Malformed tables", "Invalid heading nesting"],
                  "action": "Reject write with error: 'Invalid markdown syntax at line X'"
                },
                "rule_2_minimum_content_length": {
                  "check": "Ensure note has at least 50 characters (non-whitespace)",
                  "rationale": "Prevent empty or trivial notes",
                  "action": "Reject write with error: 'Note content too short (minimum 50 characters)'"
                },
                "rule_3_duplicate_detection": {
                  "check": "Compute content hash (SHA-256), compare with existing notes",
                  "similarity_threshold": "If hash matches OR cosine similarity > 0.95 with existing note, flag as duplicate",
                  "action": "Reject write with warning: 'Similar note already exists: {note_title}'"
                },
                "rule_4_hallucination_detection": {
                  "check": "If agent claims to reference specific source, verify citation exists in RAG context",
                  "method": "Extract citations from agent output (e.g., [1], [source: X]), check if source was in RAG retrieval",
                  "action": "Flag suspicious citations, require agent to regenerate with accurate sources"
                },
                "rule_5_profanity_filter": {
                  "check": "Scan for inappropriate content (optional, configurable)",
                  "library": "Use profanity filter library (e.g., bad-words npm package)",
                  "action": "Reject write with error: 'Content contains inappropriate language'"
                },
                "rule_6_max_note_size": {
                  "check": "Ensure note is under 100KB (configurable)",
                  "rationale": "Prevent runaway generation, database bloat",
                  "action": "Reject write with error: 'Note exceeds maximum size (100KB)'"
                }
              },
              "validation_pipeline": {
                "step_1_pre_validation": "Before calling agent, set validation rules in prompt: 'Generate valid markdown, min 50 chars, no hallucinations'",
                "step_2_post_generation": "After agent generates content, run validation pipeline",
                "step_3_auto_retry": "If validation fails with recoverable error (e.g., syntax), ask agent to fix: 'Your output had invalid markdown at line 5. Please regenerate with valid syntax.'",
                "step_4_max_retries": "Allow 2 auto-retries, then fail permanently and log error",
                "step_5_logging": "Log all validation failures for analysis and prompt tuning"
              },
              "api_integration": {
                "endpoint": "POST /agent/write-note",
                "request_body": "{\n  note_id?: string (for updates),\n  content: string,\n  metadata: {\n    agent_model: string,\n    rag_context_ids: string[] (notes used for RAG),\n    generation_timestamp: ISO8601\n  }\n}",
                "validation_response_success": "{\n  status: 'success',\n  note_id: string,\n  validation_passed: true\n}",
                "validation_response_failure": "{\n  status: 'error',\n  error_code: 'VALIDATION_FAILED',\n  validation_errors: [\n    { rule: 'markdown_syntax', message: 'Unclosed code block at line 23' },\n    { rule: 'duplicate_detection', message: 'Similar note exists: API Documentation' }\n  ],\n  recoverable: true (agent can retry)\n}"
              },
              "database_tracking": {
                "table": "agent_write_attempts",
                "columns": [
                  "id UUID PRIMARY KEY",
                  "note_id UUID (null if validation failed before creation)",
                  "agent_model TEXT",
                  "content_preview TEXT (first 500 chars)",
                  "validation_status TEXT CHECK (status IN ('passed', 'failed', 'retried'))",
                  "validation_errors JSONB",
                  "rag_context_ids TEXT[]",
                  "created_at TIMESTAMP DEFAULT NOW()"
                ],
                "purpose": "Track validation failures to improve prompts and detect systemic issues"
              },
              "monitoring_metrics": {
                "agent_write_success_rate": "passed_writes / total_write_attempts (target: >95%)",
                "validation_failure_breakdown": "Count failures by rule type (e.g., 60% markdown_syntax, 30% duplicate, 10% hallucination)",
                "auto_retry_success_rate": "retried_and_passed / total_retries (target: >70%)"
              }
            },
            "guarantees": "Prevent low-quality agent writes, automatic error recovery, visibility into agent failure modes"
          },
          "2_feedback_loop": {
            "problem": "Agent makes same mistakes repeatedly - need feedback mechanism to learn from corrections",
            "solution": "Track user corrections to agent-generated notes, use corrections to improve prompts and fine-tuning data",
            "implementation": {
              "correction_tracking": {
                "database_schema": {
                  "table": "agent_note_corrections",
                  "columns": [
                    "id UUID PRIMARY KEY",
                    "note_id UUID REFERENCES notes(id) ON DELETE CASCADE",
                    "original_content TEXT (agent-generated content)",
                    "corrected_content TEXT (user-edited content)",
                    "correction_type TEXT CHECK (type IN ('syntax_fix', 'factual_correction', 'style_improvement', 'deletion', 'major_rewrite'))",
                    "edit_distance INTEGER (Levenshtein distance between original and corrected)",
                    "user_id UUID REFERENCES users(id)",
                    "agent_model TEXT (model that generated original)",
                    "corrected_at TIMESTAMP DEFAULT NOW()"
                  ],
                  "indexes": [
                    "CREATE INDEX idx_corrections_note ON agent_note_corrections(note_id);",
                    "CREATE INDEX idx_corrections_type ON agent_note_corrections(correction_type, corrected_at);"
                  ]
                },
                "correction_detection": {
                  "trigger": "When user edits agent-generated note (note_type = 'agent_generated'), capture before/after",
                  "classification_logic": {
                    "syntax_fix": "Edit distance < 50 AND mostly punctuation/formatting changes",
                    "factual_correction": "Edit distance 50-200 AND content semantic meaning changed (detected via embedding comparison)",
                    "style_improvement": "Edit distance > 200 AND semantic meaning preserved (embedding similarity > 0.9)",
                    "deletion": "User deleted entire note within 5 minutes of generation",
                    "major_rewrite": "Edit distance > 500 OR embedding similarity < 0.7"
                  },
                  "capture_workflow": "ON UPDATE notes WHERE note_type = 'agent_generated'\n  INSERT INTO agent_note_corrections (note_id, original_content, corrected_content, correction_type, edit_distance, ...)\n  SELECT ...\n  FROM old_version, new_version;"
                }
              },
              "feedback_analysis": {
                "aggregate_metrics": {
                  "correction_rate": "corrected_notes / total_agent_notes (target: <20%)",
                  "correction_type_distribution": "Breakdown: 40% style, 30% syntax, 20% factual, 10% deletion",
                  "high_correction_patterns": "Which types of notes get corrected most? (e.g., technical docs vs. summaries)"
                },
                "prompt_improvement": {
                  "method": "Periodically review top 50 corrections, identify common issues",
                  "example_finding": "Agent often generates incorrect code syntax in examples → update prompt: 'Verify all code examples are syntactically correct'",
                  "feedback_cycle": "Monthly review of corrections → update prompts → measure correction rate improvement"
                }
              },
              "fine_tuning_dataset": {
                "approach": "Export corrections as fine-tuning examples for future model training",
                "export_query": "SELECT \n  original_content AS input,\n  corrected_content AS expected_output,\n  correction_type AS label\nFROM agent_note_corrections\nWHERE correction_type IN ('factual_correction', 'style_improvement')\nAND edit_distance > 50\nLIMIT 1000;",
                "format": "JSONL format for model fine-tuning:\n{\n  \"messages\": [\n    { \"role\": \"system\", \"content\": \"Generate accurate documentation notes\" },\n    { \"role\": \"user\", \"content\": \"Write a note about...\" },\n    { \"role\": \"assistant\", \"content\": \"{corrected_content}\" }\n  ]\n}",
                "usage": "Feed to Claude fine-tuning API or GPT fine-tuning once dataset reaches 500+ examples"
              },
              "user_feedback_ui": {
                "explicit_feedback": {
                  "thumbs_up_down": "After agent generates note, show: '👍 Helpful | 👎 Not helpful'",
                  "feedback_reasons": "If 👎, ask: 'What was wrong? [Inaccurate] [Wrong format] [Unhelpful] [Too long]'",
                  "storage": "Store feedback in agent_note_corrections table with correction_type = 'user_dislike'"
                },
                "implicit_feedback": {
                  "immediate_deletion": "If user deletes agent note within 1 minute → strong negative signal",
                  "no_interaction": "If agent note never opened/edited → weak negative signal",
                  "frequent_reference": "If agent note viewed 10+ times → strong positive signal"
                }
              },
              "closed_loop_system": {
                "feedback_flow": [
                  "1. Agent generates note",
                  "2. User corrects or provides feedback",
                  "3. Correction logged in database",
                  "4. Weekly analysis identifies top issues",
                  "5. Prompts updated to address issues",
                  "6. New agent writes use improved prompts",
                  "7. Monitor correction rate improvement"
                ],
                "success_metrics": {
                  "phase_1_baseline": "Measure initial correction rate (week 1)",
                  "phase_2_iteration": "Update prompts based on feedback (weeks 2-4)",
                  "phase_3_measurement": "Measure correction rate improvement (week 5)",
                  "target": "Reduce correction rate by 30% after 1 month of feedback-driven iterations"
                }
              }
            },
            "guarantees": "Continuous agent quality improvement, data-driven prompt optimization, fine-tuning dataset for future model training"
          }
        },
        "code_quality_improvements": {
          "overview": {
            "problem": "Extension lacks validation, testing infrastructure, and maintainability patterns - risk of regressions and tech debt",
            "approach": "Implement schema validation, dependency injection for testing, config validation, error taxonomy, and state migration"
          },
          "1_markdown_validation": {
            "problem": "No validation of markdown content before sync - malformed markdown can break rendering in IDE or Noted app",
            "solution": "Use remark/unified to parse and validate markdown AST, define content schema, reject invalid markdown",
            "implementation": {
              "validation_library": {
                "library": "remark + unified ecosystem",
                "installation": "npm install remark remark-parse remark-stringify unified",
                "benefits": ["AST-based parsing", "Plugin ecosystem", "Supports CommonMark + GFM"]
              },
              "validation_pipeline": {
                "step_1_parse": "Parse markdown into AST using remark-parse",
                "step_2_validate": "Walk AST and validate structure (no malformed nodes)",
                "step_3_schema_check": "Optionally validate against content schema (e.g., notes must have heading)",
                "step_4_lint": "Run remark-lint for style issues",
                "code_example": "import { unified } from 'unified';\nimport remarkParse from 'remark-parse';\nimport remarkGfm from 'remark-gfm';\nimport remarkLint from 'remark-lint';\n\nasync function validateMarkdown(content: string): Promise<{ valid: boolean; errors: string[] }> {\n  const errors: string[] = [];\n  \n  try {\n    const file = await unified()\n      .use(remarkParse)\n      .use(remarkGfm) // GitHub Flavored Markdown\n      .use(remarkLint)\n      .process(content);\n    \n    // Check for parse errors\n    if (file.messages.length > 0) {\n      errors.push(...file.messages.map(m => `Line ${m.line}: ${m.message}`));\n    }\n    \n    return { valid: errors.length === 0, errors };\n  } catch (error) {\n    return { valid: false, errors: [`Parse error: ${error.message}`] };\n  }\n}\n\n// Usage\nconst result = await validateMarkdown(note.content);\nif (!result.valid) {\n  throw new ValidationError('Invalid markdown', result.errors);\n}"
              },
              "content_schema": {
                "optional_constraints": [
                  "Notes must start with h1 heading (# Title)",
                  "Code blocks must specify language (```js not just ```)",
                  "Links must be valid (no broken internal references)",
                  "Tables must have proper structure"
                ],
                "enforcement": "Define schema in config, validate before sync",
                "example_schema": "{\n  \"requireH1\": true,\n  \"requireCodeLanguage\": false,\n  \"maxHeadingDepth\": 6,\n  \"allowedElements\": [\"heading\", \"paragraph\", \"list\", \"code\", \"blockquote\", \"table\"]\n}"
              },
              "integration_points": {
                "before_ide_sync": "Validate markdown before calling POST /ide-sync/create",
                "before_agent_write": "Validate agent-generated content before saving",
                "manual_validation": "Expose 'Validate Markdown' command in VS Code for on-demand validation"
              },
              "performance": {
                "optimization": "Cache validation results by content hash (skip re-validation for unchanged content)",
                "target": "<50ms for validation of typical 5KB note"
              }
            },
            "guarantees": "Prevent malformed markdown from reaching database, consistent rendering across platforms, early error detection"
          },
          "2_dependency_injection": {
            "problem": "Extension tightly coupled to VS Code APIs - difficult to test, hard to mock dependencies",
            "solution": "Abstract VS Code APIs behind interfaces, inject dependencies, enable unit testing without VS Code runtime",
            "implementation": {
              "abstraction_interfaces": {
                "IFileSystem": "interface IFileSystem {\n  readFile(path: string): Promise<string>;\n  writeFile(path: string, content: string): Promise<void>;\n  deleteFile(path: string): Promise<void>;\n  watchFiles(pattern: string, callback: (uri: string) => void): Disposable;\n}",
                "IWorkspace": "interface IWorkspace {\n  getWorkspaceFolder(): string | undefined;\n  findFiles(pattern: string): Promise<string[]>;\n  onDidChangeFiles(callback: (event: FileChangeEvent) => void): Disposable;\n}",
                "INotifications": "interface INotifications {\n  showInfo(message: string): void;\n  showError(message: string): void;\n  showProgress<T>(title: string, task: () => Promise<T>): Promise<T>;\n}",
                "IApiClient": "interface IApiClient {\n  createNote(content: string): Promise<{ id: string }>;\n  updateNote(id: string, content: string): Promise<void>;\n  deleteNote(id: string): Promise<void>;\n  listNotes(): Promise<Note[]>;\n}"
              },
              "implementation_classes": {
                "VsCodeFileSystem": "class VsCodeFileSystem implements IFileSystem {\n  async readFile(path: string): Promise<string> {\n    const uri = vscode.Uri.file(path);\n    const bytes = await vscode.workspace.fs.readFile(uri);\n    return Buffer.from(bytes).toString('utf8');\n  }\n  // ... other methods\n}",
                "MockFileSystem": "class MockFileSystem implements IFileSystem {\n  private files = new Map<string, string>();\n  \n  async readFile(path: string): Promise<string> {\n    const content = this.files.get(path);\n    if (!content) throw new Error(`File not found: ${path}`);\n    return content;\n  }\n  \n  async writeFile(path: string, content: string): Promise<void> {\n    this.files.set(path, content);\n  }\n  // ... other methods\n}"
              },
              "dependency_injection_container": {
                "approach": "Use simple constructor injection (no complex DI framework needed)",
                "service_registration": "// services/ServiceContainer.ts\nexport class ServiceContainer {\n  constructor(\n    public fileSystem: IFileSystem,\n    public workspace: IWorkspace,\n    public notifications: INotifications,\n    public apiClient: IApiClient\n  ) {}\n  \n  static createProduction(): ServiceContainer {\n    return new ServiceContainer(\n      new VsCodeFileSystem(),\n      new VsCodeWorkspace(),\n      new VsCodeNotifications(),\n      new NotedApiClient()\n    );\n  }\n  \n  static createTest(): ServiceContainer {\n    return new ServiceContainer(\n      new MockFileSystem(),\n      new MockWorkspace(),\n      new MockNotifications(),\n      new MockApiClient()\n    );\n  }\n}",
                "usage_in_extension": "// extension.ts\nexport function activate(context: vscode.ExtensionContext) {\n  const services = ServiceContainer.createProduction();\n  const syncService = new SyncService(services);\n  \n  context.subscriptions.push(\n    vscode.commands.registerCommand('noted.sync', () => syncService.syncAll())\n  );\n}"
              },
              "testing_benefits": {
                "unit_tests": "Can test SyncService in isolation without VS Code runtime",
                "example_test": "// sync.test.ts\nimport { SyncService } from './sync';\nimport { ServiceContainer } from './ServiceContainer';\n\ntest('syncAll creates notes for all markdown files', async () => {\n  const services = ServiceContainer.createTest();\n  services.fileSystem.writeFile('/workspace/note1.md', '# Test Note');\n  services.fileSystem.writeFile('/workspace/note2.md', '# Another Note');\n  \n  const syncService = new SyncService(services);\n  await syncService.syncAll();\n  \n  const calls = (services.apiClient as MockApiClient).getCreateCalls();\n  expect(calls).toHaveLength(2);\n  expect(calls[0].content).toBe('# Test Note');\n});",
                "integration_tests": "Can test against real VS Code workspace using createProduction()"
              }
            },
            "guarantees": "Testable codebase, reduced coupling, easier mocking, faster unit tests without VS Code runtime"
          },
          "3_configuration_validation": {
            "problem": "Extension settings not validated - invalid config causes runtime errors, poor error messages",
            "solution": "Use Zod to define and validate configuration schema, provide clear error messages for invalid settings",
            "implementation": {
              "zod_library": {
                "installation": "npm install zod",
                "benefits": ["Type-safe validation", "Great error messages", "Schema inference", "Transform values"]
              },
              "config_schema": {
                "definition": "import { z } from 'zod';\n\nexport const NotedConfigSchema = z.object({\n  apiKey: z.string().min(20, 'API key must be at least 20 characters').startsWith('sk-', 'API key must start with sk-'),\n  apiUrl: z.string().url('API URL must be valid URL').default('https://api.noted.dev'),\n  syncInterval: z.number().int().min(5).max(300).default(10), // seconds\n  syncEnabled: z.boolean().default(true),\n  syncPattern: z.string().regex(/^\\*\\*\\/\\*\\.md$/).default('**/*.md'),\n  excludePatterns: z.array(z.string()).default(['**/node_modules/**', '**/.git/**']),\n  conflictResolution: z.enum(['ask', 'local-wins', 'remote-wins']).default('ask'),\n  logLevel: z.enum(['debug', 'info', 'warn', 'error']).default('info')\n});\n\nexport type NotedConfig = z.infer<typeof NotedConfigSchema>;",
                "validation_example": "function loadConfig(): NotedConfig {\n  const rawConfig = vscode.workspace.getConfiguration('noted');\n  \n  try {\n    const config = NotedConfigSchema.parse({\n      apiKey: rawConfig.get('apiKey'),\n      apiUrl: rawConfig.get('apiUrl'),\n      syncInterval: rawConfig.get('syncInterval'),\n      syncEnabled: rawConfig.get('syncEnabled'),\n      syncPattern: rawConfig.get('syncPattern'),\n      excludePatterns: rawConfig.get('excludePatterns'),\n      conflictResolution: rawConfig.get('conflictResolution'),\n      logLevel: rawConfig.get('logLevel')\n    });\n    \n    return config;\n  } catch (error) {\n    if (error instanceof z.ZodError) {\n      const messages = error.errors.map(e => `${e.path.join('.')}: ${e.message}`);\n      throw new ConfigurationError(`Invalid configuration:\\n${messages.join('\\n')}`);\n    }\n    throw error;\n  }\n}"
              },
              "runtime_validation": {
                "on_activation": "Validate config on extension activation, show error notification if invalid",
                "on_config_change": "Re-validate whenever user changes settings",
                "example": "vscode.workspace.onDidChangeConfiguration(event => {\n  if (event.affectsConfiguration('noted')) {\n    try {\n      const config = loadConfig();\n      vscode.window.showInformationMessage('Noted configuration updated');\n      syncService.updateConfig(config);\n    } catch (error) {\n      vscode.window.showErrorMessage(`Invalid configuration: ${error.message}`);\n    }\n  }\n});"
              },
              "default_values": {
                "approach": "Use Zod .default() for sensible defaults, reduce user config burden",
                "fallback_strategy": "If config completely invalid, fall back to safe defaults (sync disabled) instead of crashing"
              }
            },
            "guarantees": "Type-safe configuration, early validation, clear error messages, prevent runtime config errors"
          },
          "4_error_classification": {
            "problem": "All errors treated the same - transient network errors retry forever, permanent errors never succeed",
            "solution": "Classify errors into TransientError (retry) vs PermanentError (don't retry), implement smart retry logic",
            "implementation": {
              "error_hierarchy": {
                "base_class": "abstract class NotedError extends Error {\n  constructor(\n    message: string,\n    public code: string,\n    public context?: Record<string, any>\n  ) {\n    super(message);\n    this.name = this.constructor.name;\n  }\n  \n  abstract isRetryable(): boolean;\n}",
                "transient_error": "class TransientError extends NotedError {\n  constructor(message: string, code: string, context?: Record<string, any>) {\n    super(message, code, context);\n  }\n  \n  isRetryable(): boolean {\n    return true;\n  }\n}\n\n// Examples:\n// - NetworkError (ECONNREFUSED, timeout)\n// - RateLimitError (429 from API)\n// - ServiceUnavailableError (503 from API)\n// - DatabaseLockError (database temporarily locked)",
                "permanent_error": "class PermanentError extends NotedError {\n  constructor(message: string, code: string, context?: Record<string, any>) {\n    super(message, code, context);\n  }\n  \n  isRetryable(): boolean {\n    return false;\n  }\n}\n\n// Examples:\n// - AuthenticationError (401, invalid API key)\n// - ValidationError (400, malformed request)\n// - NotFoundError (404, resource doesn't exist)\n// - PermissionError (403, insufficient permissions)\n// - QuotaExceededError (user over resource limit)"
              },
              "error_classification_logic": {
                "http_status_codes": "function classifyHttpError(status: number, body: any): NotedError {\n  const retryableStatuses = [408, 429, 502, 503, 504];\n  const permanentStatuses = [400, 401, 403, 404];\n  \n  if (retryableStatuses.includes(status)) {\n    return new TransientError(`HTTP ${status}`, `HTTP_${status}`, { body });\n  }\n  \n  if (permanentStatuses.includes(status)) {\n    return new PermanentError(`HTTP ${status}`, `HTTP_${status}`, { body });\n  }\n  \n  // 500 errors could be either - default to transient for now\n  return new TransientError(`HTTP ${status}`, `HTTP_${status}`, { body });\n}",
                "network_errors": "function classifyNetworkError(error: any): NotedError {\n  const transientCodes = ['ECONNREFUSED', 'ETIMEDOUT', 'ECONNRESET', 'EHOSTUNREACH'];\n  \n  if (transientCodes.includes(error.code)) {\n    return new TransientError('Network error', error.code, { originalError: error.message });\n  }\n  \n  // Unknown network error - default to transient\n  return new TransientError('Unknown network error', 'NETWORK_ERROR', { originalError: error.message });\n}"
              },
              "retry_logic": {
                "exponential_backoff": "async function retryOperation<T>(\n  operation: () => Promise<T>,\n  maxAttempts: number = 3\n): Promise<T> {\n  let lastError: Error;\n  \n  for (let attempt = 1; attempt <= maxAttempts; attempt++) {\n    try {\n      return await operation();\n    } catch (error) {\n      lastError = error;\n      \n      // If error is not retryable, throw immediately\n      if (error instanceof NotedError && !error.isRetryable()) {\n        throw error;\n      }\n      \n      // If max attempts reached, throw\n      if (attempt >= maxAttempts) {\n        throw error;\n      }\n      \n      // Exponential backoff: 2^attempt seconds\n      const delayMs = Math.pow(2, attempt) * 1000;\n      console.log(`Attempt ${attempt} failed, retrying in ${delayMs}ms...`);\n      await sleep(delayMs);\n    }\n  }\n  \n  throw lastError!;\n}",
                "usage": "try {\n  await retryOperation(() => apiClient.createNote(content));\n} catch (error) {\n  if (error instanceof PermanentError) {\n    vscode.window.showErrorMessage(`Failed to create note: ${error.message}`);\n  } else {\n    vscode.window.showErrorMessage(`Failed after 3 retries: ${error.message}`);\n  }\n}"
              },
              "error_reporting": {
                "user_notifications": "Show user-friendly messages based on error type",
                "logging": "Log error details (code, context, stack trace) for debugging",
                "telemetry": "Optionally report error rates to monitoring service (grouped by error code)"
              }
            },
            "guarantees": "Smart retry logic, clear error categories, prevent infinite retry loops, better error messages"
          },
          "5_state_migration": {
            "problem": "Extension state changes between versions - old state format breaks new code, no migration path",
            "solution": "Version extension state, implement migration functions for state schema changes",
            "implementation": {
              "state_versioning": {
                "state_schema": "interface ExtensionState {\n  version: number; // Current schema version\n  lastSyncTimestamp: string | null;\n  syncedNotes: Map<string, SyncedNote>;\n  syncOriginId: string;\n  conflictResolutionHistory: ConflictResolution[];\n}",
                "current_version": "const CURRENT_STATE_VERSION = 3;",
                "version_history": "// Version 1: Initial release\n// Version 2: Added conflictResolutionHistory\n// Version 3: Changed syncedNotes from array to map for performance"
              },
              "migration_functions": {
                "migration_registry": "const migrations: Record<number, (oldState: any) => any> = {\n  1: (state) => {\n    // Migration from v1 to v2: Add conflictResolutionHistory\n    return {\n      ...state,\n      version: 2,\n      conflictResolutionHistory: []\n    };\n  },\n  2: (state) => {\n    // Migration from v2 to v3: Convert syncedNotes array to map\n    const notesMap = new Map(state.syncedNotes.map((note: SyncedNote) => [note.id, note]));\n    return {\n      ...state,\n      version: 3,\n      syncedNotes: notesMap\n    };\n  }\n};",
                "migration_runner": "function migrateState(storedState: any): ExtensionState {\n  let state = storedState;\n  const startVersion = state.version || 1;\n  \n  // Apply migrations sequentially from stored version to current\n  for (let version = startVersion; version < CURRENT_STATE_VERSION; version++) {\n    const migrate = migrations[version];\n    if (migrate) {\n      console.log(`Migrating state from v${version} to v${version + 1}`);\n      state = migrate(state);\n    } else {\n      throw new Error(`Missing migration function for version ${version}`);\n    }\n  }\n  \n  return state;\n}"
              },
              "state_persistence": {
                "load_state": "function loadState(context: vscode.ExtensionContext): ExtensionState {\n  const storedState = context.globalState.get('notedState');\n  \n  if (!storedState) {\n    // First run - create initial state\n    return {\n      version: CURRENT_STATE_VERSION,\n      lastSyncTimestamp: null,\n      syncedNotes: new Map(),\n      syncOriginId: generateOriginId(),\n      conflictResolutionHistory: []\n    };\n  }\n  \n  // Migrate state if needed\n  try {\n    return migrateState(storedState);\n  } catch (error) {\n    console.error('State migration failed, resetting state:', error);\n    vscode.window.showWarningMessage('Noted extension state was reset due to migration failure');\n    return createInitialState();\n  }\n}",
                "save_state": "async function saveState(context: vscode.ExtensionContext, state: ExtensionState): Promise<void> {\n  await context.globalState.update('notedState', state);\n}"
              },
              "testing_migrations": {
                "test_approach": "Create fixtures with state from each version, verify migration succeeds",
                "example_test": "test('migration from v2 to v3 converts array to map', () => {\n  const v2State = {\n    version: 2,\n    syncedNotes: [\n      { id: 'note1', title: 'Test' },\n      { id: 'note2', title: 'Another' }\n    ],\n    conflictResolutionHistory: []\n  };\n  \n  const migrated = migrateState(v2State);\n  \n  expect(migrated.version).toBe(3);\n  expect(migrated.syncedNotes).toBeInstanceOf(Map);\n  expect(migrated.syncedNotes.size).toBe(2);\n  expect(migrated.syncedNotes.get('note1')?.title).toBe('Test');\n});"
              },
              "rollback_strategy": {
                "backup_before_migration": "Before migrating, create backup: context.globalState.update('notedState.backup', storedState)",
                "manual_rollback": "Provide 'Noted: Reset State' command for users to manually reset if migration causes issues"
              }
            },
            "guarantees": "Smooth version upgrades, backward compatibility, no data loss during migration, fallback to clean state if migration fails"
          }
        }
      }
    },

    "phase_3_two_way_sync": {
      "description": "Enable Noted -> IDE sync (agent writes, developer sees in IDE)",
      "duration": "6-8 hours",
      "deliverables": [
        "IDE extension polls Noted API for updates",
        "New notes created in Noted appear in IDE",
        "Note edits in Noted update IDE files",
        "Conflict detection and resolution UI",
        "Merge conflict markers (similar to git)"
      ],
      "technical_details": {
        "polling_strategy": {
          "method": "Poll Noted API every 10 seconds for updates",
          "endpoint": "GET /ide-sync/changes?since={timestamp}",
          "optimization": "Only fetch notes modified after last sync",
          "fallback": "WebSocket for real-time updates (phase 4)"
        },
        "conflict_detection_and_resolution": {
          "scenario": "Note edited in both IDE and Noted app simultaneously",
          "detection_strategy": {
            "mechanism": "Three-way timestamp comparison",
            "timestamps_compared": [
              "last_ide_sync_at (when IDE last synced this note)",
              "ide_last_modified (when file was last modified in IDE)",
              "updated_at (when note was last updated in Noted app)"
            ],
            "conflict_conditions": [
              "Condition 1: ide_last_modified > last_ide_sync_at (IDE has changes)",
              "Condition 2: updated_at > last_ide_sync_at (Noted has changes)",
              "Condition 3: Both conditions true = CONFLICT"
            ],
            "no_conflict_scenarios": [
              "If only IDE changed: Auto-sync IDE -> Noted (no conflict)",
              "If only Noted changed: Auto-sync Noted -> IDE (no conflict)",
              "If neither changed: No action needed"
            ]
          },
          "conflict_resolution_workflow": {
            "step_1_detection": {
              "when": "IDE extension polls for updates every 10 seconds",
              "action": "Extension compares timestamps and detects conflict",
              "notification": "Show VS Code warning notification: '⚠️ Conflict detected in architecture.md'"
            },
            "step_2_prevent_data_loss": {
              "action": "Create backup of both versions before any resolution",
              "ide_backup": "Save current IDE file to .noted-conflicts/architecture.md.ide.backup",
              "noted_backup": "Save Noted version to .noted-conflicts/architecture.md.noted.backup",
              "versioning": "Each conflict creates timestamped backups (e.g., architecture.md.conflict.2025-10-09T11-45-30Z)"
            },
            "step_3_present_to_user": {
              "ui_component": "VS Code conflict resolution panel",
              "panel_content": {
                "title": "Sync Conflict: architecture.md",
                "description": "This note was modified in both VS Code and the Noted app. Choose how to resolve:",
                "metadata_display": {
                  "ide_version": "Modified in VS Code: 2025-10-09 11:45 AM (5 minutes ago)",
                  "noted_version": "Modified in Noted app: 2025-10-09 11:50 AM (just now)",
                  "last_synced": "Last synced: 2025-10-09 11:30 AM"
                },
                "diff_preview": "Side-by-side diff showing changes in both versions (similar to git merge conflicts)"
              }
            },
            "step_4_resolution_options": {
              "option_1_keep_ide": {
                "label": "Keep IDE Version",
                "description": "Overwrite Noted app version with your IDE changes",
                "action": "POST /ide-sync/update/{id} with IDE content + force_overwrite: true",
                "result": "Noted app version is replaced, no data loss (Noted version backed up)"
              },
              "option_2_keep_noted": {
                "label": "Keep Noted App Version",
                "description": "Overwrite IDE file with Noted app changes",
                "action": "Update IDE file with Noted content",
                "result": "IDE version is replaced, no data loss (IDE version backed up)"
              },
              "option_3_manual_merge": {
                "label": "Merge Manually",
                "description": "Open both versions in diff editor and merge changes yourself",
                "action": "Open VS Code 3-way merge editor",
                "editor_layout": {
                  "left_pane": "IDE version (your local changes)",
                  "center_pane": "Base version (last synced state)",
                  "right_pane": "Noted version (app changes)",
                  "bottom_pane": "Merged result (editable)"
                },
                "workflow": [
                  "1. User reviews changes in both versions",
                  "2. User edits bottom pane to create merged version",
                  "3. User clicks 'Accept Merge' button",
                  "4. Extension saves merged version to IDE",
                  "5. Extension syncs merged version to Noted app"
                ]
              },
              "option_4_create_separate_notes": {
                "label": "Keep Both as Separate Notes",
                "description": "Create two separate notes in Noted app",
                "action": "Rename IDE version to 'architecture.md (IDE)' and keep Noted version as 'architecture.md (Noted)'",
                "result": "Two notes exist in Noted app, user can manually reconcile later"
              },
              "option_5_defer": {
                "label": "Resolve Later",
                "description": "Don't sync this note until you manually resolve the conflict",
                "action": "Mark note as 'conflict-deferred' in extension state",
                "result": "Note remains in conflict state, shown in sidebar with ⚠️ icon",
                "reminder": "Extension shows reminder every hour until resolved"
              }
            },
            "step_5_post_resolution": {
              "update_timestamps": "Set last_ide_sync_at = NOW() after successful resolution",
              "clear_conflict_state": "Remove conflict markers and backups (optional)",
              "log_resolution": "Log conflict resolution decision to .noted-sync-log for audit trail",
              "user_confirmation": "Show success toast: '✓ Conflict resolved for architecture.md'"
            }
          },
          "versioning_strategy": {
            "approach": "Event-sourced conflict log",
            "conflict_log_table": {
              "table": "sync_conflicts",
              "columns": [
                "id UUID PRIMARY KEY",
                "note_id UUID REFERENCES notes(id)",
                "user_id UUID REFERENCES users(id)",
                "ide_version_content TEXT",
                "noted_version_content TEXT",
                "resolution_strategy TEXT ('keep_ide' | 'keep_noted' | 'manual_merge' | 'create_both' | 'deferred')",
                "merged_content TEXT (if manual merge)",
                "detected_at TIMESTAMP",
                "resolved_at TIMESTAMP",
                "resolved_by TEXT ('user' | 'auto')"
              ]
            },
            "retention_policy": "Keep conflict logs for 30 days, then archive to cold storage",
            "recovery": "User can view past conflicts and restore previous versions from logs"
          },
          "automatic_conflict_prevention": {
            "optimistic_locking": {
              "mechanism": "Include last_known_updated_at in update requests",
              "api_change": "PUT /ide-sync/update/{id} includes last_known_updated_at field",
              "server_validation": "Server returns 409 Conflict if last_known_updated_at != current updated_at",
              "benefit": "Prevents accidental overwrites before they happen"
            },
            "file_watcher_debouncing": {
              "mechanism": "Wait 5 seconds after last file change before syncing",
              "rationale": "Avoid syncing every keystroke, wait for user to finish editing",
              "implementation": "Debounce file watcher events using setTimeout"
            },
            "user_intent_detection": {
              "mechanism": "Only sync when user actively working in IDE (not idle)",
              "detection": "Check if VS Code window has focus and file is actively edited",
              "benefit": "Avoid conflicts when user makes quick edit in Noted app while IDE is idle"
            }
          },
          "edge_cases": {
            "case_1_simultaneous_delete": {
              "scenario": "Note deleted in IDE, but edited in Noted app",
              "resolution": "Show dialog: 'This note was deleted in IDE but modified in Noted. Restore or keep deleted?'",
              "options": ["Restore note", "Keep deleted (move Noted version to trash)"]
            },
            "case_2_offline_conflicts": {
              "scenario": "User edits note in Noted app while offline, then comes online with IDE changes",
              "resolution": "Queue conflicts until user is online, then resolve using normal workflow"
            },
            "case_3_multiple_conflicts": {
              "scenario": "User has 10+ notes in conflict state",
              "resolution": "Show conflict resolution dashboard with batch actions ('Keep all IDE versions', 'Review one by one')"
            }
          },
          "ux_mockup": {
            "vs_code_notification": "⚠️ Conflict in architecture.md | [Review] [Keep IDE] [Keep Noted] [Dismiss]",
            "sidebar_conflict_indicator": "📄 architecture.md ⚠️ (conflict)",
            "diff_editor_commands": [
              "Accept Left (IDE)",
              "Accept Right (Noted)",
              "Accept Both",
              "Custom Edit"
            ]
          }
        },
        "file_updates": {
          "strategy": "Write to IDE file system using VS Code workspace API",
          "notification": "Show toast: 'architecture.md updated by Noted app'",
          "editor_behavior": "If file open in editor, show reload prompt"
        }
      }
    },

    "phase_4_agent_integration_layer": {
      "description": "Enable AI agents to read/write notes via API",
      "duration": "8-10 hours",
      "deliverables": [
        "Agent API client library (TypeScript/Python)",
        "Methods: queryNotes(), createNote(), updateNote()",
        "Integration with RAG system (query by semantic meaning)",
        "Agent authentication via service account or user delegation",
        "Usage examples and documentation"
      ],
      "technical_details": {
        "agent_api_client": {
          "file": "services/agent/noted-agent-client.ts",
          "methods": [
            "queryNotes(query: string, options?: QueryOptions): Promise<Note[]> - Semantic search via RAG",
            "createNote(title: string, content: string, metadata?: NoteMetadata): Promise<Note>",
            "updateNote(noteId: string, updates: Partial<Note>): Promise<Note>",
            "listProjectNotes(projectId?: string): Promise<Note[]>",
            "generateEmbedding(content: string): Promise<number[]> - For custom RAG implementations"
          ]
        },
        "rag_integration": {
          "requirement": "RAG system must be implemented (see rag-ask-your-notes-implementation.json)",
          "workflow": [
            "1. Agent calls queryNotes('how does authentication work?')",
            "2. Client generates embedding for query",
            "3. Client calls vector similarity search",
            "4. Client returns top K relevant notes",
            "5. Agent reads notes and formulates answer"
          ],
          "infrastructure_stack": {
            "embedding_provider": {
              "provider": "Voyage AI (voyage-3 model)",
              "rationale": "2x cheaper than OpenAI, specifically optimized for code/documentation",
              "dimensions": 1024,
              "cost_per_1k_tokens": "$0.00006",
              "latency": "100-200ms per embedding",
              "endpoint": "https://api.voyageai.com/v1/embeddings",
              "fallback": "Claude 3 Haiku (text-embedding-3-small equivalent)",
              "authentication": "User's Voyage AI API key stored in user_api_keys table"
            },
            "vector_database": {
              "database": "PostgreSQL with pgvector extension",
              "version": "pgvector 0.5.0+",
              "table_schema": {
                "table": "note_embeddings",
                "columns": [
                  "id UUID PRIMARY KEY",
                  "note_id UUID REFERENCES notes(id) ON DELETE CASCADE",
                  "embedding vector(1024) NOT NULL",
                  "content_hash TEXT NOT NULL",
                  "created_at TIMESTAMP DEFAULT NOW()",
                  "updated_at TIMESTAMP DEFAULT NOW()"
                ],
                "indexes": [
                  "CREATE INDEX idx_note_embeddings_vector ON note_embeddings USING ivfflat (embedding vector_cosine_ops) WITH (lists = 100);",
                  "CREATE INDEX idx_note_embeddings_note_id ON note_embeddings(note_id);",
                  "CREATE UNIQUE INDEX idx_note_embeddings_content_hash ON note_embeddings(content_hash);"
                ]
              },
              "index_configuration": {
                "algorithm": "IVFFlat (Inverted File with Flat Compression)",
                "distance_metric": "Cosine similarity",
                "lists_parameter": 100,
                "rationale": "Good balance for <100k notes. Use HNSW for >100k notes."
              }
            },
            "retrieval_strategy": {
              "similarity_search_query": "SELECT n.id, n.title, n.content, 1 - (e.embedding <=> $1) AS similarity FROM note_embeddings e JOIN notes n ON e.note_id = n.id WHERE n.user_id = $2 AND 1 - (e.embedding <=> $1) > $3 ORDER BY e.embedding <=> $1 LIMIT $4;",
              "parameters": {
                "k_value": "5 (top 5 results by default, configurable 1-20)",
                "similarity_threshold": "0.7 (only return results with >70% similarity)",
                "max_content_length": "2000 chars per note (truncate longer notes)",
                "total_context_budget": "8000 tokens max (for Claude context window)"
              },
              "optimization": {
                "caching": "Cache embeddings by content_hash to avoid regenerating",
                "incremental_updates": "Only regenerate embeddings when note content changes >20%",
                "lazy_indexing": "Generate embeddings async after note save, not blocking"
              }
            },
            "edge_function": {
              "file": "supabase/functions/rag-query/index.ts",
              "responsibilities": [
                "1. Receive query string from agent client",
                "2. Generate embedding for query using Voyage AI",
                "3. Execute vector similarity search in PostgreSQL",
                "4. Return top K results with similarity scores",
                "5. Log query for analytics (optional)"
              ],
              "endpoint": "POST /rag-query",
              "request_schema": {
                "query": "string (required, max 500 chars)",
                "k": "number (optional, default 5, max 20)",
                "similarity_threshold": "number (optional, default 0.7, range 0-1)",
                "filters": {
                  "folder_id": "uuid | null (optional)",
                  "synced_from_ide": "boolean (optional, filter IDE-synced notes only)",
                  "date_range": {
                    "start": "ISO 8601 timestamp (optional)",
                    "end": "ISO 8601 timestamp (optional)"
                  }
                }
              },
              "response_schema": {
                "success": "boolean",
                "results": [
                  {
                    "note_id": "uuid",
                    "title": "string",
                    "content": "string (truncated to 2000 chars)",
                    "similarity": "number (0-1)",
                    "ide_file_path": "string | null",
                    "created_at": "ISO 8601 timestamp"
                  }
                ],
                "query_time_ms": "number (latency measurement)"
              },
              "latency_targets": {
                "embedding_generation": "100-200ms",
                "vector_search": "50-150ms",
                "total_e2e": "<500ms p95"
              },
              "cost_per_query": {
                "embedding": "$0.00006 per query",
                "database": "~$0.00001 (negligible)",
                "total": "~$0.0001 per query"
              }
            },
            "embedding_generation_workflow": {
              "trigger": "Note created or updated in Noted app or via IDE sync",
              "async_processing": {
                "method": "Supabase Edge Function triggered by database webhook",
                "function": "supabase/functions/generate-embeddings/index.ts",
                "workflow": [
                  "1. Detect note insert/update via database trigger",
                  "2. Check if content_hash changed (avoid redundant embeddings)",
                  "3. If changed, call Voyage AI to generate embedding",
                  "4. Upsert embedding into note_embeddings table",
                  "5. Update content_hash"
                ],
                "database_trigger": "CREATE TRIGGER note_updated_trigger AFTER INSERT OR UPDATE ON notes FOR EACH ROW EXECUTE FUNCTION trigger_embedding_generation();"
              },
              "content_preprocessing": {
                "steps": [
                  "1. Extract plain text from markdown (strip formatting)",
                  "2. Truncate to first 8000 chars (Voyage AI limit)",
                  "3. Remove duplicate whitespace",
                  "4. Hash content for caching check"
                ]
              }
            },
            "migration_plan": {
              "step_1": "Enable pgvector extension: CREATE EXTENSION IF NOT EXISTS vector;",
              "step_2": "Create note_embeddings table with schema above",
              "step_3": "Create IVFFlat index (after generating embeddings for >1000 notes)",
              "step_4": "Backfill embeddings for existing notes (async batch job)",
              "estimated_backfill_time": "~1 second per note (100 notes = ~2 minutes)"
            }
          }
        },
        "authentication_strategies": {
          "option_1_user_delegation": {
            "description": "Agent acts on behalf of user",
            "flow": "User grants agent access to their Noted account via OAuth",
            "pros": ["Proper authorization", "Audit trail"],
            "cons": ["Complex setup", "Requires user action"]
          },
          "option_2_service_account": {
            "description": "Agent has its own Noted account",
            "flow": "Create special 'agent' user with API key",
            "pros": ["Simple", "No user action needed"],
            "cons": ["All agents share same account", "Less secure"]
          },
          "option_3_project_api_key": {
            "description": "Project-level API key (recommended)",
            "flow": "User generates project-specific API key in Noted, adds to .env",
            "pros": ["Project-scoped", "Easy to revoke", "No OAuth complexity"],
            "cons": ["Must be kept secret"]
          },
          "recommended": "Option 3 - Project API key"
        },
        "usage_example": {
          "language": "TypeScript",
          "code": "import { NotedAgentClient } from '@noted/agent-client';\n\n// Initialize client\nconst noted = new NotedAgentClient({\n  apiKey: process.env.NOTED_PROJECT_API_KEY,\n  projectId: 'my-project-123'\n});\n\n// Agent queries documentation\nconst docs = await noted.queryNotes('authentication architecture');\nconsole.log(`Found ${docs.length} relevant docs`);\n\n// Agent reads first doc\nconst authDoc = docs[0];\nconsole.log(authDoc.title); // 'Authentication Architecture'\nconsole.log(authDoc.content); // Full markdown content\n\n// Agent implements feature based on docs\n// ...\n\n// Agent documents implementation\nawait noted.createNote(\n  'Auth Implementation Notes',\n  '# Implementation\\n\\n- Used JWT tokens\\n- Session stored in Redis\\n...'\n);"
        }
      }
    },

    "phase_5_ide_sidebar_panel": {
      "description": "Add VS Code sidebar panel for managing synced notes",
      "duration": "6-8 hours",
      "deliverables": [
        "Custom sidebar view in VS Code",
        "Tree view of all synced notes",
        "Quick actions: open, sync, create new note",
        "Sync status indicators",
        "Search/filter notes"
      ],
      "technical_details": {
        "sidebar_ui": {
          "location": "VS Code Activity Bar (new icon)",
          "icon": "noted-logo or document icon",
          "view_content": [
            "📁 Synced Notes (folder)",
            "  📄 architecture.md (synced 2m ago)",
            "  📄 api-design.md (synced 5m ago)",
            "  ⚠️ database.md (conflict)",
            "➕ Create New Note",
            "🔄 Sync All",
            "⚙️ Settings"
          ]
        },
        "tree_view_implementation": {
          "class": "NotedTreeDataProvider implements vscode.TreeDataProvider",
          "methods": [
            "getTreeItem(element): TreeItem",
            "getChildren(element?): Note[]",
            "refresh(): void - Reload tree from API"
          ]
        },
        "context_menu_actions": [
          "Open in Editor",
          "Open in Noted App",
          "Copy Public URL (if published)",
          "Sync Now",
          "Resolve Conflict",
          "Remove from Sync"
        ]
      }
    },

    "phase_6_real_time_collaboration": {
      "description": "Replace polling with real-time WebSocket updates",
      "duration": "6-8 hours",
      "deliverables": [
        "WebSocket connection between IDE and Noted",
        "Live updates when agent writes new note",
        "Live updates when note edited in Noted app",
        "Operational transforms for simultaneous edits (advanced)",
        "Presence indicators (who's editing what)"
      ],
      "technical_details": {
        "websocket_implementation": {
          "server": "Supabase Realtime (built-in WebSocket support)",
          "client": "VS Code extension subscribes to note changes",
          "subscription": "supabase.channel('notes').on('postgres_changes', callback)"
        },
        "real_time_events": [
          "note_created - New note added by agent/app",
          "note_updated - Note content changed",
          "note_deleted - Note removed",
          "sync_conflict - Simultaneous edit detected"
        ],
        "operational_transforms": {
          "description": "Advanced conflict resolution for simultaneous edits",
          "library": "Use Yjs or Automerge for CRDT-based merging",
          "complexity": "High - consider for Phase 7 or later",
          "fallback": "Simple last-write-wins for MVP"
        }
      }
    }
  },

  "technical_architecture": {
    "components_overview": {
      "vs_code_extension": {
        "language": "TypeScript",
        "framework": "VS Code Extension API",
        "size": "~2000 lines of code",
        "dependencies": ["@types/vscode", "axios for API calls", "chokidar for file watching"]
      },
      "noted_backend": {
        "location": "Supabase Edge Functions",
        "language": "TypeScript (Deno runtime)",
        "new_functions": ["ide-sync", "agent-query"],
        "modifications": "Add ide_sync metadata to notes table"
      },
      "agent_client_library": {
        "languages": ["TypeScript (primary)", "Python (optional)"],
        "distribution": "npm package @noted/agent-client",
        "dependencies": ["axios", "openai (for embeddings)"]
      }
    },
    "data_flow_detailed": {
      "create_note_flow": [
        "1. Developer creates docs/auth.md in VS Code",
        "2. File watcher detects new file",
        "3. Extension reads file content",
        "4. Extension POST /ide-sync/create with content + metadata",
        "5. Edge Function validates API key",
        "6. Edge Function inserts note into notes table",
        "7. Edge Function generates embedding (calls generate-embedding)",
        "8. Edge Function returns note ID + sync status",
        "9. Extension stores mapping (file_path -> note_id)",
        "10. Status bar shows 'Synced ✓'"
      ],
      "agent_query_flow": [
        "1. User asks agent: 'How does auth work?'",
        "2. Agent calls noted.queryNotes('authentication')",
        "3. Client generates embedding for 'authentication'",
        "4. Client calls rag-query Edge Function",
        "5. Edge Function searches notes via vector similarity",
        "6. Edge Function returns top 5 relevant notes",
        "7. Agent reads notes and formulates answer",
        "8. Agent responds to user with info from notes"
      ],
      "agent_create_flow": [
        "1. Agent finishes implementing feature",
        "2. Agent calls noted.createNote('Impl Notes', content)",
        "3. Client POST /ide-sync/create",
        "4. Edge Function creates note with synced_from_ide=false",
        "5. WebSocket emits note_created event",
        "6. IDE extension receives event",
        "7. Extension writes new file to docs/impl-notes.md",
        "8. VS Code shows notification: 'New note from agent'"
      ]
    },
    "security_considerations": [
      "API keys stored in VS Code secure storage (not plain text)",
      "API keys scoped to project (not global user access)",
      "Rate limiting on all sync endpoints",
      "File path validation (prevent directory traversal)",
      "Content size limits (max 1MB per note)",
      "No execution of code from synced files"
    ],
    "performance_optimizations": [
      "Debounce file changes (500ms) to avoid excessive syncs",
      "Batch sync multiple file changes into single API call",
      "Cache note metadata locally (reduce API calls)",
      "Incremental sync (only changed files)",
      "Lazy load embeddings (don't block sync on embedding generation)"
    ]
  },

  "user_experience": {
    "developer_workflow": {
      "setup": [
        "1. Install VS Code extension from marketplace",
        "2. Open project in VS Code",
        "3. Extension prompts: 'Connect to Noted account'",
        "4. Developer generates API key in Noted app settings",
        "5. Paste API key into VS Code extension",
        "6. Select sync folder (defaults to /docs)",
        "7. Extension scans and uploads existing notes",
        "8. Status bar: 'Noted: 5 notes synced ✓'"
      ],
      "daily_use": [
        "Developer writes docs in /docs folder as normal",
        "Every save auto-syncs to Noted (no manual action)",
        "Status bar always shows sync status",
        "Sidebar panel shows list of synced notes",
        "If agent creates new note, notification appears",
        "Click notification to open new file in editor"
      ],
      "conflict_handling": [
        "Extension detects conflict (edited in both places)",
        "Shows VS Code diff editor with both versions",
        "Developer chooses: keep local, keep remote, or merge",
        "After resolution, extension syncs final version"
      ]
    },
    "agent_workflow": {
      "reading_docs": [
        "User asks agent a question",
        "Agent queries Noted via RAG",
        "Agent receives relevant documentation",
        "Agent formulates answer from docs",
        "Agent cites source notes in response"
      ],
      "writing_docs": [
        "Agent implements feature",
        "Agent documents implementation decisions",
        "Agent creates note via API",
        "Note syncs to IDE automatically",
        "Developer reviews and refines documentation"
      ]
    }
  },

  "extension_metadata": {
    "extension_name": "Noted Sync",
    "display_name": "Noted - AI Documentation Hub",
    "description": "Sync your project documentation to Noted and enable AI agents to read/write living docs",
    "publisher": "noted-app",
    "categories": ["Documentation", "AI", "Productivity"],
    "keywords": ["noted", "documentation", "ai", "rag", "sync", "markdown"]
  },

  "cost_analysis": {
    "development_costs": {
      "phase_1": "8-12 hours (VS Code extension MVP)",
      "phase_2": "4-6 hours (Noted API endpoints)",
      "phase_3": "6-8 hours (Two-way sync)",
      "phase_4": "8-10 hours (Agent integration)",
      "phase_5": "6-8 hours (Sidebar panel)",
      "phase_6": "6-8 hours (Real-time WebSocket)",
      "total": "38-52 hours for complete implementation"
    },
    "ongoing_costs": {
      "api_usage": "Minimal - same Supabase costs as normal app usage",
      "embeddings": "$0.00001 per note (one-time per note)",
      "rag_queries": "$0.00001 per query (agent queries)",
      "storage": "Same as normal notes (no additional cost)"
    },
    "user_cost_model": "Free to use (users already pay for Noted + their own AI API keys)"
  },

  "success_metrics": {
    "technical_performance": [
      "Sync reliability (% successful syncs)",
      "Average sync latency (target: < 1s)",
      "Conflict rate (% of syncs with conflicts)",
      "API response time (target: < 500ms)"
    ],
    "functionality": [
      "Number of sync operations per day (personal usage)",
      "Number of agent queries to synced notes",
      "Number of notes created by agents",
      "Two-way sync working correctly"
    ],
    "quality": [
      "Zero data loss during sync",
      "Error handling covers edge cases",
      "File watcher detects all changes",
      "Conflict resolution works as expected"
    ]
  },

  "implementation_approach": {
    "phase_1_foundation": {
      "duration": "1-2 weeks",
      "scope": "Personal use only",
      "features": ["One-way sync (IDE -> Noted)", "Basic file watching", "Simple API"],
      "goals": ["Validate core sync functionality", "Test API reliability", "Ensure data integrity"]
    },
    "phase_2_bidirectional": {
      "duration": "2-3 weeks",
      "scope": "Personal use with agent integration",
      "features": ["Two-way sync", "Conflict resolution", "Agent RAG queries"],
      "goals": ["Enable agent to read/write notes", "Test real-world workflow", "Refine sync logic"]
    },
    "phase_3_polish": {
      "duration": "1-2 weeks",
      "scope": "Production-ready for personal use",
      "features": ["Real-time sync", "Sidebar panel", "Error recovery"],
      "goals": ["Stable daily driver", "Complete agent workflow", "Documentation"]
    }
  },

  "future_enhancements": {
    "phase_7_multi_ide_support": [
      "JetBrains plugin (IntelliJ, WebStorm, PyCharm)",
      "Neovim plugin",
      "Sublime Text plugin",
      "Use same backend API (just different IDE clients)"
    ],
    "phase_8_team_collaboration": [
      "Shared project workspaces",
      "Team member presence indicators",
      "Review and approval workflow for agent-generated docs",
      "Commenting and discussions on notes"
    ],
    "phase_9_advanced_features": [
      "Code snippet extraction (auto-link code to notes)",
      "Visual diagrams from markdown (Mermaid, PlantUML)",
      "Version history and rollback",
      "Custom templates for different doc types",
      "Integration with GitHub PRs (auto-link notes to PRs)"
    ]
  },

  "comparison_with_existing_tools": {
    "vs_notion_api": {
      "noted_advantage": "Built-in RAG, designed for AI agents from ground up",
      "notion_limitation": "No native RAG, complex API, slow for real-time sync"
    },
    "vs_obsidian_sync": {
      "noted_advantage": "Cloud-first with RAG, multi-device, agent-friendly API",
      "obsidian_limitation": "Local-first, no RAG, limited agent integration"
    },
    "vs_github_wiki": {
      "noted_advantage": "Real-time IDE sync, RAG queries, richer formatting",
      "github_limitation": "Manual git workflow, no RAG, no AI agent integration"
    },
    "vs_confluence": {
      "noted_advantage": "Developer-first, IDE integration, lightweight",
      "confluence_limitation": "Enterprise bloat, slow, not designed for code projects"
    }
  },

  "open_source_strategy": {
    "approach": "Open source the VS Code extension, keep backend proprietary",
    "rationale": [
      "Extension code benefits from community contributions",
      "Backend API is core value (keep proprietary)",
      "Open source extension builds trust and adoption",
      "Community can add features and fix bugs"
    ],
    "license": "MIT License for extension",
    "repository": "https://github.com/noted-app/noted-vscode",
    "contribution_guidelines": "Standard CONTRIBUTING.md with PR templates"
  },

  "technical_challenges": {
    "challenge_1_file_conflicts": {
      "problem": "Note edited in both IDE and Noted app simultaneously",
      "solution": "Three-way merge with visual diff editor",
      "complexity": "Medium",
      "mitigation": "Start with last-write-wins, add advanced merge later"
    },
    "challenge_2_large_files": {
      "problem": "Syncing 10MB+ files (e.g., huge markdown docs)",
      "solution": "Chunk large files, only sync changed chunks",
      "complexity": "High",
      "mitigation": "Set file size limit (1MB) for MVP"
    },
    "challenge_3_performance": {
      "problem": "Syncing 1000+ notes on initial setup takes minutes",
      "solution": "Background batch sync with progress indicator",
      "complexity": "Low",
      "mitigation": "Show progress bar, allow background sync"
    },
    "challenge_4_auth_security": {
      "problem": "API keys must be secure but easy to use",
      "solution": "Use VS Code secure storage API",
      "complexity": "Low",
      "mitigation": "Never store in plain text, use encryption"
    }
  },

  "security_controls": {
    "overview": {
      "security_philosophy": "Defense in depth - multiple layers of security controls",
      "threat_model": "Protect against: unauthorized access, data breaches, API abuse, MITM attacks, credential theft",
      "compliance_targets": "GDPR (data privacy), SOC 2 Type II (security controls), OWASP Top 10"
    },
    "1_api_key_management": {
      "generation": {
        "format": "noted_sk_live_32_random_chars (e.g., noted_sk_live_a1b2c3d4e5f6g7h8i9j0k1l2m3n4o5p6)",
        "algorithm": "Cryptographically secure random bytes (crypto.randomBytes(32))",
        "prefix": "noted_sk_live_ for production, noted_sk_test_ for development",
        "entropy": "256 bits of entropy (extremely high security)"
      },
      "storage": {
        "client_side": "VS Code SecretStorage API (encrypted at rest, OS keychain integration)",
        "server_side": "PostgreSQL with column-level encryption using pgcrypto",
        "hashing": "API keys hashed with bcrypt (cost factor 12) before database storage",
        "never_log": "API keys never appear in logs, error messages, or HTTP responses"
      },
      "validation": {
        "request_flow": [
          "1. Client sends API key in Authorization: Bearer {key} header",
          "2. Edge Function extracts key from header",
          "3. Query database: SELECT user_id, expires_at FROM user_api_keys WHERE key_hash = bcrypt({key})",
          "4. Check expires_at > NOW()",
          "5. Return 401 if invalid/expired, proceed if valid"
        ],
        "validation_latency": "<10ms (bcrypt comparison is fast)",
        "caching": "Cache valid API keys in Redis for 5 minutes to reduce database queries"
      },
      "rotation": {
        "policy": "Force rotation every 90 days",
        "grace_period": "7-day grace period where both old and new keys work",
        "user_notification": "Email sent 14 days before expiration",
        "rotation_workflow": [
          "1. User clicks 'Rotate API Key' in Noted settings",
          "2. New key generated, old key marked for deletion in 7 days",
          "3. User copies new key to VS Code extension settings",
          "4. Old key expires after 7 days"
        ],
        "emergency_revocation": "Immediate revocation available for compromised keys"
      },
      "permissions": {
        "scope": "API keys scoped to specific operations (read, write, delete)",
        "least_privilege": "Default scope: read + write (no delete)",
        "user_control": "User can create multiple keys with different scopes",
        "example_scopes": ["notes:read", "notes:write", "notes:delete", "rag:query", "ide:sync"]
      }
    },
    "2_rate_limiting": {
      "implementation": {
        "algorithm": "Token bucket with Redis backend",
        "storage_key": "ratelimit:{user_id}:{endpoint}:{window}",
        "window": "1 minute sliding window",
        "token_refill": "100 tokens per minute, refilled continuously"
      },
      "limits_by_endpoint": {
        "POST_ide_sync_create": {
          "limit": "30 requests per minute",
          "burst": "10 (allows 10 rapid requests then throttles)",
          "rationale": "Prevent accidental sync loops"
        },
        "PUT_ide_sync_update": {
          "limit": "60 requests per minute",
          "burst": "15",
          "rationale": "Allow normal file editing without hitting limit"
        },
        "GET_ide_sync_list": {
          "limit": "10 requests per minute",
          "burst": "3",
          "rationale": "Expensive query, limit to prevent abuse"
        },
        "POST_rag_query": {
          "limit": "20 requests per minute",
          "burst": "5",
          "rationale": "Expensive embedding + vector search, limit strictly"
        },
        "global": {
          "limit": "100 requests per minute across all endpoints",
          "burst": "20"
        }
      },
      "response_headers": {
        "X_RateLimit_Limit": "100",
        "X_RateLimit_Remaining": "85",
        "X_RateLimit_Reset": "1728475860 (Unix timestamp)",
        "Retry_After": "60 (seconds until reset, only on 429 errors)"
      },
      "429_error_response": {
        "error": "Rate limit exceeded",
        "message": "You have exceeded the rate limit of 100 requests per minute. Please retry after 60 seconds.",
        "retry_after": 60,
        "limit": 100,
        "window": "1 minute"
      },
      "monitoring": {
        "alerts": "Alert when user exceeds 80% of limit (potential abuse)",
        "dashboard": "Real-time rate limit metrics in admin dashboard",
        "user_visibility": "Users can see their rate limit usage in Noted settings"
      },
      "exemptions": {
        "internal_services": "Internal services (e.g., batch jobs) exempt from rate limits",
        "premium_users": "Premium users get 2x rate limits (200 req/min)"
      },
      "agent_specific_rate_limits": {
        "problem": "Agent and human share same 100 req/min quota - agent RAG queries could starve user's manual syncs",
        "solution": "Separate rate limit buckets for agent vs user traffic",
        "detection": "Identify client type via X-Client-Type header: agent | user | ide_extension",
        "separated_quotas": {
          "user_initiated": {
            "limit": "60 requests per minute",
            "endpoints": ["POST /ide-sync/create", "PUT /ide-sync/update", "DELETE /ide-sync/delete", "GET /ide-sync/list"],
            "rationale": "User-initiated file syncs have priority"
          },
          "agent_initiated": {
            "limit": "40 requests per minute total",
            "read_unlimited": "GET /ide-sync/list unlimited (agents need to query frequently)",
            "write_limited": "POST /ide-sync/create, PUT /ide-sync/update limited to 10 requests per minute",
            "rag_query_limited": "POST /rag-query limited to 20 requests per minute",
            "rationale": "Agents can read freely but writes are throttled to prevent runaway agents"
          }
        },
        "implementation": {
          "request_classification": [
            "1. Extract X-Client-Type header from request",
            "2. If header missing: default to 'user' (backward compatibility)",
            "3. Increment separate counters: ratelimit:{user_id}:agent:{endpoint} vs ratelimit:{user_id}:user:{endpoint}",
            "4. Check appropriate limit based on client type"
          ],
          "header_enforcement": "VS Code extension includes X-Client-Type: ide_extension, agent client library includes X-Client-Type: agent"
        },
        "monitoring": {
          "split_dashboards": "Separate dashboards for agent vs user traffic",
          "alerts": [
            "Alert if agent writes >8 req/min (suspicious, may indicate runaway)",
            "Alert if user syncs hit limit (indicates quota too low)"
          ]
        }
      }
    },
    "3_audit_logging": {
      "events_logged": [
        "API key created/rotated/revoked",
        "Note created via IDE sync",
        "Note updated via IDE sync",
        "Note deleted via IDE sync",
        "Conflict detected and resolved",
        "RAG query performed",
        "Rate limit exceeded (429 error)",
        "Authentication failure (401 error)",
        "Unauthorized access attempt (403 error)"
      ],
      "log_schema": {
        "table": "audit_logs",
        "columns": [
          "id UUID PRIMARY KEY",
          "user_id UUID REFERENCES users(id)",
          "event_type TEXT NOT NULL",
          "event_data JSONB",
          "ip_address INET",
          "user_agent TEXT",
          "api_key_id UUID REFERENCES user_api_keys(id)",
          "timestamp TIMESTAMP DEFAULT NOW()",
          "severity TEXT ('info' | 'warning' | 'error' | 'critical')"
        ],
        "indexes": [
          "CREATE INDEX idx_audit_logs_user_id ON audit_logs(user_id, timestamp DESC);",
          "CREATE INDEX idx_audit_logs_event_type ON audit_logs(event_type, timestamp DESC);",
          "CREATE INDEX idx_audit_logs_severity ON audit_logs(severity) WHERE severity IN ('error', 'critical');"
        ]
      },
      "log_example": {
        "id": "a1b2c3d4-e5f6-7890-abcd-ef1234567890",
        "user_id": "user-123",
        "event_type": "note.created.ide_sync",
        "event_data": {
          "note_id": "note-456",
          "note_title": "Architecture Overview",
          "ide_file_path": "docs/architecture.md",
          "content_length": 1234,
          "synced_from": "VS Code 1.85.0"
        },
        "ip_address": "192.168.1.100",
        "user_agent": "NotedVSCodeExtension/1.0.0",
        "api_key_id": "key-789",
        "timestamp": "2025-10-09T12:30:45Z",
        "severity": "info"
      },
      "retention": {
        "hot_storage": "Last 90 days in PostgreSQL (fast queries)",
        "cold_storage": "91-365 days in S3/Glacier (compliance archive)",
        "deletion": "Logs older than 1 year deleted (unless legal hold)"
      },
      "access_control": {
        "user_access": "Users can view their own audit logs in Noted settings",
        "admin_access": "Admins can view all logs with filtering",
        "compliance_export": "Export logs as CSV/JSON for compliance audits"
      },
      "real_time_monitoring": {
        "suspicious_activity_detection": [
          "Multiple 401 errors from same IP (credential stuffing)",
          "Sudden spike in API calls (potential abuse)",
          "Note deletion from unfamiliar location (account compromise)",
          "RAG queries with sensitive keywords (data exfiltration attempt)"
        ],
        "alerting": "Send alerts to security team via PagerDuty/Slack",
        "automated_response": "Auto-revoke API key if 10+ 401 errors in 1 minute"
      }
    },
    "3_5_distributed_tracing": {
      "purpose": "Debug latency issues across distributed request path: IDE -> API -> DB -> Embedding service",
      "problem": "Without tracing, cannot identify which component is slow (is it network? DB query? Embedding generation?)",
      "solution": "OpenTelemetry for end-to-end distributed tracing",
      "implementation": {
        "instrumentation": {
          "vs_code_extension": {
            "library": "@opentelemetry/sdk-node",
            "auto_instrumentation": "Automatic HTTP client instrumentation",
            "custom_spans": [
              "span: 'file_watcher.detect_change' (measures file event -> sync trigger latency)",
              "span: 'sync_operation' (entire sync workflow)",
              "span: 'conflict_detection' (conflict resolution time)"
            ]
          },
          "edge_functions": {
            "library": "@opentelemetry/api",
            "context_propagation": "Extract trace context from X-Trace-Id header",
            "automatic_spans": [
              "HTTP request handling",
              "Database queries (via pg instrumentation)",
              "External API calls (Voyage AI)"
            ],
            "custom_spans": [
              "span: 'idempotency_check' (cache lookup time)",
              "span: 'version_vector_comparison' (conflict detection algorithm)",
              "span: 'embedding_generation' (Voyage AI call)"
            ]
          },
          "database": {
            "instrumentation": "pg library auto-instrumentation",
            "tracked_queries": "All SQL queries with execution time",
            "slow_query_threshold": "Mark query as slow if >100ms"
          }
        },
        "trace_context_propagation": {
          "flow": [
            "1. Extension generates trace_id + span_id on operation start",
            "2. Extension includes X-Trace-Id and X-Span-Id headers in API request",
            "3. Edge Function extracts headers, creates child span",
            "4. Edge Function propagates trace context to DB and external APIs",
            "5. All components log with same trace_id for correlation"
          ],
          "header_format": {
            "X_Trace_Id": "128-bit hex string (e.g., 'a1b2c3d4e5f6g7h8i9j0k1l2m3n4o5p6')",
            "X_Span_Id": "64-bit hex string",
            "X_Parent_Span_Id": "64-bit hex string (for hierarchical traces)"
          }
        },
        "span_attributes": {
          "required_attributes": [
            "user_id: string (for filtering user-specific traces)",
            "operation: create | update | delete | query (operation type)",
            "note_id: string (which note was affected)",
            "client_type: ide_extension | agent | user (source of request)",
            "sync_origin_id: string (prevent sync loops)"
          ],
          "performance_attributes": [
            "content_size_bytes: number (payload size)",
            "db_query_count: number (N+1 query detection)",
            "cache_hit: boolean (cache effectiveness)",
            "retry_count: number (how many retries occurred)"
          ],
          "error_attributes": [
            "error_code: string (e.g., 'NETWORK_TIMEOUT')",
            "error_message: string",
            "stack_trace: string (for debugging)"
          ]
        },
        "trace_sampling": {
          "strategy": "Adaptive sampling based on latency",
          "always_sample": [
            "Requests with errors (100% sampled)",
            "Requests >2 seconds (100% sampled)",
            "Conflict resolution requests (100% sampled)"
          ],
          "probabilistic_sample": "Normal requests: 10% sampled (reduces overhead)",
          "user_override": "X-Force-Trace: true header forces 100% sampling for debugging"
        },
        "trace_export": {
          "backend": "Jaeger or Honeycomb (cloud-based)",
          "export_interval": "5 seconds (batch export to reduce network overhead)",
          "fallback": "If export fails, log traces locally to fallback file"
        }
      },
      "example_trace": {
        "trace_id": "a1b2c3d4e5f6g7h8i9j0k1l2m3n4o5p6",
        "root_span": {
          "span_id": "1234567890abcdef",
          "name": "sync_operation",
          "start_time": "2025-10-09T12:30:00.000Z",
          "end_time": "2025-10-09T12:30:01.500Z",
          "duration_ms": 1500,
          "attributes": {
            "user_id": "user-123",
            "operation": "update",
            "note_id": "note-456",
            "client_type": "ide_extension"
          },
          "children": [
            {
              "span_id": "fedcba0987654321",
              "name": "http_post_ide_sync_update",
              "duration_ms": 1400,
              "children": [
                {
                  "span_id": "1111222233334444",
                  "name": "idempotency_check",
                  "duration_ms": 10,
                  "attributes": {
                    "cache_hit": false
                  }
                },
                {
                  "span_id": "5555666677778888",
                  "name": "version_vector_comparison",
                  "duration_ms": 5,
                  "attributes": {
                    "conflict_detected": false
                  }
                },
                {
                  "span_id": "9999aaaabbbbcccc",
                  "name": "db_update_note",
                  "duration_ms": 50,
                  "attributes": {
                    "query": "UPDATE notes SET content = $1 WHERE id = $2",
                    "rows_affected": 1
                  }
                },
                {
                  "span_id": "ddddeeeeffffgggg",
                  "name": "embedding_generation",
                  "duration_ms": 1300,
                  "attributes": {
                    "provider": "voyage_ai",
                    "content_size_bytes": 5000,
                    "embedding_dimensions": 1024
                  }
                }
              ]
            }
          ]
        }
      },
      "debugging_workflow": {
        "scenario": "User reports slow sync (>10 seconds)",
        "steps": [
          "1. Ask user for trace_id (shown in error message or status bar)",
          "2. Search Jaeger/Honeycomb for trace_id",
          "3. View waterfall diagram of all spans",
          "4. Identify slowest span (e.g., embedding_generation took 12 seconds)",
          "5. Drill into slow span to see attributes (e.g., content_size_bytes: 1000000 -> file too large)",
          "6. Root cause identified: user syncing 1MB file, exceeds limit",
          "7. Fix: return 413 Payload Too Large error earlier in request pipeline"
        ]
      },
      "latency_targets": {
        "p50_targets": {
          "sync_create": "<500ms",
          "sync_update": "<300ms",
          "sync_delete": "<100ms",
          "rag_query": "<500ms"
        },
        "p95_targets": {
          "sync_create": "<2000ms",
          "sync_update": "<1000ms",
          "sync_delete": "<500ms",
          "rag_query": "<1500ms"
        },
        "alerts": "Alert if p95 latency exceeds targets for >5 minutes"
      },
      "cost_and_overhead": {
        "performance_overhead": "<5ms per request (minimal)",
        "storage_cost": "$0.10 per GB of traces (Honeycomb pricing)",
        "estimated_monthly_cost": "10,000 requests/day × 10% sampling × 5KB per trace × 30 days = ~15GB = $1.50/month",
        "roi": "Debugging time reduced from hours to minutes - easily worth the cost"
      }
    },
    "4_data_encryption": {
      "in_transit": {
        "protocol": "TLS 1.3 only (TLS 1.2 disabled)",
        "cipher_suites": "Only AEAD ciphers (AES-GCM, ChaCha20-Poly1305)",
        "certificate": "Let's Encrypt wildcard certificate (*.noted.com)",
        "hsts": "Strict-Transport-Security: max-age=31536000; includeSubDomains; preload",
        "certificate_pinning": "VS Code extension pins Noted API certificate (prevents MITM)"
      },
      "at_rest": {
        "database": "PostgreSQL with transparent data encryption (TDE)",
        "note_content": "Note content encrypted with AES-256-GCM using user-specific keys",
        "encryption_keys": "Stored in AWS KMS / Google Cloud KMS (never in application code)",
        "key_derivation": "PBKDF2 with 100,000 iterations for user-specific encryption keys",
        "encrypted_columns": ["notes.content", "notes.ai_summary", "user_api_keys.key"]
      },
      "key_management": {
        "master_key": "Single master key in KMS, rotated annually",
        "data_keys": "Per-user data encryption keys derived from master key + user_id",
        "rotation": "Master key rotation does not require re-encrypting all data (envelope encryption)",
        "backup_keys": "Encrypted backups stored separately from primary keys"
      }
    },
    "5_access_control": {
      "authentication": {
        "methods": ["API key (for extensions/agents)", "OAuth 2.0 (for web app)", "Passkeys (WebAuthn)"],
        "multi_factor": "Optional 2FA via TOTP (Google Authenticator)",
        "session_management": "JWT tokens with 1-hour expiration, refresh tokens valid for 30 days"
      },
      "authorization": {
        "model": "Role-Based Access Control (RBAC) + Attribute-Based Access Control (ABAC)",
        "roles": ["owner", "editor", "viewer", "agent"],
        "permissions": {
          "owner": ["read", "write", "delete", "share", "manage_api_keys"],
          "editor": ["read", "write"],
          "viewer": ["read"],
          "agent": ["read", "write", "rag_query"]
        },
        "row_level_security": "PostgreSQL RLS policies ensure users only access their own notes",
        "rls_policy_example": "CREATE POLICY user_notes ON notes FOR ALL USING (user_id = current_user_id());"
      },
      "ip_allowlisting": {
        "feature": "Users can restrict API key usage to specific IP ranges",
        "use_case": "Lock API key to office IP or home IP",
        "enforcement": "Edge Function checks request IP against allowlist before processing"
      }
    },
    "6_secret_rotation": {
      "api_keys": {
        "rotation_frequency": "Every 90 days (enforced)",
        "grace_period": "7 days dual-key validity",
        "notification": "Email at 14 days, 7 days, 1 day before expiration"
      },
      "database_credentials": {
        "rotation_frequency": "Every 30 days",
        "automation": "Automated rotation via Terraform/Ansible",
        "zero_downtime": "Blue-green credential rotation (new creds before old expire)"
      },
      "encryption_keys": {
        "master_key_rotation": "Annually via KMS",
        "data_key_rotation": "On-demand when user changes password",
        "automatic_re_encryption": "Background job re-encrypts data with new keys"
      },
      "oauth_secrets": {
        "client_secret_rotation": "Every 6 months",
        "webhook_secrets": "Every 3 months",
        "jwt_signing_keys": "Every 6 months with 30-day overlap"
      }
    },
    "7_compliance": {
      "gdpr": {
        "data_subject_rights": [
          "Right to access (export all user data as JSON)",
          "Right to erasure (delete account + all notes within 30 days)",
          "Right to portability (export notes as markdown ZIP)",
          "Right to rectification (edit/update data)"
        ],
        "consent_management": "Explicit opt-in for AI features (can disable AI processing)",
        "data_processing_agreement": "DPA available for enterprise customers",
        "data_residency": "EU users' data stored in EU region (Frankfurt AWS)"
      },
      "soc_2_type_ii": {
        "controls": [
          "Access control (RBAC + MFA)",
          "Encryption (TLS 1.3 + AES-256)",
          "Audit logging (90-day retention)",
          "Incident response (documented procedures)",
          "Vendor management (third-party risk assessment)"
        ],
        "audit_frequency": "Annual SOC 2 audit by independent auditor",
        "report_availability": "SOC 2 report available to enterprise customers under NDA"
      },
      "owasp_top_10": {
        "A01_broken_access_control": "Mitigated by RLS + API key validation",
        "A02_cryptographic_failures": "Mitigated by TLS 1.3 + AES-256 encryption",
        "A03_injection": "Mitigated by parameterized queries + input validation",
        "A04_insecure_design": "Mitigated by threat modeling + security reviews",
        "A05_security_misconfiguration": "Mitigated by automated security scanning",
        "A07_identification_authentication_failures": "Mitigated by MFA + passkeys",
        "A08_software_data_integrity_failures": "Mitigated by signed releases + SRI",
        "A09_security_logging_monitoring_failures": "Mitigated by comprehensive audit logging",
        "A10_ssrf": "Mitigated by URL validation + allowlist for external requests"
      }
    },
    "8_incident_response": {
      "detection": {
        "automated_alerts": [
          "Unusual API activity (10x normal volume)",
          "Multiple failed auth attempts (>10 in 5 min)",
          "Unexpected data deletion (>100 notes deleted at once)",
          "RAG queries with suspicious patterns (e.g., SELECT * FROM passwords)"
        ],
        "monitoring_tools": "Datadog, Sentry, CloudWatch Logs"
      },
      "response_procedures": {
        "severity_1_critical": {
          "definition": "Data breach, system compromise, or service-wide outage",
          "response_time": "Within 15 minutes",
          "actions": [
            "1. Page on-call engineer immediately",
            "2. Assess scope of breach/compromise",
            "3. Isolate affected systems",
            "4. Revoke compromised API keys",
            "5. Notify affected users within 72 hours (GDPR requirement)",
            "6. Engage legal and PR teams",
            "7. Conduct post-mortem and implement fixes"
          ]
        },
        "severity_2_high": {
          "definition": "Elevated error rates, potential security vulnerability",
          "response_time": "Within 1 hour",
          "actions": [
            "1. Investigate logs and metrics",
            "2. Identify root cause",
            "3. Implement temporary mitigation",
            "4. Deploy permanent fix within 24 hours",
            "5. Update security runbooks"
          ]
        }
      },
      "post_incident": {
        "root_cause_analysis": "Required for all severity 1 and 2 incidents",
        "documentation": "Incident report with timeline, root cause, and remediation",
        "preventive_measures": "Update security controls to prevent recurrence",
        "customer_communication": "Transparent incident report published to status page"
      }
    },
    "9_third_party_security": {
      "vendor_assessment": {
        "criteria": [
          "SOC 2 Type II compliance",
          "ISO 27001 certification",
          "GDPR compliance",
          "Incident response SLA",
          "Data encryption at rest and in transit"
        ],
        "approved_vendors": {
          "anthropic": "Claude API - SOC 2 compliant, ISO 27001 certified",
          "voyage_ai": "Voyage AI - SOC 2 in progress, TLS 1.3 enforced",
          "supabase": "SOC 2 Type II, ISO 27001, HIPAA compliant"
        }
      },
      "data_sharing": {
        "principle": "Minimize data shared with third parties",
        "anthropic": "Only note content sent (no user PII)",
        "voyage_ai": "Only note content for embedding (no metadata)",
        "analytics": "Anonymized usage data only (no note content)"
      },
      "contract_requirements": {
        "data_processing_addendum": "Required for all vendors processing user data",
        "liability_clauses": "Vendor liable for data breaches caused by their negligence",
        "termination_rights": "Right to terminate immediately if vendor breaches security"
      }
    },
    "10_penetration_testing": {
      "frequency": "Quarterly penetration tests by external security firm",
      "scope": [
        "API endpoints (authentication bypass, injection attacks)",
        "VS Code extension (malicious code execution)",
        "Web application (XSS, CSRF, clickjacking)",
        "Infrastructure (network segmentation, privilege escalation)"
      ],
      "remediation_sla": {
        "critical": "Fix within 7 days",
        "high": "Fix within 30 days",
        "medium": "Fix within 90 days",
        "low": "Fix in next release cycle"
      },
      "bug_bounty": {
        "program": "Public bug bounty program via HackerOne",
        "rewards": "$100-$5000 depending on severity",
        "scope": "All production systems except third-party services"
      }
    }
  },

  "implementation_timeline": {
    "overview": {
      "total_duration": "30-40 hours for MVP (phases 1-4)",
      "extended_duration": "44-56 hours for complete implementation (all 6 phases)",
      "working_schedule": "Solo developer, 4-6 hours per day = 7-10 days for MVP",
      "critical_path": "Phase 1 -> Phase 2 -> Phase 3 -> Phase 4 (sequential dependencies)"
    },
    "dependency_graph": {
      "description": "Visual representation of phase dependencies",
      "ascii_diagram": [
        "Phase 1 (Foundation)",
        "    |",
        "    v",
        "Phase 2 (Noted API) <-- MUST complete Phase 1 first",
        "    |",
        "    v",
        "Phase 3 (Two-way Sync) <-- MUST complete Phase 2 first",
        "    |",
        "    +---> RAG System (parallel track, can start after Phase 2)",
        "    |         |",
        "    v         v",
        "Phase 4 (Agent Integration) <-- MUST complete Phase 3 AND RAG",
        "    |",
        "    v",
        "Phase 5 (IDE Sidebar) <-- Optional, can start after Phase 4",
        "    |",
        "    v",
        "Phase 6 (Real-time) <-- Optional, can start after Phase 5"
      ],
      "parallel_work_opportunities": [
        "RAG implementation can happen while Phase 3 is being built",
        "Database migrations can be written during Phase 1",
        "VS Code extension UI can be designed while backend is being built"
      ]
    },
    "detailed_timeline": {
      "week_1": {
        "days": "1-3 (12-18 hours)",
        "phases": ["Phase 1: VS Code Extension Foundation"],
        "milestones": [
          {
            "milestone": "M1: File watcher operational",
            "hours": "4-6h",
            "acceptance_criteria": [
              "Extension activates when VS Code opens",
              "File watcher detects changes in /docs folder",
              "Console logs show detected file changes",
              "Debouncing works (5-second delay after last edit)"
            ],
            "testing": [
              "Create test.md in /docs, verify detection",
              "Edit test.md 5 times rapidly, verify single sync trigger",
              "Delete test.md, verify deletion detected"
            ],
            "blockers": [
              "VS Code extension API learning curve",
              "File watcher configuration complexity"
            ]
          },
          {
            "milestone": "M2: API authentication working",
            "hours": "2-3h",
            "acceptance_criteria": [
              "User can enter API key in VS Code settings",
              "API key stored securely in VS Code SecretStorage",
              "Extension validates API key on startup",
              "Invalid key shows error notification"
            ],
            "testing": [
              "Enter valid API key, verify green checkmark",
              "Enter invalid key, verify error message",
              "Restart VS Code, verify key persists"
            ],
            "dependencies": ["M1 complete"]
          },
          {
            "milestone": "M3: One-way sync IDE -> Noted",
            "hours": "6-9h",
            "acceptance_criteria": [
              "File changes in IDE trigger API call to Noted",
              "New note created in Noted app with correct title/content",
              "Note metadata includes synced_from_ide: true",
              "ide_file_path stored correctly",
              "Success notification shown in VS Code"
            ],
            "testing": [
              "Create architecture.md in /docs",
              "Write markdown content",
              "Wait 5 seconds, verify note appears in Noted app",
              "Edit file, verify note updates in Noted app"
            ],
            "dependencies": ["M1 complete", "M2 complete", "Phase 2 M1 complete (API endpoint exists)"],
            "note": "Cannot complete until Phase 2 M1 (create endpoint) is done - work in parallel"
          }
        ]
      },
      "week_1_parallel": {
        "days": "1-3 (6-8 hours, parallel to Phase 1)",
        "phases": ["Phase 2: Noted API Sync Endpoints"],
        "milestones": [
          {
            "milestone": "M1: Create endpoint operational",
            "hours": "3-4h",
            "acceptance_criteria": [
              "POST /ide-sync/create endpoint exists",
              "Accepts title, content, ide_file_path, ide_last_modified",
              "Creates note in database with synced_from_ide: true",
              "Returns note ID and metadata in response",
              "Rate limiting enforced (100 req/min)"
            ],
            "testing": [
              "Send POST request with curl, verify note created",
              "Check database: synced_from_ide column is true",
              "Send 101 requests, verify 101st returns 429 error"
            ],
            "dependencies": ["Database migration complete"],
            "blockers": ["Database migration must be written and applied first"]
          },
          {
            "milestone": "M2: Update/Delete endpoints operational",
            "hours": "2-3h",
            "acceptance_criteria": [
              "PUT /ide-sync/update/{id} updates note content",
              "DELETE /ide-sync/delete/{id} deletes note",
              "Endpoints validate API key",
              "Endpoints return proper error codes (401, 404, 409)"
            ],
            "testing": [
              "Update note via API, verify changes in database",
              "Delete note via API, verify removal from database",
              "Send request with invalid API key, verify 401 error"
            ],
            "dependencies": ["M1 complete"]
          },
          {
            "milestone": "M3: Database migration applied",
            "hours": "1-2h",
            "acceptance_criteria": [
              "ALTER TABLE notes ADD COLUMN synced_from_ide executed",
              "ALTER TABLE notes ADD COLUMN ide_file_path executed",
              "ALTER TABLE notes ADD COLUMN last_ide_sync_at executed",
              "Index created on (user_id, synced_from_ide)",
              "Migration rollback tested"
            ],
            "testing": [
              "Run migration on local database",
              "Verify columns exist: SELECT * FROM notes LIMIT 1",
              "Rollback migration, verify columns removed",
              "Re-apply migration, verify success"
            ],
            "dependencies": [],
            "note": "Can be done first, before Phase 1 or Phase 2 work starts"
          }
        ]
      },
      "week_2": {
        "days": "4-6 (10-14 hours)",
        "phases": ["Phase 3: Two-Way Sync"],
        "milestones": [
          {
            "milestone": "M1: Polling mechanism working",
            "hours": "3-4h",
            "acceptance_criteria": [
              "Extension polls /ide-sync/changes every 10 seconds",
              "Only fetches notes modified since last poll",
              "Polling pauses when VS Code loses focus (optimization)",
              "Polling resumes when VS Code regains focus"
            ],
            "testing": [
              "Edit note in Noted app, wait 10 seconds, verify detection",
              "Switch to different app, verify polling paused",
              "Switch back to VS Code, verify polling resumed"
            ],
            "dependencies": ["Phase 2 M1 complete (API exists)"]
          },
          {
            "milestone": "M2: Noted -> IDE sync working",
            "hours": "3-4h",
            "acceptance_criteria": [
              "New note created in Noted appears in IDE /docs folder",
              "Note edits in Noted update IDE files",
              "File created with correct name (title.md)",
              "VS Code toast notification shown on sync"
            ],
            "testing": [
              "Create note in Noted app with title 'Test Sync'",
              "Wait 10 seconds, verify test-sync.md appears in IDE",
              "Edit note in Noted app, verify file updated in IDE"
            ],
            "dependencies": ["M1 complete"]
          },
          {
            "milestone": "M3: Conflict detection operational",
            "hours": "4-6h",
            "acceptance_criteria": [
              "Simultaneous edits detected (three-way timestamp comparison)",
              "VS Code notification shown when conflict detected",
              "Conflict resolution panel opens with diff view",
              "User can choose resolution option (5 options available)",
              "Backups created before resolution (.noted-conflicts/ folder)"
            ],
            "testing": [
              "Edit architecture.md in IDE, don't save",
              "Edit same note in Noted app, save",
              "Save IDE file, verify conflict notification",
              "Choose 'Keep IDE Version', verify Noted updated",
              "Repeat, choose 'Manual Merge', verify 3-way editor opens"
            ],
            "dependencies": ["M2 complete"]
          }
        ]
      },
      "week_2_parallel": {
        "days": "4-6 (8-10 hours, parallel to Phase 3)",
        "phases": ["RAG System Implementation"],
        "milestones": [
          {
            "milestone": "M1: pgvector setup complete",
            "hours": "2h",
            "acceptance_criteria": [
              "pgvector extension enabled in PostgreSQL",
              "note_embeddings table created",
              "IVFFlat index created with cosine similarity",
              "Test embedding inserted and queried successfully"
            ],
            "testing": [
              "Run: CREATE EXTENSION vector;",
              "Create note_embeddings table",
              "Insert test vector: INSERT INTO note_embeddings ...",
              "Query: SELECT * FROM note_embeddings ORDER BY embedding <=> $1 LIMIT 5"
            ],
            "dependencies": []
          },
          {
            "milestone": "M2: Embedding generation working",
            "hours": "4-5h",
            "acceptance_criteria": [
              "Voyage AI API key configured",
              "Edge Function generate-embeddings deployed",
              "Database trigger fires on note insert/update",
              "Embedding generated and stored in note_embeddings table",
              "Content hash prevents redundant embeddings"
            ],
            "testing": [
              "Create new note in Noted app",
              "Wait 2 seconds, verify embedding exists in note_embeddings",
              "Edit note content, verify new embedding generated",
              "Edit note title only, verify embedding NOT regenerated"
            ],
            "dependencies": ["M1 complete"]
          },
          {
            "milestone": "M3: RAG query endpoint working",
            "hours": "2-3h",
            "acceptance_criteria": [
              "POST /rag-query endpoint accepts query string",
              "Query embedding generated",
              "Vector similarity search returns top K results",
              "Results sorted by similarity score",
              "Latency < 500ms p95"
            ],
            "testing": [
              "Send query: 'authentication architecture'",
              "Verify returns relevant notes with similarity > 0.7",
              "Send unrelated query: 'xyz123', verify no results",
              "Measure latency: verify < 500ms"
            ],
            "dependencies": ["M2 complete"]
          }
        ]
      },
      "week_3": {
        "days": "7-9 (8-10 hours)",
        "phases": ["Phase 4: Agent Integration Layer"],
        "milestones": [
          {
            "milestone": "M1: Agent API client library complete",
            "hours": "4-5h",
            "acceptance_criteria": [
              "NotedAgentClient class implemented in TypeScript",
              "Methods: queryNotes(), createNote(), updateNote(), listProjectNotes()",
              "API key authentication working",
              "Error handling for all failure cases",
              "TypeScript types exported"
            ],
            "testing": [
              "Import client: import { NotedAgentClient } from '@noted/agent-client'",
              "Initialize with API key",
              "Call queryNotes('test'), verify results returned",
              "Call createNote('Test', 'Content'), verify note created"
            ],
            "dependencies": ["Phase 2 complete", "Phase 3 complete", "RAG M3 complete"]
          },
          {
            "milestone": "M2: RAG integration working",
            "hours": "3-4h",
            "acceptance_criteria": [
              "queryNotes() uses RAG endpoint internally",
              "Semantic search returns relevant results",
              "Agent can query by meaning, not just keywords",
              "Results include similarity scores"
            ],
            "testing": [
              "Agent queries: 'how does auth work?'",
              "Verify returns authentication-related notes",
              "Agent queries: 'database schema'",
              "Verify returns database design notes"
            ],
            "dependencies": ["M1 complete"]
          },
          {
            "milestone": "M3: Documentation and examples complete",
            "hours": "1h",
            "acceptance_criteria": [
              "README with quickstart guide",
              "API reference documentation",
              "3 code examples (query, create, update)",
              "Usage instructions for Claude Code integration"
            ],
            "testing": [
              "Follow quickstart guide from scratch, verify works",
              "Copy-paste code examples, verify they run"
            ],
            "dependencies": ["M2 complete"]
          }
        ]
      },
      "week_4_optional": {
        "days": "10-12 (12-16 hours, optional phases)",
        "phases": ["Phase 5: IDE Sidebar Panel", "Phase 6: Real-time Collaboration"],
        "note": "These are optional enhancements, not required for MVP",
        "milestones": [
          {
            "milestone": "M1: Sidebar tree view working",
            "hours": "6-8h",
            "acceptance_criteria": [
              "Sidebar panel appears in VS Code activity bar",
              "Tree view lists all synced notes",
              "Click note opens file in editor",
              "Sync status indicators show (synced, conflict, syncing)"
            ],
            "testing": [
              "Click Noted icon in activity bar, verify sidebar opens",
              "Click note in tree, verify file opens",
              "Create note in Noted app, verify appears in tree"
            ],
            "dependencies": ["Phase 4 complete"]
          },
          {
            "milestone": "M2: WebSocket real-time sync",
            "hours": "6-8h",
            "acceptance_criteria": [
              "WebSocket connection established on extension activation",
              "Note changes pushed in real-time (no 10-second delay)",
              "Fallback to polling if WebSocket fails",
              "Connection resilience (auto-reconnect)"
            ],
            "testing": [
              "Edit note in Noted app, verify IDE file updates instantly",
              "Disconnect internet, verify fallback to polling",
              "Reconnect internet, verify WebSocket reconnects"
            ],
            "dependencies": ["Phase 5 complete"]
          }
        ]
      }
    },
    "critical_path_analysis": {
      "critical_path": [
        "Phase 2 M3 (Database migration) - 1-2h",
        "Phase 1 M1 (File watcher) - 4-6h",
        "Phase 1 M2 (API auth) - 2-3h",
        "Phase 2 M1 (Create endpoint) - 3-4h",
        "Phase 1 M3 (One-way sync) - 6-9h",
        "Phase 2 M2 (Update/Delete endpoints) - 2-3h",
        "Phase 3 M1 (Polling) - 3-4h",
        "Phase 3 M2 (Noted -> IDE) - 3-4h",
        "Phase 3 M3 (Conflict detection) - 4-6h",
        "RAG M1 (pgvector) - 2h (parallel)",
        "RAG M2 (Embeddings) - 4-5h (parallel)",
        "RAG M3 (Query endpoint) - 2-3h (parallel)",
        "Phase 4 M1 (Client library) - 4-5h",
        "Phase 4 M2 (RAG integration) - 3-4h",
        "Phase 4 M3 (Documentation) - 1h"
      ],
      "total_critical_path_hours": "38-53h",
      "optimized_with_parallel_work": "30-40h (RAG can be built while Phase 3 is in progress)",
      "bottlenecks": [
        "Phase 1 M3 cannot start until Phase 2 M1 complete (API must exist)",
        "Phase 4 cannot start until both Phase 3 AND RAG complete",
        "File watcher (M1) is foundational - nothing else can progress without it"
      ]
    },
    "testing_milestones": {
      "unit_tests": {
        "when": "After each milestone completion",
        "coverage_target": "70% for core logic (API client, conflict detection)",
        "tools": "Jest for TypeScript, pytest for Python examples"
      },
      "integration_tests": {
        "when": "After each phase completion",
        "scenarios": [
          "End-to-end sync: IDE -> Noted -> back to IDE",
          "Conflict resolution: simultaneous edits",
          "RAG query: semantic search returns relevant results",
          "Agent workflow: query -> create -> update"
        ]
      },
      "acceptance_tests": {
        "when": "After Phase 4 complete (MVP done)",
        "scenarios": [
          "Developer installs extension, syncs 10 notes, no errors",
          "Agent queries documentation, gets correct answers",
          "Conflict occurs, developer resolves successfully",
          "All features work across IDE restart"
        ],
        "success_criteria": "All scenarios pass without manual intervention"
      },
      "property_based_testing": {
        "purpose": "Find edge cases in sync logic that manual test cases miss",
        "problem": "Distributed sync has infinite edge cases: create/edit/delete sequences, network failures, clock skew, concurrent edits",
        "solution": "Generate random operation sequences and verify invariants hold",
        "tool": "fast-check (TypeScript property-based testing library)",
        "implementation": {
          "test_scenario_1_sync_convergence": {
            "description": "All clients converge to same state despite random operation order",
            "property": "Given random sequence of (create, edit, delete) operations across 3 clients (IDE, Noted, Agent), after all syncs complete, all clients have identical notes",
            "generator": "Generate 100 random operations: 50% edits, 30% creates, 20% deletes",
            "invariants": [
              "No data loss: sum(creates) - sum(deletes) == final note count",
              "Eventual consistency: IDE.notes == Noted.notes == Agent.notes (after syncs)",
              "Version vector monotonicity: version counters only increase"
            ],
            "code_example": "fc.assert(\n  fc.property(\n    fc.array(fc.oneof(\n      fc.record({op: fc.constant('create'), title: fc.string(), content: fc.string()}),\n      fc.record({op: fc.constant('edit'), id: fc.uuid(), content: fc.string()}),\n      fc.record({op: fc.constant('delete'), id: fc.uuid()})\n    ), {minLength: 10, maxLength: 100}),\n    async (operations) => {\n      const [ide, noted, agent] = await setupClients();\n      for (const op of operations) {\n        const client = randomChoice([ide, noted, agent]);\n        await client.execute(op);\n      }\n      await syncAll([ide, noted, agent]);\n      expect(ide.notes).toEqual(noted.notes);\n      expect(noted.notes).toEqual(agent.notes);\n    }\n  )\n)"
          },
          "test_scenario_2_conflict_detection": {
            "description": "Conflicts detected for all concurrent edits",
            "property": "If two clients edit same note concurrently (without syncing), conflict is always detected",
            "generator": "Generate pairs of concurrent edits to same note",
            "invariants": [
              "Concurrent edits always produce conflict",
              "Sequential edits never produce conflict",
              "Conflict resolution never loses data"
            ]
          },
          "test_scenario_3_idempotency": {
            "description": "Retrying same operation produces same result",
            "property": "Sending same create request 10 times with same idempotency_key results in exactly 1 note created",
            "generator": "Generate random note content and idempotency key",
            "invariants": [
              "Idempotent creates: N identical requests -> 1 note",
              "Response identical for all retries",
              "No duplicate notes in database"
            ]
          },
          "test_scenario_4_offline_sync": {
            "description": "Offline edits sync correctly when reconnected",
            "property": "Client can make arbitrary edits while offline, syncs correctly on reconnect without data loss",
            "generator": "Generate random offline edit sequences + network partition duration",
            "invariants": [
              "All offline edits eventually sync",
              "No edits lost during partition",
              "Conflicts detected for overlapping edits"
            ]
          }
        },
        "execution": {
          "frequency": "Run in CI on every commit to sync logic",
          "test_count": "Run each property 100 times with different random seeds",
          "shrinking": "When failure found, fast-check automatically shrinks to minimal failing case",
          "example_shrunk_case": "Property fails on sequence: [create(A), edit(A), delete(A), edit(A)] -> shrunk to: [delete(A), edit(A)]"
        },
        "coverage": {
          "scenarios_tested": ">10,000 random sync sequences per CI run",
          "bugs_found_estimate": "5-10 edge cases in first run (typical for distributed systems)",
          "confidence": "95% confidence all common edge cases covered after 100 runs"
        }
      },
      "chaos_engineering": {
        "purpose": "Verify resilience under real-world failures",
        "problem": "Need to test behavior when API times out, network partitions, DB deadlocks occur",
        "solution": "Inject failures in staging environment and verify graceful degradation",
        "failure_injection_types": [
          "Network latency: Add 1-10 second delays to API responses",
          "Timeouts: Force API to timeout 50% of requests",
          "Partial responses: Return truncated JSON responses",
          "Rate limiting: Trigger 429 errors artificially",
          "DB deadlocks: Concurrent writes to same note",
          "Network partitions: Disconnect IDE for 1-60 seconds"
        ],
        "verification": {
          "expected_behaviors": [
            "Extension retries with exponential backoff",
            "Circuit breaker opens after 5 failures",
            "Queued operations persist across restarts",
            "User sees clear error messages",
            "No data loss despite failures"
          ],
          "failure_criteria": [
            "Extension crashes (unacceptable)",
            "Data loss occurs (unacceptable)",
            "Infinite retry loops (unacceptable)"
          ]
        },
        "tools": {
          "manual": "Use Proxyman/Charles Proxy to inject failures manually",
          "automated": "Chaos Monkey for VS Code (custom tool)",
          "staging_only": "Never run chaos tests in production (staging only)"
        }
      }
    },
    "risk_mitigation_timeline": {
      "risk_1_api_changes": {
        "risk": "Supabase or Voyage AI API changes break integration",
        "mitigation": "Version pinning, abstraction layer for API clients",
        "when": "Phase 2 and RAG implementation",
        "buffer": "Add 2-4h buffer for API troubleshooting"
      },
      "risk_2_vs_code_api": {
        "risk": "VS Code extension API learning curve slows Phase 1",
        "mitigation": "Study official examples first, allocate extra time",
        "when": "Phase 1 M1 (file watcher)",
        "buffer": "Add 2-3h buffer for learning"
      },
      "risk_3_conflict_complexity": {
        "risk": "Conflict resolution UI more complex than estimated",
        "mitigation": "Start with simple 2-option UI, expand later",
        "when": "Phase 3 M3",
        "buffer": "Add 2-4h buffer for UI work"
      },
      "risk_4_rag_performance": {
        "risk": "Vector search latency exceeds 500ms target",
        "mitigation": "Optimize index parameters, consider caching",
        "when": "RAG M3",
        "buffer": "Add 2-3h buffer for performance tuning"
      }
    },
    "adjusted_timeline_with_buffers": {
      "baseline_estimate": "30-40 hours",
      "risk_buffers": "8-14 hours (20-35% buffer)",
      "realistic_estimate": "38-54 hours for MVP",
      "aggressive_timeline": "30 hours (if everything goes perfectly)",
      "conservative_timeline": "54 hours (if multiple blockers hit)",
      "recommended_timeline": "40-45 hours (assumes some blockers, allows for learning)"
    }
  },

  "developer_documentation": {
    "guides_needed": [
      "Quickstart: Install and setup in 5 minutes",
      "Configuration: All settings explained",
      "Agent Integration: How to query notes from your AI agent",
      "API Reference: Complete REST API documentation",
      "Troubleshooting: Common issues and solutions",
      "Advanced: Custom sync strategies and conflict resolution"
    ],
    "code_examples": [
      "Basic sync setup",
      "Agent querying notes via RAG",
      "Agent creating new notes",
      "Handling conflicts programmatically",
      "Custom file watchers"
    ]
  },

  "notes": [
    "This is a game-changer for AI-assisted development",
    "Turns Noted into the 'operating system' for human-AI collaboration",
    "Living documentation is the future - this makes it practical",
    "RAG + IDE integration = persistent agent memory",
    "Start with VS Code (widest audience), expand to other IDEs later",
    "Open source the extension to build community and trust",
    "Focus on developer experience - make sync invisible and automatic",
    "This positions Noted as the go-to tool for AI-assisted development teams"
  ]
}

